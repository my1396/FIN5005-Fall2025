[
  {
    "objectID": "00_environment_setup.html",
    "href": "00_environment_setup.html",
    "title": "1 Seting Up Your Environment",
    "section": "",
    "text": "This guide will help you set up your coding environment for FIN5005, with both cloud-based and local options available (you may choose either).\n\nYou will learn how to use RStudio Cloud for quick, browser-based access and RStudio Desktop for the full set of features on your computer.\nIt is strongly recommended to install the local version (RStudio Desktop) on your computer.\n\nFollow the instructions below to get started.\nEstimated reading time: 6 minutes\n\n\n\n\n\nCloud solution means that you don‚Äôt need to install anything on your computer. You can run R code directly in your web browser. This is the easiest way to get started without worrying about installation issues.\nBut soon enough, you will realize that the cloud-based solution is limited in:\n\ncompute power: 1 CPU core, 1 GB RAM\nexecution hours: 25 hours per month;\nslow performance\nAI integration is better in local environment\nGitHub Copilot is a powerful AI tool that offers autocomplete-style suggestions as you code. The tool can give you suggestions based on the code you want to use or by simply inquiring about what you want the code to do.\nIt is developed by GitHub in partnership with OpenAI, and it is designated to best assist developers in writing code more efficiently.\nSome of its highlight features include:\n\nCode autocompletion: generating suggestions while typing the code.\nCode generation: Copilot will use the context of the active document to generate suggestions for code that might be useful.\nAnswering questions: it can also be used to ask simple questions while you are coding (e.g., ‚ÄúWhat is the definition of mean?‚Äù).\nLanguage support: supports multiple programming languages, including R, Python, SQL, HTML, and JavaScript.\n\nTo enable Copilot in RStudio: click on Tools ‚Üí Global Options ‚Üí Copilot ‚Üí tick the box saying ‚ÄúEnable GitHub Copilot‚Äù ‚Üí sign in to your GitHub account, and there you go; you are ready to start!\nGiHub Copilot free plan has limited usage: 2000 code autocompletions per month, 30 code generations per month, and 30 question answers per month.\nSoon enough, you will realize that you need more than that. Because every character you type counts as usage, regless of whether you accept the suggestion or not.\nüéà The good news is: You can use Copilot Pro for free if you are a student!\nSee Apply to GitHub Education as a student.\nIf you wonder anything, look up in GitHub Copilot Documentation.\n\n\nNonetheless, here are two popular cloud solutions:\n\nGoogle Colab: good support for Jupyter Notebooks .ipynb.\nWe will use Jupyter Notebooks in this course.\nRStudio Cloud: basic emulator for RStudio IDE in the cloud.\n\n\n\nOnce in the dashboard, you can click on the New Project - New RStudio Project to get started:\n\nOnce the new project is created, you will see the RStudio IDE interface, which is similar to the one you would find in RStudio Desktop. You can write and execute R code, create scripts, and manage your projects directly in the cloud.\n\n\n\n\n\n\n\nRStudio Desktop is a free and open-source integrated development environment (IDE) for R. It provides a user-friendly interface for writing and executing R code, making it easier to work with R scripts, data analysis, visualization.\nTo install RStudio Desktop, follow these steps:\n\nInstall R\nInstall RStudio Desktop\n\nGo to RStudio Desktop Download and download the installer for your operating system (Windows, macOS, or Linux).\n\nref:\n\nRStudio Cloud - How to Get Started For Free",
    "crumbs": [
      "Home",
      "Environment Setup"
    ]
  },
  {
    "objectID": "00_environment_setup.html#cloud-solution",
    "href": "00_environment_setup.html#cloud-solution",
    "title": "1 Seting Up Your Environment",
    "section": "",
    "text": "Cloud solution means that you don‚Äôt need to install anything on your computer. You can run R code directly in your web browser. This is the easiest way to get started without worrying about installation issues.\nBut soon enough, you will realize that the cloud-based solution is limited in:\n\ncompute power: 1 CPU core, 1 GB RAM\nexecution hours: 25 hours per month;\nslow performance\nAI integration is better in local environment\nGitHub Copilot is a powerful AI tool that offers autocomplete-style suggestions as you code. The tool can give you suggestions based on the code you want to use or by simply inquiring about what you want the code to do.\nIt is developed by GitHub in partnership with OpenAI, and it is designated to best assist developers in writing code more efficiently.\nSome of its highlight features include:\n\nCode autocompletion: generating suggestions while typing the code.\nCode generation: Copilot will use the context of the active document to generate suggestions for code that might be useful.\nAnswering questions: it can also be used to ask simple questions while you are coding (e.g., ‚ÄúWhat is the definition of mean?‚Äù).\nLanguage support: supports multiple programming languages, including R, Python, SQL, HTML, and JavaScript.\n\nTo enable Copilot in RStudio: click on Tools ‚Üí Global Options ‚Üí Copilot ‚Üí tick the box saying ‚ÄúEnable GitHub Copilot‚Äù ‚Üí sign in to your GitHub account, and there you go; you are ready to start!\nGiHub Copilot free plan has limited usage: 2000 code autocompletions per month, 30 code generations per month, and 30 question answers per month.\nSoon enough, you will realize that you need more than that. Because every character you type counts as usage, regless of whether you accept the suggestion or not.\nüéà The good news is: You can use Copilot Pro for free if you are a student!\nSee Apply to GitHub Education as a student.\nIf you wonder anything, look up in GitHub Copilot Documentation.\n\n\nNonetheless, here are two popular cloud solutions:\n\nGoogle Colab: good support for Jupyter Notebooks .ipynb.\nWe will use Jupyter Notebooks in this course.\nRStudio Cloud: basic emulator for RStudio IDE in the cloud.\n\n\n\nOnce in the dashboard, you can click on the New Project - New RStudio Project to get started:\n\nOnce the new project is created, you will see the RStudio IDE interface, which is similar to the one you would find in RStudio Desktop. You can write and execute R code, create scripts, and manage your projects directly in the cloud.",
    "crumbs": [
      "Home",
      "Environment Setup"
    ]
  },
  {
    "objectID": "00_environment_setup.html#local-solution",
    "href": "00_environment_setup.html#local-solution",
    "title": "1 Seting Up Your Environment",
    "section": "",
    "text": "RStudio Desktop is a free and open-source integrated development environment (IDE) for R. It provides a user-friendly interface for writing and executing R code, making it easier to work with R scripts, data analysis, visualization.\nTo install RStudio Desktop, follow these steps:\n\nInstall R\nInstall RStudio Desktop\n\nGo to RStudio Desktop Download and download the installer for your operating system (Windows, macOS, or Linux).\n\nref:\n\nRStudio Cloud - How to Get Started For Free",
    "crumbs": [
      "Home",
      "Environment Setup"
    ]
  },
  {
    "objectID": "01_Lab-1.html",
    "href": "01_Lab-1.html",
    "title": "Lab 1: Probability & Descriptive Statistics",
    "section": "",
    "text": "Open in Google Colab: Link\nFocus: explore core descriptive statistics and dependence concepts using simple synthetic data.\nYou will:\nAlternatively, copy and paste the code below into RStuido Desktop or RStudio Cloud.",
    "crumbs": [
      "Home",
      "Basics",
      "Lab 1: Probability & Descriptive Statistics"
    ]
  },
  {
    "objectID": "01_Lab-1.html#datasets",
    "href": "01_Lab-1.html#datasets",
    "title": "Lab 1: Probability & Descriptive Statistics",
    "section": "1 Datasets",
    "text": "1 Datasets\nSynthetic generators used:\n\nRight-skewed transaction values (log-normal)\nApprox. normal reference sample\nTwo correlated asset return series (œÅ = 0.90) for sampling exercises",
    "crumbs": [
      "Home",
      "Basics",
      "Lab 1: Probability & Descriptive Statistics"
    ]
  },
  {
    "objectID": "01_Lab-1.html#instructions",
    "href": "01_Lab-1.html#instructions",
    "title": "Lab 1: Probability & Descriptive Statistics",
    "section": "2 Instructions",
    "text": "2 Instructions\nRun each cell sequentially. Read the comments carefully.\n\n# ---- Setup: packages & helper (updated) ----\n# Unified required package list\npkgs &lt;- c(\"tidyverse\", \"moments\", \"knitr\", \"kableExtra\")\nmissing &lt;- setdiff(pkgs, rownames(installed.packages()))\nif (length(missing) &gt; 0) install.packages(missing)\n\n# Load all packages (silently)\ninvisible(lapply(pkgs, library, character.only = TRUE))\n\n# Set default options for figures in Jupyter Notebook\noptions(repr.plot.width = 12, repr.plot.height = 4)  # wider default figures\n\nmessage(\"Setup complete (packages loaded: \", paste(pkgs, collapse = \", \"), \").\")\n\nSetup complete (packages loaded: tidyverse, moments, knitr, kableExtra).",
    "crumbs": [
      "Home",
      "Basics",
      "Lab 1: Probability & Descriptive Statistics"
    ]
  },
  {
    "objectID": "01_introduction.html",
    "href": "01_introduction.html",
    "title": "1 Probability and Data in Business Analytics",
    "section": "",
    "text": "Study Objectives\n\nApply probability rules to real business events.\nDistinguish clearly between population parameters and sample estimators.\nCompute and explain expectation, variance, and standard deviation in business contexts.\nInterpret higher moments: skewness and kurtosis for tail and asymmetry assessment.\nCombine random variables (e.g., portfolio or multi‚Äëchannel forecast) and decompose variance with covariance terms.\nCalculate and interpret covariance and correlation; relate independence vs.¬†zero correlation.\n\n\n\n\nProbability is the foundation for making data-driven decisions in business. This section covers the essential concepts, formulas, and business examples you‚Äôll use in analytics.\n\n\nProbability measures how likely an event is.\n\\[P(A) = \\frac{\\text{Number of ways A can occur}}{\\text{Total possible outcomes}}\\]\n\n\\(P(A) \\geq 0\\) for any event \\(A\\)\n\\(P(\\text{all possible outcomes}) = 1\\)\n\\(P(A \\text{ or } B) = P(A) + P(B) - P(A\\text{ and }B)\\)\nIf \\(A\\) and \\(B\\) are mutually exclusive, then \\[\nP(A \\text{ and } B) = 0\n\\] and \\[\nP(A \\text{ or } B) = P(A) + P(B)\n\\] Mutually exclusive means ‚Äúcannot happen together.‚Äù\n\n\nExample 1 A retailer estimates a 30% chance of a supply chain disruption and a 50% chance of a demand spike. If these events are mutually exclusive, what is the probability of either event occurring?\n\n\nSolution\n\n\nSolution 1. Since the events are mutually exclusive, \\(P(A \\text{ and } B)=0,\\) then \\[\n\\begin{split}\nP(A \\text{ and } B)=P(A)+P(B)-P(A \\text{ and } B)=0.30+0.50=0.80.\n\\end{split}\n\\]\n\n\nExample 2 In a multi-channel marketing campaign, 35% of customers click an email offer (Event A) and 25% engage with a social media ad (Event B). Data shows 10% of customers do both. What is the probability a randomly selected customer engages with at least one of the two channels (email OR social)?\n\n\nSolution\n\n\nSolution 2. \n\\(P(A) + P(B) - P(A \\text{ and } B) = 0.35 + 0.25 - 0.10 = 0.50\\)\n\n\n\n\nIn business, we use samples to estimate population characteristics.\nSample mean:\n\\[\n\\overline{X} = \\frac{1}{n} \\sum_{i=1}^n X_i\n\\]\n\nExample 3 A telecom company surveys a randomly selected sample of 500 customers about service quality. The sample mean satisfaction score is 4.2. How can this estimate help guide company-wide improvements?\n\n\nSolution\n\n\nSolution 3. \n\nThe sample mean (4.2) is based on 500 surveyed customers (the sample).\nThe population is all customers of the telecom company.\nTo guide company-wide improvements, we need to know if the sample is representative of the population.\nThere is uncertainty in the estimate; larger samples make the sample mean closer to the true population mean.\nThe sample mean is useful, but its reliability depends on sample representativeness and size.\n\n\n\nExample 4 Why is it important to distinguish between sample and population in business analytics?\n\n\nSolution\n\n\nSolution 4. \nBecause we use samples to estimate population parameters, and understanding the difference helps us make better decisions and avoid bias.\n\n\n\n\n\n\n\nExpectation (population): Average level of the outcome (true but unknown in practice). \\[\n\\mathbb{E}[X] =\n\\begin{cases}\n\\displaystyle \\sum_{i} x_i \\, P(X = x_i) & \\text{(discrete)} \\\\\n\\displaystyle \\int_{-\\infty}^{\\infty} x \\, f(x) \\, dx & \\text{(continuous)} \\\\\n\\end{cases}\n\\] Sample estimator: Replace the distribution by observed data: \\[\\bar{X} = \\dfrac{1}{n}\\sum_{i=1}^n X_i.\\] The sample mean \\(\\bar{X}\\) estimates the population mean \\(\\mathbb{E}[X]\\).\nVariance (population): How much values fluctuate around the true mean.\n\\[\\text{Var}(X) = E[(X - E[X])^2]\\]\nThe variance can also be expressed as:\n\\[\n\\begin{aligned}\n\\color{#008B45FF}{\\text{Var}(X)} &= \\mathbb{E}\\left[\\left(X - \\mathbb{E}[X]\\right)^2\\right] \\\\\n&= \\color{#008B45FF}{\\mathbb{E}[X^2] - (\\mathbb{E}[X])^2}.\n\\end{aligned}\n\\]\nThis second form is often faster to compute when you already have (or can easily get) \\(\\mathbb{E}[X^2]\\) (e.g., from grouped data or a probability model). In finance, we frequently estimate variance from simulated or historical returns by computing the average of squared returns (giving \\(\\mathbb{E}[X^2]\\)) and subtracting the square of the average return.\nSample estimators:\n\nBiased (population style) estimator: \\[ \\hat{\\sigma}^2 = \\dfrac{1}{n}\\sum_{i=1}^n (X_i - \\bar{X})^2 \\]\nUnbiased estimator: \\[s^2 = \\dfrac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X})^2 \\] The latter (\\(s^2\\)) subtracts 1 from \\(n\\) in the denominator, which is known as a degrees of freedom correction. This version has some desirable properties but we will not discuss these for now. Suffice to say that both versions are usually fine.\n\n\n\nExample 5 An operations planner models next-hour demand arrivals for a micro-warehouse (number of urgent orders) as \\(X \\in \\{0,1,2\\}\\) with probabilities 0.2, 0.5, 0.3. Compute the variability (variance) of order arrivals using the shortcut formula \\(\\mathbb{E}[X^2] - (\\mathbb{E}[X])^2\\) to inform staffing.\n\n\nSolution\n\n\nSolution 5. \\[\n\\begin{split}\n\\mathbb{E}[X] &= 0(0.2)+1(0.5)+2(0.3)=1.1 \\\\\n\\mathbb{E}[X^2] &= 0^2(0.2)+1^2(0.5)+2^2(0.3)=0+0.5+1.2=1.7 \\\\\n\\text{Var}(X) &= \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 = 1.7 - 1.1^2 = 1.7 - 1.21 = 0.49\n\\end{split}\n\\]\n\n\nExample 6 A SaaS company tracks daily change in Daily Active Users (DAU) (in %) over 5 days: \\(0.4\\), \\(0.6\\), \\(-0.2\\), \\(0.5\\), \\(0.7\\). Verify that the direct variance definition and the shortcut formula match; interpret what the variance implies for short‚Äëterm engagement volatility.\n\n\nSolution\n\n\nSolution 6. Daily % changes: \\(0.4,\\ 0.6,\\ -0.2,\\ 0.5,\\ 0.7\\). Let units be percentage points (pp). \\[\n\\mathbb{E}[X] = \\frac{0.4+0.6-0.2+0.5+0.7}{5} = 0.4 \\text{ pp}\n\\] \\[\n\\mathbb{E}[X^2] = \\frac{0.16+0.36+0.04+0.25+0.49}{5} = \\frac{1.30}{5} = 0.26 \\text{ (pp)}^2\n\\] Variance (percentage-point squared): \\[\n\\text{Var}(X) = 0.26 - (0.4)^2 = 0.26 - 0.16 = 0.10 \\text{ (pp)}^2\n\\] Standard deviation \\(= \\sqrt{0.10} \\approx 0.316\\) pp (about \\(0.32\\) percentage points). Direct (long) method matches (0.10).\nInterpretation: Moderate short-term volatility compared to the mean change of 0.4 pp.¬†This suggests some fluctuations in user engagement, but not extreme.\n\n\nStandard Deviation (population): Square root of variance, showing average deviation from the true mean.\n\\[\\text{sd}(X) = \\sqrt{\\text{Var}(X)}\\]\nSample estimator: \\(s = \\sqrt{s^2}\\) (using the unbiased \\(s^2\\) above).\n\n\nExample 7 A hedge fund analyzes daily returns of two portfolios. Portfolio A has higher mean but also higher variance. How should risk-adjusted performance be compared?\n\n\nSolution\n\n\nSolution 7. \nCompare risk-adjusted metrics (e.g., Sharpe ratio = (mean - rf)/sd). Higher mean with disproportionate variance may yield lower Sharpe.\n\n\nExample 8 A business has daily profits of $200, $250, $180, $220, and $210. Calculate the mean and variance.\n\n\nSolution\n\n\nSolution 8. Mean: \\(\\bar{x}=\\dfrac{200+250+180+220+210}{5}=\\dfrac{1060}{5}=212\\).\nVariance calculations:\n\nSum of squared deviations\n\\[\n  \\begin{aligned}\n  (200-212)^2 &+ (250-212)^2 + (180-212)^2 + (220-212)^2 + (210-212)^2 \\\\\n  &= 144 + 1444 + 1024 + 64 + 4 = 2680\n  \\end{aligned}\n  \\]\nPopulation variance (divide by \\(n=5\\))\n\\[ \\sigma^2 = \\frac{2680}{5} = 536 \\]\nSample variance (unbiased, divide by \\(n-1=4\\))\n\\[ s^2 = \\frac{2680}{4} = 670 \\]\n\nPopulation variance uses \\(n\\) when treating these 5 observations as the entire population; sample variance uses \\(n-1\\) to unbiasedly estimate \\(\\text{Var}(X)\\) of a larger process.\n\n\n\n\nPopulation linearity of expectation: \\[E[X + Y] = E[X] + E[Y]\\]\nSample counterpart: \\(\\overline{X+Y} = \\bar{X} + \\bar{Y}\\) (sample means add componentwise).\nIf \\(X\\) and \\(Y\\) are independent, the population variance of their sum is the sum of their variances: \\[\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\]\nIf not independent, add twice the covariance: \\[\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y) + 2\\text{Cov}(X, Y)\\]\nSample counterpart (unbiased style): \\[s_{X+Y}^2 = s_X^2 + s_Y^2 + 2 s_{XY},\\] where \\(s_{XY} = \\dfrac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar{X})(Y_i-\\bar{Y}).\\)\nIntuitive Example: In portfolio management, the total risk (variance) of a portfolio depends on the variances of individual assets and how they move together (covariance).\n\nExample 9 Two independent projects have profit variances of 100 and 150. What is the variance of total profit?\n\n\nSolution\n\n\nSolution 9. \n\\(100 + 150 = 250\\).\n\n\n\nThe variance operator is the expectation of a quadratic function, so it is not linear. Instead, for constants \\(a, b\\):\n\\[\n\\text{Var}(a + bX) = b^2\\,\\text{Var}(X).\n\\]\nMore generally, for constants \\(a_i \\in \\mathbb{R}, i=1,\\ldots,n\\):\n\\[\n\\text{Var}\\left(\\sum_{i=1}^n a_i X_i \\right) = \\sum_{i=1}^n a_i^2 \\, \\text{Var}(X_i) + \\sum_{i=1}^n \\sum_{j=1}^n a_i a_j \\, \\text{Cov}(X_i, X_j).\n\\]\nIf the \\(X_i\\) are pairwise independent (or uncorrelated), the double sum of covariance terms drops out for \\(i\\neq j\\), leaving only the first sum.\n\nExample 10 A portfolio allocates 40% to Asset A and 60% to Asset B. \\(\\text{Var}(A)=0.04\\), \\(\\text{Var}(B)=0.09\\), and \\(\\text{Cov}(A,B)=0.01\\). Compute portfolio variance using the general formula.\n\n\nSolution\n\n\nSolution 10. \n\\(\\text{Var}(P)=0.4^2(0.04)+0.6^2(0.09)+2(0.4)(0.6)(0.01)=0.0064+0.0324+0.0048=0.0436\\).\n\n\nExample 11 A production planner combines forecasts: Final forecast \\(F = 2 + 0.7X_1 + 0.3X_2\\). If \\(\\text{Var}(X_1)=25\\), \\(\\text{Var}(X_2)=9\\), and \\(\\text{Cov}(X_1,X_2)=6\\), find \\(\\text{Var}(F)\\).\n\n\nSolution\n\n\nSolution 11. \nConstant 2 adds no variance. \\(\\text{Var}(F)=0.7^2(25)+0.3^2(9)+2(0.7)(0.3)(6)=12.25+0.81+2.52=15.58\\).\n\n\n\n\n\nSkewness measures asymmetry in a distribution.\n\nSkewness = 0: Symmetric (e.g., normal)\nSkewness &lt; 0: Longer left tail (large losses more likely than large gains)\nSkewness &gt; 0: Longer right tail (occasional big gains)\n\n\n\n\n\n\n\n\n\nFigure¬†1: Diagram of Skewness.\n\n\n\n\n\nBusiness reading: Positive skew in quarterly sales means you usually hit average numbers but sometimes land a very big contract. Negative skew in operational losses could mean rare but severe downside events that need contingency planning.\nLet \\(\\mu_3 = E[(X-\\mathbb{E}[X])^3]\\) denote the third central moment. Standardized (population) skewness: \\[\\gamma_1 = \\frac{\\mu_3}{\\text{sd}(X)^3}.\\]\nSample (bias‚Äëadjusted) skewness estimator: \\[\\hat{\\gamma}_1 = \\frac{n}{(n-1)(n-2)} \\sum_{i=1}^n \\left(\\frac{X_i-\\bar{X}}{s}\\right)^3.\\]\n\n\n\n\n\n\nTip\n\n\n\nNote for skewness and kurtosis, it is NOT expected to hand calculate the values, the focus is on\n\nthe interpretation of given results.\nbe able to recognize skewness and kurtosis in data visualizations (e.g., histograms, box plots).\n\n\n\n\n\n\nKurtosis measures tail weight (propensity for outliers) relative to a normal distribution.\n\nKurtosis ‚âà 3: Normal tail thickness\nKurtosis &gt; 3 (leptokurtic): Heavy / fat tails, more extreme outcomes\nKurtosis &lt; 3 (platykurtic): Light / thin tails, fewer extremes\n\n\n\n\n\n\n\n\n\nFigure¬†2: Examples of heavy-tailed distributions.\n\n\n\n\n\n\\(t\\)-distribution has higher kurtosis than normal distributions.\n\nMeaning that \\(t\\)-distribution has a higher probability of obtaining values that are far from the mean than a normal distribution.\nIt is less peaked in the center and higher in the tails than normal distribution.\nAs the degree of freedom increases, \\(t\\)-distribution approximates to normal distribution, kurtosis decreases and approximates to 3.\n\n\n\n\n\n\n\n\n\nFigure¬†3: The comparison between the t-distribution and the normal distribution at degrees of freedom ranging from 1 to 50. Figure source: from T.J. Kyner.\n\n\n\n\n\n\n\n\n\n\n\nFor the \\(t\\)-distribution:\n\n\n\n\nAs the degrees of freedom (df) increase, the \\(t\\)-distribution approaches the normal distribution.\nA common rule of thumb is that for \\(df &gt; 30\\), one can pretty safely use the normal distribution in place of a t-distribution unless you are interested in the extreme tails.\n\n\n\nLet \\(\\mu_4 = E[(X-\\mathbb{E}[X])^4]\\) be the fourth central moment. Standardized (population) kurtosis: \\[\\kappa = \\frac{\\mu_4}{\\text{sd}(X)^4}, \\qquad \\text{Excess kurtosis} = \\kappa - 3.\\] Sample (Fisher) excess kurtosis estimator: \\[\\hat{g}_2 = \\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\sum_{i=1}^n \\left(\\frac{X_i-\\bar{X}}{s}\\right)^4 - \\frac{3(n-1)^2}{(n-2)(n-3)}.\\] High excess kurtosis signals greater tail risk; low or negative excess indicates fewer extreme outcomes.\nInterpretation link: High positive skewness without high kurtosis means upside spikes with limited extra tail risk; high kurtosis (regardless of skew sign) means fatter tails and a need to focus on outlier management.\n\nExample 12 A company‚Äôs quarterly profits have excess kurtosis of 4. What does this mean for risk management?\n\n\nSolution\n\n\nSolution 12. Excess kurtosis is defined relative to a normal distribution (which has kurtosis 3).\n\nExcess kurtosis = Kurtosis ‚àí 3\nSo an excess kurtosis of 4 means the total kurtosis is 7, much higher than a normal distribution.\n\nThis implies the distribution of quarterly profits has fat tails ‚Äî extreme profit or loss outcomes are more likely than under a normal distribution. Recommend to focus risk management on outliers.\n\n\n\n\n\nPopulation covariance measures the direction of comovement between two variables:\n\nPositive covariance: Both variables increase or decrease together\nNegative covariance: One increases while the other decreases\nMagnitude: Does not indicate strength of relationship\n\nPopulation formulas: \\[\\text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])]\\] \\[\\text{Cov}(X, Y) = E[XY] - E[X]E[Y]\\]\nSample covariance (unbiased): \\[s_{XY} = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y}).\\]\nPopulation correlation standardizes covariance to a scale from -1 to 1: \\[\\text{Corr}(X, Y) = \\frac{\\text{Cov}(X, Y)}{\\text{sd}(X) \\cdot \\text{sd}(Y)}\\]\nSample correlation: \\[r_{XY} = \\frac{s_{XY}}{s_X s_Y}.\\]\n\nCorrelation = 1: Perfect positive relationship\nCorrelation = \\(-1\\): Perfect negative relationship\nCorrelation = 0: No linear relationship\n\nIndependence means knowing one event doesn‚Äôt affect the other: \\[P(A \\text{ and } B) = P(A) \\cdot P(B)\\]\n\n\n\n\n\n\nIndependence vs.¬†Correlation\n\n\n\nIndependence means no influence between events, while correlation measures linear relationship strength.\nIndependence implies zero correlation, but zero correlation does not imply independence.\nE.g., \\(X\\) and \\(Y = X^2\\) have zero correlation but are not independent.\n\n\nBusiness Example: Suppose a retailer finds that sales and weather are correlated. This information can be used to adjust inventory planning for seasonal effects. If two marketing campaigns are independent, the outcome of one does not affect the other.\n\nExample 13 Suppose a customer visits a supermarket. Let:\n\n\\(A\\) = ‚Äúcustomer buys a loaf of bread today‚Äù and \\(P(A)=30\\%\\).\n\\(B\\) = ‚Äúcustomer buys laundry detergent today‚Äù and \\(P(B)=10\\%\\).\n\nWhat is the probability the customer buys both bread and detergent today? What does observing a bread purchase tell you about \\(P(B)\\)?\n\n\nSolution\n\n\nSolution 13. Bread (frequent staple) and detergent (infrequent refill) decisions are unrelated, so it is reasonable to assume independence. Hence \\[\nP(A\\text{ and }B) = 30\\% \\times 10\\% = 3\\%\n\\] Observing bread does not change \\(P(B)=10\\%\\).\n\n\nExample 14 If the correlation between two stocks is \\(-0.8,\\) what does this mean for portfolio diversification?\n\n\nSolution\n\n\nSolution 14. \nThey tend to move in opposite directions, aiding diversification and reducing portfolio variance.\n\n\n\n\n\n\n\nMutually Exclusive vs Independence\n\n\n\nMutually exclusive: Cannot happen together \\[P(A\\cap B)=0\\]\nIndependent: Knowledge of one does not change probability of the other \\[P(A\\cap B)=P(A)P(B)\\]\nThink: Disjoint = ‚Äúnever together‚Äù; Independent = ‚Äúno influence‚Äù.\n\n\n\n\n\nWhen analysing data we distinguish between:\n\nPopulation parameters (target): Fixed (but usually unknown) numerical characteristics of the underlying process (e.g., \\(\\mathbb{E}[X], \\text{Var}(X), \\gamma_1, \\kappa, \\text{Cov}(X,Y)\\)).\nSample statistics (estimators): Functions of observed data used to approximate population parameters (e.g., \\(\\bar{X}, s^2, s, \\hat{\\gamma}_1, \\hat{g}_2, s_{XY}, r_{XY}\\)). They vary from sample to sample.\n\nKey principles:\n\nReplace integrals / probability-weighted sums with empirical averages.\nCenter around the sample mean when the population mean is unknown.\nUse \\(n-1\\) (unbiased) denominators for second moments (variance, covariance) when estimating from an i.i.d. sample.\nAlways interpret sample statistics as estimates with sampling variability; risk management and forecasting should account for estimation error (e.g., standard deviation and confidence intervals).\n\n\n\n\n\n\n\n\n\n\n\n\nConcept\nPopulation Symbol / Definition\nSample Estimator\nNotes\n\n\n\n\nMean\n\\(\\mathbb{E}[X]\\)\n\\(\\bar{X}=\\tfrac{1}{n}\\sum X_i\\)\nUnbiased for mean (i.i.d.)\n\n\nVariance\n\\(\\text{Var}(X)=E[(X-\\mathbb{E}[X])^2]\\)\n\\(s^2=\\tfrac{1}{n-1}\\sum (X_i-\\bar{X})^2\\)\n\\(s_n^2\\) (divide by \\(n\\)) is biased low\n\n\nStd. Dev.\n\\(\\text{sd}(X)=\\sqrt{\\text{Var}(X)}\\)\n\\(s=\\sqrt{s^2}\\)\nPlug-in\n\n\nSkewness\n\\(\\gamma_1=\\mu_3/\\text{sd}^3\\), \\(\\mu_3=E[(X-\\mathbb{E}[X])^3]\\)\n\\(\\hat{\\gamma}_1=\\frac{n}{(n-1)(n-2)}\\sum (\\frac{X_i-\\bar{X}}{s})^3\\)\nMeasures asymmetry\n\n\nKurtosis (excess)\n\\(\\kappa-3\\), \\(\\kappa=\\mu_4/\\text{sd}^4\\)\n\\(\\hat{g}_2\\) (Fisher)\nTail heaviness vs.¬†normal\n\n\nCovariance\n\\(\\text{Cov}(X,Y)=E[(X-\\mathbb{E}X)(Y-\\mathbb{E}Y)]\\)\n\\(s_{XY}=\\tfrac{1}{n-1}\\sum (X_i-\\bar{X})(Y_i-\\bar{Y})\\)\nSign = direction\n\n\nCorrelation\n\\(\\rho=\\dfrac{\\text{Cov}(X,Y)}{\\text{sd}(X)\\text{sd}(Y)}\\)\n\\(r_{XY}=\\dfrac{s_{XY}}{s_X s_Y}\\)\nScale-free ( -1 to 1 )\n\n\n\n\n\n\n\n\n\nInterpretation tip\n\n\n\nIf a sample statistic looks extreme (e.g., very high skewness or kurtosis), examine sample size and outliers; small samples amplify noise in higher-moment estimates.",
    "crumbs": [
      "Home",
      "Basics",
      "Probability and Data in Business Analytics"
    ]
  },
  {
    "objectID": "01_introduction.html#probability-axioms",
    "href": "01_introduction.html#probability-axioms",
    "title": "1 Probability and Data in Business Analytics",
    "section": "",
    "text": "Probability measures how likely an event is.\n\\[P(A) = \\frac{\\text{Number of ways A can occur}}{\\text{Total possible outcomes}}\\]\n\n\\(P(A) \\geq 0\\) for any event \\(A\\)\n\\(P(\\text{all possible outcomes}) = 1\\)\n\\(P(A \\text{ or } B) = P(A) + P(B) - P(A\\text{ and }B)\\)\nIf \\(A\\) and \\(B\\) are mutually exclusive, then \\[\nP(A \\text{ and } B) = 0\n\\] and \\[\nP(A \\text{ or } B) = P(A) + P(B)\n\\] Mutually exclusive means ‚Äúcannot happen together.‚Äù\n\n\nExample 1 A retailer estimates a 30% chance of a supply chain disruption and a 50% chance of a demand spike. If these events are mutually exclusive, what is the probability of either event occurring?\n\n\nSolution\n\n\nSolution 1. Since the events are mutually exclusive, \\(P(A \\text{ and } B)=0,\\) then \\[\n\\begin{split}\nP(A \\text{ and } B)=P(A)+P(B)-P(A \\text{ and } B)=0.30+0.50=0.80.\n\\end{split}\n\\]\n\n\nExample 2 In a multi-channel marketing campaign, 35% of customers click an email offer (Event A) and 25% engage with a social media ad (Event B). Data shows 10% of customers do both. What is the probability a randomly selected customer engages with at least one of the two channels (email OR social)?\n\n\nSolution\n\n\nSolution 2. \n\\(P(A) + P(B) - P(A \\text{ and } B) = 0.35 + 0.25 - 0.10 = 0.50\\)",
    "crumbs": [
      "Home",
      "Basics",
      "Probability and Data in Business Analytics"
    ]
  },
  {
    "objectID": "01_introduction.html#sample-vs.-population-estimating-the-big-picture",
    "href": "01_introduction.html#sample-vs.-population-estimating-the-big-picture",
    "title": "1 Probability and Data in Business Analytics",
    "section": "",
    "text": "In business, we use samples to estimate population characteristics.\nSample mean:\n\\[\n\\overline{X} = \\frac{1}{n} \\sum_{i=1}^n X_i\n\\]\n\nExample 3 A telecom company surveys a randomly selected sample of 500 customers about service quality. The sample mean satisfaction score is 4.2. How can this estimate help guide company-wide improvements?\n\n\nSolution\n\n\nSolution 3. \n\nThe sample mean (4.2) is based on 500 surveyed customers (the sample).\nThe population is all customers of the telecom company.\nTo guide company-wide improvements, we need to know if the sample is representative of the population.\nThere is uncertainty in the estimate; larger samples make the sample mean closer to the true population mean.\nThe sample mean is useful, but its reliability depends on sample representativeness and size.\n\n\n\nExample 4 Why is it important to distinguish between sample and population in business analytics?\n\n\nSolution\n\n\nSolution 4. \nBecause we use samples to estimate population parameters, and understanding the difference helps us make better decisions and avoid bias.",
    "crumbs": [
      "Home",
      "Basics",
      "Probability and Data in Business Analytics"
    ]
  },
  {
    "objectID": "01_introduction.html#moments",
    "href": "01_introduction.html#moments",
    "title": "1 Probability and Data in Business Analytics",
    "section": "",
    "text": "Expectation (population): Average level of the outcome (true but unknown in practice). \\[\n\\mathbb{E}[X] =\n\\begin{cases}\n\\displaystyle \\sum_{i} x_i \\, P(X = x_i) & \\text{(discrete)} \\\\\n\\displaystyle \\int_{-\\infty}^{\\infty} x \\, f(x) \\, dx & \\text{(continuous)} \\\\\n\\end{cases}\n\\] Sample estimator: Replace the distribution by observed data: \\[\\bar{X} = \\dfrac{1}{n}\\sum_{i=1}^n X_i.\\] The sample mean \\(\\bar{X}\\) estimates the population mean \\(\\mathbb{E}[X]\\).\nVariance (population): How much values fluctuate around the true mean.\n\\[\\text{Var}(X) = E[(X - E[X])^2]\\]\nThe variance can also be expressed as:\n\\[\n\\begin{aligned}\n\\color{#008B45FF}{\\text{Var}(X)} &= \\mathbb{E}\\left[\\left(X - \\mathbb{E}[X]\\right)^2\\right] \\\\\n&= \\color{#008B45FF}{\\mathbb{E}[X^2] - (\\mathbb{E}[X])^2}.\n\\end{aligned}\n\\]\nThis second form is often faster to compute when you already have (or can easily get) \\(\\mathbb{E}[X^2]\\) (e.g., from grouped data or a probability model). In finance, we frequently estimate variance from simulated or historical returns by computing the average of squared returns (giving \\(\\mathbb{E}[X^2]\\)) and subtracting the square of the average return.\nSample estimators:\n\nBiased (population style) estimator: \\[ \\hat{\\sigma}^2 = \\dfrac{1}{n}\\sum_{i=1}^n (X_i - \\bar{X})^2 \\]\nUnbiased estimator: \\[s^2 = \\dfrac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X})^2 \\] The latter (\\(s^2\\)) subtracts 1 from \\(n\\) in the denominator, which is known as a degrees of freedom correction. This version has some desirable properties but we will not discuss these for now. Suffice to say that both versions are usually fine.\n\n\n\nExample 5 An operations planner models next-hour demand arrivals for a micro-warehouse (number of urgent orders) as \\(X \\in \\{0,1,2\\}\\) with probabilities 0.2, 0.5, 0.3. Compute the variability (variance) of order arrivals using the shortcut formula \\(\\mathbb{E}[X^2] - (\\mathbb{E}[X])^2\\) to inform staffing.\n\n\nSolution\n\n\nSolution 5. \\[\n\\begin{split}\n\\mathbb{E}[X] &= 0(0.2)+1(0.5)+2(0.3)=1.1 \\\\\n\\mathbb{E}[X^2] &= 0^2(0.2)+1^2(0.5)+2^2(0.3)=0+0.5+1.2=1.7 \\\\\n\\text{Var}(X) &= \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 = 1.7 - 1.1^2 = 1.7 - 1.21 = 0.49\n\\end{split}\n\\]\n\n\nExample 6 A SaaS company tracks daily change in Daily Active Users (DAU) (in %) over 5 days: \\(0.4\\), \\(0.6\\), \\(-0.2\\), \\(0.5\\), \\(0.7\\). Verify that the direct variance definition and the shortcut formula match; interpret what the variance implies for short‚Äëterm engagement volatility.\n\n\nSolution\n\n\nSolution 6. Daily % changes: \\(0.4,\\ 0.6,\\ -0.2,\\ 0.5,\\ 0.7\\). Let units be percentage points (pp). \\[\n\\mathbb{E}[X] = \\frac{0.4+0.6-0.2+0.5+0.7}{5} = 0.4 \\text{ pp}\n\\] \\[\n\\mathbb{E}[X^2] = \\frac{0.16+0.36+0.04+0.25+0.49}{5} = \\frac{1.30}{5} = 0.26 \\text{ (pp)}^2\n\\] Variance (percentage-point squared): \\[\n\\text{Var}(X) = 0.26 - (0.4)^2 = 0.26 - 0.16 = 0.10 \\text{ (pp)}^2\n\\] Standard deviation \\(= \\sqrt{0.10} \\approx 0.316\\) pp (about \\(0.32\\) percentage points). Direct (long) method matches (0.10).\nInterpretation: Moderate short-term volatility compared to the mean change of 0.4 pp.¬†This suggests some fluctuations in user engagement, but not extreme.\n\n\nStandard Deviation (population): Square root of variance, showing average deviation from the true mean.\n\\[\\text{sd}(X) = \\sqrt{\\text{Var}(X)}\\]\nSample estimator: \\(s = \\sqrt{s^2}\\) (using the unbiased \\(s^2\\) above).\n\n\nExample 7 A hedge fund analyzes daily returns of two portfolios. Portfolio A has higher mean but also higher variance. How should risk-adjusted performance be compared?\n\n\nSolution\n\n\nSolution 7. \nCompare risk-adjusted metrics (e.g., Sharpe ratio = (mean - rf)/sd). Higher mean with disproportionate variance may yield lower Sharpe.\n\n\nExample 8 A business has daily profits of $200, $250, $180, $220, and $210. Calculate the mean and variance.\n\n\nSolution\n\n\nSolution 8. Mean: \\(\\bar{x}=\\dfrac{200+250+180+220+210}{5}=\\dfrac{1060}{5}=212\\).\nVariance calculations:\n\nSum of squared deviations\n\\[\n  \\begin{aligned}\n  (200-212)^2 &+ (250-212)^2 + (180-212)^2 + (220-212)^2 + (210-212)^2 \\\\\n  &= 144 + 1444 + 1024 + 64 + 4 = 2680\n  \\end{aligned}\n  \\]\nPopulation variance (divide by \\(n=5\\))\n\\[ \\sigma^2 = \\frac{2680}{5} = 536 \\]\nSample variance (unbiased, divide by \\(n-1=4\\))\n\\[ s^2 = \\frac{2680}{4} = 670 \\]\n\nPopulation variance uses \\(n\\) when treating these 5 observations as the entire population; sample variance uses \\(n-1\\) to unbiasedly estimate \\(\\text{Var}(X)\\) of a larger process.\n\n\n\n\nPopulation linearity of expectation: \\[E[X + Y] = E[X] + E[Y]\\]\nSample counterpart: \\(\\overline{X+Y} = \\bar{X} + \\bar{Y}\\) (sample means add componentwise).\nIf \\(X\\) and \\(Y\\) are independent, the population variance of their sum is the sum of their variances: \\[\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\]\nIf not independent, add twice the covariance: \\[\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y) + 2\\text{Cov}(X, Y)\\]\nSample counterpart (unbiased style): \\[s_{X+Y}^2 = s_X^2 + s_Y^2 + 2 s_{XY},\\] where \\(s_{XY} = \\dfrac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar{X})(Y_i-\\bar{Y}).\\)\nIntuitive Example: In portfolio management, the total risk (variance) of a portfolio depends on the variances of individual assets and how they move together (covariance).\n\nExample 9 Two independent projects have profit variances of 100 and 150. What is the variance of total profit?\n\n\nSolution\n\n\nSolution 9. \n\\(100 + 150 = 250\\).\n\n\n\nThe variance operator is the expectation of a quadratic function, so it is not linear. Instead, for constants \\(a, b\\):\n\\[\n\\text{Var}(a + bX) = b^2\\,\\text{Var}(X).\n\\]\nMore generally, for constants \\(a_i \\in \\mathbb{R}, i=1,\\ldots,n\\):\n\\[\n\\text{Var}\\left(\\sum_{i=1}^n a_i X_i \\right) = \\sum_{i=1}^n a_i^2 \\, \\text{Var}(X_i) + \\sum_{i=1}^n \\sum_{j=1}^n a_i a_j \\, \\text{Cov}(X_i, X_j).\n\\]\nIf the \\(X_i\\) are pairwise independent (or uncorrelated), the double sum of covariance terms drops out for \\(i\\neq j\\), leaving only the first sum.\n\nExample 10 A portfolio allocates 40% to Asset A and 60% to Asset B. \\(\\text{Var}(A)=0.04\\), \\(\\text{Var}(B)=0.09\\), and \\(\\text{Cov}(A,B)=0.01\\). Compute portfolio variance using the general formula.\n\n\nSolution\n\n\nSolution 10. \n\\(\\text{Var}(P)=0.4^2(0.04)+0.6^2(0.09)+2(0.4)(0.6)(0.01)=0.0064+0.0324+0.0048=0.0436\\).\n\n\nExample 11 A production planner combines forecasts: Final forecast \\(F = 2 + 0.7X_1 + 0.3X_2\\). If \\(\\text{Var}(X_1)=25\\), \\(\\text{Var}(X_2)=9\\), and \\(\\text{Cov}(X_1,X_2)=6\\), find \\(\\text{Var}(F)\\).\n\n\nSolution\n\n\nSolution 11. \nConstant 2 adds no variance. \\(\\text{Var}(F)=0.7^2(25)+0.3^2(9)+2(0.7)(0.3)(6)=12.25+0.81+2.52=15.58\\).\n\n\n\n\n\nSkewness measures asymmetry in a distribution.\n\nSkewness = 0: Symmetric (e.g., normal)\nSkewness &lt; 0: Longer left tail (large losses more likely than large gains)\nSkewness &gt; 0: Longer right tail (occasional big gains)\n\n\n\n\n\n\n\n\n\nFigure¬†1: Diagram of Skewness.\n\n\n\n\n\nBusiness reading: Positive skew in quarterly sales means you usually hit average numbers but sometimes land a very big contract. Negative skew in operational losses could mean rare but severe downside events that need contingency planning.\nLet \\(\\mu_3 = E[(X-\\mathbb{E}[X])^3]\\) denote the third central moment. Standardized (population) skewness: \\[\\gamma_1 = \\frac{\\mu_3}{\\text{sd}(X)^3}.\\]\nSample (bias‚Äëadjusted) skewness estimator: \\[\\hat{\\gamma}_1 = \\frac{n}{(n-1)(n-2)} \\sum_{i=1}^n \\left(\\frac{X_i-\\bar{X}}{s}\\right)^3.\\]\n\n\n\n\n\n\nTip\n\n\n\nNote for skewness and kurtosis, it is NOT expected to hand calculate the values, the focus is on\n\nthe interpretation of given results.\nbe able to recognize skewness and kurtosis in data visualizations (e.g., histograms, box plots).\n\n\n\n\n\n\nKurtosis measures tail weight (propensity for outliers) relative to a normal distribution.\n\nKurtosis ‚âà 3: Normal tail thickness\nKurtosis &gt; 3 (leptokurtic): Heavy / fat tails, more extreme outcomes\nKurtosis &lt; 3 (platykurtic): Light / thin tails, fewer extremes\n\n\n\n\n\n\n\n\n\nFigure¬†2: Examples of heavy-tailed distributions.\n\n\n\n\n\n\\(t\\)-distribution has higher kurtosis than normal distributions.\n\nMeaning that \\(t\\)-distribution has a higher probability of obtaining values that are far from the mean than a normal distribution.\nIt is less peaked in the center and higher in the tails than normal distribution.\nAs the degree of freedom increases, \\(t\\)-distribution approximates to normal distribution, kurtosis decreases and approximates to 3.\n\n\n\n\n\n\n\n\n\nFigure¬†3: The comparison between the t-distribution and the normal distribution at degrees of freedom ranging from 1 to 50. Figure source: from T.J. Kyner.\n\n\n\n\n\n\n\n\n\n\n\nFor the \\(t\\)-distribution:\n\n\n\n\nAs the degrees of freedom (df) increase, the \\(t\\)-distribution approaches the normal distribution.\nA common rule of thumb is that for \\(df &gt; 30\\), one can pretty safely use the normal distribution in place of a t-distribution unless you are interested in the extreme tails.\n\n\n\nLet \\(\\mu_4 = E[(X-\\mathbb{E}[X])^4]\\) be the fourth central moment. Standardized (population) kurtosis: \\[\\kappa = \\frac{\\mu_4}{\\text{sd}(X)^4}, \\qquad \\text{Excess kurtosis} = \\kappa - 3.\\] Sample (Fisher) excess kurtosis estimator: \\[\\hat{g}_2 = \\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\sum_{i=1}^n \\left(\\frac{X_i-\\bar{X}}{s}\\right)^4 - \\frac{3(n-1)^2}{(n-2)(n-3)}.\\] High excess kurtosis signals greater tail risk; low or negative excess indicates fewer extreme outcomes.\nInterpretation link: High positive skewness without high kurtosis means upside spikes with limited extra tail risk; high kurtosis (regardless of skew sign) means fatter tails and a need to focus on outlier management.\n\nExample 12 A company‚Äôs quarterly profits have excess kurtosis of 4. What does this mean for risk management?\n\n\nSolution\n\n\nSolution 12. Excess kurtosis is defined relative to a normal distribution (which has kurtosis 3).\n\nExcess kurtosis = Kurtosis ‚àí 3\nSo an excess kurtosis of 4 means the total kurtosis is 7, much higher than a normal distribution.\n\nThis implies the distribution of quarterly profits has fat tails ‚Äî extreme profit or loss outcomes are more likely than under a normal distribution. Recommend to focus risk management on outliers.",
    "crumbs": [
      "Home",
      "Basics",
      "Probability and Data in Business Analytics"
    ]
  },
  {
    "objectID": "01_introduction.html#covariance-correlation-independence",
    "href": "01_introduction.html#covariance-correlation-independence",
    "title": "1 Probability and Data in Business Analytics",
    "section": "",
    "text": "Population covariance measures the direction of comovement between two variables:\n\nPositive covariance: Both variables increase or decrease together\nNegative covariance: One increases while the other decreases\nMagnitude: Does not indicate strength of relationship\n\nPopulation formulas: \\[\\text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])]\\] \\[\\text{Cov}(X, Y) = E[XY] - E[X]E[Y]\\]\nSample covariance (unbiased): \\[s_{XY} = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y}).\\]\nPopulation correlation standardizes covariance to a scale from -1 to 1: \\[\\text{Corr}(X, Y) = \\frac{\\text{Cov}(X, Y)}{\\text{sd}(X) \\cdot \\text{sd}(Y)}\\]\nSample correlation: \\[r_{XY} = \\frac{s_{XY}}{s_X s_Y}.\\]\n\nCorrelation = 1: Perfect positive relationship\nCorrelation = \\(-1\\): Perfect negative relationship\nCorrelation = 0: No linear relationship\n\nIndependence means knowing one event doesn‚Äôt affect the other: \\[P(A \\text{ and } B) = P(A) \\cdot P(B)\\]\n\n\n\n\n\n\nIndependence vs.¬†Correlation\n\n\n\nIndependence means no influence between events, while correlation measures linear relationship strength.\nIndependence implies zero correlation, but zero correlation does not imply independence.\nE.g., \\(X\\) and \\(Y = X^2\\) have zero correlation but are not independent.\n\n\nBusiness Example: Suppose a retailer finds that sales and weather are correlated. This information can be used to adjust inventory planning for seasonal effects. If two marketing campaigns are independent, the outcome of one does not affect the other.\n\nExample 13 Suppose a customer visits a supermarket. Let:\n\n\\(A\\) = ‚Äúcustomer buys a loaf of bread today‚Äù and \\(P(A)=30\\%\\).\n\\(B\\) = ‚Äúcustomer buys laundry detergent today‚Äù and \\(P(B)=10\\%\\).\n\nWhat is the probability the customer buys both bread and detergent today? What does observing a bread purchase tell you about \\(P(B)\\)?\n\n\nSolution\n\n\nSolution 13. Bread (frequent staple) and detergent (infrequent refill) decisions are unrelated, so it is reasonable to assume independence. Hence \\[\nP(A\\text{ and }B) = 30\\% \\times 10\\% = 3\\%\n\\] Observing bread does not change \\(P(B)=10\\%\\).\n\n\nExample 14 If the correlation between two stocks is \\(-0.8,\\) what does this mean for portfolio diversification?\n\n\nSolution\n\n\nSolution 14. \nThey tend to move in opposite directions, aiding diversification and reducing portfolio variance.\n\n\n\n\n\n\n\nMutually Exclusive vs Independence\n\n\n\nMutually exclusive: Cannot happen together \\[P(A\\cap B)=0\\]\nIndependent: Knowledge of one does not change probability of the other \\[P(A\\cap B)=P(A)P(B)\\]\nThink: Disjoint = ‚Äúnever together‚Äù; Independent = ‚Äúno influence‚Äù.",
    "crumbs": [
      "Home",
      "Basics",
      "Probability and Data in Business Analytics"
    ]
  },
  {
    "objectID": "01_introduction.html#population-vs.-sample-summary-estimators",
    "href": "01_introduction.html#population-vs.-sample-summary-estimators",
    "title": "1 Probability and Data in Business Analytics",
    "section": "",
    "text": "When analysing data we distinguish between:\n\nPopulation parameters (target): Fixed (but usually unknown) numerical characteristics of the underlying process (e.g., \\(\\mathbb{E}[X], \\text{Var}(X), \\gamma_1, \\kappa, \\text{Cov}(X,Y)\\)).\nSample statistics (estimators): Functions of observed data used to approximate population parameters (e.g., \\(\\bar{X}, s^2, s, \\hat{\\gamma}_1, \\hat{g}_2, s_{XY}, r_{XY}\\)). They vary from sample to sample.\n\nKey principles:\n\nReplace integrals / probability-weighted sums with empirical averages.\nCenter around the sample mean when the population mean is unknown.\nUse \\(n-1\\) (unbiased) denominators for second moments (variance, covariance) when estimating from an i.i.d. sample.\nAlways interpret sample statistics as estimates with sampling variability; risk management and forecasting should account for estimation error (e.g., standard deviation and confidence intervals).\n\n\n\n\n\n\n\n\n\n\n\n\nConcept\nPopulation Symbol / Definition\nSample Estimator\nNotes\n\n\n\n\nMean\n\\(\\mathbb{E}[X]\\)\n\\(\\bar{X}=\\tfrac{1}{n}\\sum X_i\\)\nUnbiased for mean (i.i.d.)\n\n\nVariance\n\\(\\text{Var}(X)=E[(X-\\mathbb{E}[X])^2]\\)\n\\(s^2=\\tfrac{1}{n-1}\\sum (X_i-\\bar{X})^2\\)\n\\(s_n^2\\) (divide by \\(n\\)) is biased low\n\n\nStd. Dev.\n\\(\\text{sd}(X)=\\sqrt{\\text{Var}(X)}\\)\n\\(s=\\sqrt{s^2}\\)\nPlug-in\n\n\nSkewness\n\\(\\gamma_1=\\mu_3/\\text{sd}^3\\), \\(\\mu_3=E[(X-\\mathbb{E}[X])^3]\\)\n\\(\\hat{\\gamma}_1=\\frac{n}{(n-1)(n-2)}\\sum (\\frac{X_i-\\bar{X}}{s})^3\\)\nMeasures asymmetry\n\n\nKurtosis (excess)\n\\(\\kappa-3\\), \\(\\kappa=\\mu_4/\\text{sd}^4\\)\n\\(\\hat{g}_2\\) (Fisher)\nTail heaviness vs.¬†normal\n\n\nCovariance\n\\(\\text{Cov}(X,Y)=E[(X-\\mathbb{E}X)(Y-\\mathbb{E}Y)]\\)\n\\(s_{XY}=\\tfrac{1}{n-1}\\sum (X_i-\\bar{X})(Y_i-\\bar{Y})\\)\nSign = direction\n\n\nCorrelation\n\\(\\rho=\\dfrac{\\text{Cov}(X,Y)}{\\text{sd}(X)\\text{sd}(Y)}\\)\n\\(r_{XY}=\\dfrac{s_{XY}}{s_X s_Y}\\)\nScale-free ( -1 to 1 )\n\n\n\n\n\n\n\n\n\nInterpretation tip\n\n\n\nIf a sample statistic looks extreme (e.g., very high skewness or kurtosis), examine sample size and outliers; small samples amplify noise in higher-moment estimates.",
    "crumbs": [
      "Home",
      "Basics",
      "Probability and Data in Business Analytics"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Business Analytics (Fall 2025)",
    "section": "",
    "text": "What Business Analytics (BA) is about?\nThe aim of this course is to provide students advanced knowledge, skills and competencies they can use to make data driven decisions for organizations.\nData-driven decision making\n\nDescribe the data: what happened\nUsing introductory statistics to identify patterns, trends, and insights embedded in historical data. E.g., summary statistics.\nPredictive: what will happen\nUsing statistical models on historical data and forecast future outcomes. E.g., regression, time series.\nPrescriptive: what should we do\nUsing optimization, simulation, and decision analysis techniques to suggest the best course of action based on data, predictions, and constraints.\nAutonomous: automated decision-making using Machine Learning (ML) and Artificial Intelligence (AI) techniques.\n\nOutline of models and applications we will cover in this course:1\n\n\n\n\n\n\n\n\nCategory\nTopics\nApplications in Business\n\n\n\n\nDescriptive\nData Visualization, Descriptive Analysis\nAsset return ‚Üí Finance\n\n\nPredictive\nLinear regression\nAsset return prediction ‚Üí FinanceDemand prediction ‚Üí Economics\n\n\n\nHypothesis testing\nEffectiveness of advertisements ‚Üí Marketing\n\n\n\nClassification, logistic regression\nAsset return increasing/decreasing prediction ‚Üí FinanceEmployee satisfaction ‚Üí Operation\n\n\nPrescriptive\nOptimization models\nPricing optimization ‚Üí OperationSensitivity analysis, Scenario analysis ‚Üí Accounting\n\n\n\n[1] This is a preliminary plan. Topics and applications are subject to change as the course moves forward.\n\nSoftware: R programming\nR is an open-source programming language widely used for statistical computing and data analysis. It provides a rich ecosystem of packages and libraries for data manipulation, visualization, and modeling.\nWhy open-source stands out in the competition?\nAI is the game changer here. AI significantly improves coding efficiency and productivity. You don‚Äôt need to memorize every function or syntax anymore. The current role for humans is to communicate your needs to AI, and AI will generate the code for you.\n\nOpen-source software integrates cutting-edge AI tools, faster and more efficiently than paid software.\nBy contrast, paid software is often slower to implement new features due to development cycles and licensing restrictions.\n\n Demonstration in VS Code. \nBy typing # create a function taking the n-th power of a number, AI will generate the code for you.\nHere is how to use GitHub Copilot in RStudio: RStudio User Guide: Tools, GitHub Copilot."
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "Business Analytics (Fall 2025)",
    "section": "",
    "text": "What Business Analytics (BA) is about?\nThe aim of this course is to provide students advanced knowledge, skills and competencies they can use to make data driven decisions for organizations.\nData-driven decision making\n\nDescribe the data: what happened\nUsing introductory statistics to identify patterns, trends, and insights embedded in historical data. E.g., summary statistics.\nPredictive: what will happen\nUsing statistical models on historical data and forecast future outcomes. E.g., regression, time series.\nPrescriptive: what should we do\nUsing optimization, simulation, and decision analysis techniques to suggest the best course of action based on data, predictions, and constraints.\nAutonomous: automated decision-making using Machine Learning (ML) and Artificial Intelligence (AI) techniques.\n\nOutline of models and applications we will cover in this course:1\n\n\n\n\n\n\n\n\nCategory\nTopics\nApplications in Business\n\n\n\n\nDescriptive\nData Visualization, Descriptive Analysis\nAsset return ‚Üí Finance\n\n\nPredictive\nLinear regression\nAsset return prediction ‚Üí FinanceDemand prediction ‚Üí Economics\n\n\n\nHypothesis testing\nEffectiveness of advertisements ‚Üí Marketing\n\n\n\nClassification, logistic regression\nAsset return increasing/decreasing prediction ‚Üí FinanceEmployee satisfaction ‚Üí Operation\n\n\nPrescriptive\nOptimization models\nPricing optimization ‚Üí OperationSensitivity analysis, Scenario analysis ‚Üí Accounting\n\n\n\n[1] This is a preliminary plan. Topics and applications are subject to change as the course moves forward.\n\nSoftware: R programming\nR is an open-source programming language widely used for statistical computing and data analysis. It provides a rich ecosystem of packages and libraries for data manipulation, visualization, and modeling.\nWhy open-source stands out in the competition?\nAI is the game changer here. AI significantly improves coding efficiency and productivity. You don‚Äôt need to memorize every function or syntax anymore. The current role for humans is to communicate your needs to AI, and AI will generate the code for you.\n\nOpen-source software integrates cutting-edge AI tools, faster and more efficiently than paid software.\nBy contrast, paid software is often slower to implement new features due to development cycles and licensing restrictions.\n\n Demonstration in VS Code. \nBy typing # create a function taking the n-th power of a number, AI will generate the code for you.\nHere is how to use GitHub Copilot in RStudio: RStudio User Guide: Tools, GitHub Copilot."
  },
  {
    "objectID": "index.html#course-materials",
    "href": "index.html#course-materials",
    "title": "Business Analytics (Fall 2025)",
    "section": "2 Course Materials",
    "text": "2 Course Materials\nThe course materials will be self-contained.\n\nLecture notes: lecture notes will be provided in the course website.\nR code: R code will be provided in form of Lab Jupyter Notebooks.\nYou may copy and paste the code into RStudio to run it.\n\nIf you want to learn in-depth, you can refer to the following textbooks and resources.\nTextbooks\n\nEvans, J.R. (2021) Business analytics: methods, models, and decisions. Third edition. Harlow: Pearson.\nStatistics primer. Basic introduction to business analytics at the undergraduate level.\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An Introduction to Statistical Learning with Applications in R. 2nd edition. Springer. Online version\nMain textbook for the course. It covers the fundamental concepts and methods of statistical learning, with practical applications in R.\nHuntsinger, R. (2025). Business Analytics: Methods and Cases for Data-Driven Decisions. Cambridge: Cambridge University Press. eTextbook available through Cambridge University Press.\nAdvanced methods and case studies for data-driven decision-making. We might use case studies from this book in the course."
  },
  {
    "objectID": "index.html#course-evaluation",
    "href": "index.html#course-evaluation",
    "title": "Business Analytics (Fall 2025)",
    "section": "3 Course Evaluation",
    "text": "3 Course Evaluation\nCompud Assessment\n\nOppgave (home assignment): 40%\nGroup work (1-3 students) is possible; including a case study with data analysis and visualization; a report will be submitted;\nDate: week 43 (preliminary)\nThe assignment will be released in Inspera, and you will have one week to complete it.\nMore details will be provided as the course progresses.\nEksamen (school exam): 60%\nDigitally in Inspera;\nDate: 17.11.2025"
  },
  {
    "objectID": "index.html#study-objectives",
    "href": "index.html#study-objectives",
    "title": "Business Analytics (Fall 2025)",
    "section": "4 Study Objectives",
    "text": "4 Study Objectives\n\nKnowledge:\n\nFamiliarity with statistical methods and models used in business analytics.\nFocus on descriptive analytics and predictive analytics. Prescriptive analytics will be introduced if time allows.\nKnowledge of data visualization techniques and tools.\n\nSkills:\n\nAbility to analyze and interpret data using statistical methods.\nProficiency in using R for data analysis and visualization.\nCompetence in applying business analytics techniques to real-world problems.\n\nCompetencies:\n\nDevelop critical thinking skills to evaluate data-driven decisions.\nAbility to communicate findings effectively through reports and presentations."
  },
  {
    "objectID": "index.html#how-to-reach-me",
    "href": "index.html#how-to-reach-me",
    "title": "Business Analytics (Fall 2025)",
    "section": "5 How to Reach Me",
    "text": "5 How to Reach Me\n\n\n\nInstructor:\n\n\nMenghan Yuan\n\n\n\n\nEmail:\n\n\nmenghan.yuan@nord.no\n\n\n\n\nOffice Hours:\n\n\nBy appointment\n\n\n\n\nOffice:\n\n\nHovedbygning A257"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "01_Lab-1.html#section",
    "href": "01_Lab-1.html#section",
    "title": "Lab 1: Probability & Descriptive Statistics",
    "section": "3 ",
    "text": "3 \n\n# ---- Data generation ----\nskewed_sample &lt;- rlnorm(1000, meanlog = 2, sdlog = 0.5)  # log-normal (positively skewed)\nnormal_sample &lt;- rnorm(1000, mean = 20, sd = 5)\n\ncat('Preview: first 6 rows of skewed (right-tailed) vs approximately normal sample.\\n')\nhead(data.frame(skewed_sample, normal_sample))\n\nPreview: first 6 rows of skewed (right-tailed) vs approximately normal sample.\n\n\n\nA data.frame: 6 √ó 2\n\n\n\nskewed_sample\nnormal_sample\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n16.108614\n19.910099\n\n\n2\n7.654198\n19.339124\n\n\n3\n7.882490\n7.253286\n\n\n4\n17.418494\n25.202867\n\n\n5\n9.304127\n21.248629\n\n\n6\n3.925404\n32.081037\n\n\n\n\n\n\n# ---- Descriptive statistics ----\nsummary_skewed &lt;- quick_summary(skewed_sample)\nsummary_normal &lt;- quick_summary(normal_sample)\n\ncat('Comparison of key statistics: note higher skewness & kurtosis for log-normal sample.\\n')\nbind_rows(\n  skewed = summary_skewed,\n  normal = summary_normal,\n  .id = \"distribution\"\n)\n\nComparison of key statistics: note higher skewness & kurtosis for log-normal sample.\n\n\n\nA data.frame: 2 √ó 7\n\n\ndistribution\nn\nmean\nsd\nvar\nskewness\nkurtosis\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nskewed\n1000\n8.428385\n4.496494\n20.21846\n1.64801605\n7.072802\n\n\nnormal\n1000\n20.221132\n5.043640\n25.43830\n-0.01362832\n2.942717\n\n\n\n\n\n\n# ---- Visualization: Histograms ----\ncombined &lt;- rbind(\n  data.frame(value = skewed_sample, type = \"Skewed (Log-normal)\"),\n  data.frame(value = normal_sample, type = \"Approx. Normal\")\n)\n\noptions(repr.plot.width = 13, repr.plot.height = 6)\n\nggplot(combined, aes(value)) +\n  geom_histogram(bins = 40, fill = \"#3182bd\", alpha = 0.8, color = \"white\") +\n  facet_wrap(~ type, scales = \"free\", nrow = 1) +\n  labs(title = \"Distribution Contrast\", x = \"Value\", y = \"Count\") +\n  theme(panel.spacing.x = unit(1.2, \"lines\"))\n\n\n\n\n\n\n\n\n\n# ---- Covariance & Correlation example ----\n# Suppose revenue depends on marketing spend with noise\nmarketing_spend &lt;- rnorm(100, mean = 50, sd = 10)\nrevenue &lt;- 5 + 1.8 * marketing_spend + rnorm(100, sd = 15)\n\ncat('Covariance & correlation between marketing spend and revenue (positive association expected).\\n')\nsummary_df &lt;- data.frame(cov = cov(marketing_spend, revenue), cor = cor(marketing_spend, revenue))\nsummary_df\n\nCovariance & correlation between marketing spend and revenue (positive association expected).\n\n\n\nA data.frame: 1 √ó 2\n\n\ncov\ncor\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n149.4054\n0.7856426\n\n\n\n\n\n\n# ---- Simulation: Sample vs Population variance ----\ntrue_var &lt;- 25  # sd^2 with sd=5\nsample_sizes &lt;- c(5, 10, 20, 50, 100, 250, 500)\nresults &lt;- lapply(sample_sizes, function(n){\n  x &lt;- rnorm(n, mean = 0, sd = 5)\n  data.frame(n = n, var_n = mean((x - mean(x))^2), var_n1 = var(x))\n}) %&gt;% dplyr::bind_rows()\ncat('Variance estimators: var_n (biased, divide by n) vs var_n1 (unbiased, divide by n-1).\\n')\nresults\n\nVariance estimators: var_n (biased, divide by n) vs var_n1 (unbiased, divide by n-1).\n\n\n\nA data.frame: 7 √ó 3\n\n\nn\nvar_n\nvar_n1\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n5\n15.17886\n18.97357\n\n\n10\n26.34624\n29.27360\n\n\n20\n16.53123\n17.40129\n\n\n50\n21.11229\n21.54315\n\n\n100\n23.98626\n24.22854\n\n\n250\n25.38000\n25.48193\n\n\n500\n25.58307\n25.63434\n\n\n\n\n\n\n# ---- Plot: Convergence of variance estimators ----\nresults_long &lt;- results %&gt;%\n  tidyr::pivot_longer(var_n:var_n1, names_to = \"estimator\", values_to = \"value\") %&gt;%\n  dplyr::mutate(estimator = dplyr::recode(estimator, var_n = \"Divide by n\", var_n1 = \"Divide by n-1\"))\n\noptions(repr.plot.width = 8, repr.plot.height = 6)\n\nggplot(results_long, aes(n, value, color = estimator)) +\n  geom_line() +\n  geom_point() +\n  geom_hline(yintercept = true_var, linetype = \"dashed\") +\n  labs(title = \"Convergence of Variance Estimators\", y = \"Estimated variance\", x = \"Sample size\")\n\n\n\n\n\n\n\n\n\nSampling Distributions of Two Asset Returns\nWe will simulate repeated samples of two (correlated) asset return series and compare the sampling distributions of their sample means and the distribution of the portfolio mean (equal weights).\n\n# ---- Stretch: parameters ----\nset.seed(2025)\nmu &lt;- c(0.0005, 0.0008)          # daily expected returns\nsd_vec &lt;- c(0.02, 0.03)\nrho &lt;- 0.9  # increased to 0.90 to demonstrate strong correlation\nSigma2 &lt;- matrix(c(sd_vec[1]^2, rho*prod(sd_vec), rho*prod(sd_vec), sd_vec[2]^2), 2, 2)\nB &lt;- chol(Sigma2)\n\n\n# ---- Stretch: demonstrate high correlation ----\n# Single large sample to visualize relationship\nN_demo &lt;- 2000\nZ_demo &lt;- matrix(rnorm(2*N_demo), N_demo, 2) %*% B\nX_demo &lt;- sweep(Z_demo, 2, mu, \"+\")\nassetA_demo &lt;- X_demo[,1]\nassetB_demo &lt;- X_demo[,2]\n\nemp_cor &lt;- cor(assetA_demo, assetB_demo)\ncat(sprintf('Empirical correlation from demo sample: %.3f (target rho = %.2f)\\n', emp_cor, rho))\n\noptions(repr.plot.width = 6, repr.plot.height = 5)\n\nggplot(data.frame(assetA_demo, assetB_demo), aes(assetA_demo, assetB_demo)) +\n  geom_point(alpha = 0.35, color = '#1b7837') +\n  geom_smooth(method='lm', se=FALSE, color='#762a83') +\n  labs(title = 'Scatter of Two Asset Returns (High Correlation)', x='Asset A Return', y='Asset B Return',\n       subtitle = paste0('Empirical r = ', round(emp_cor,3)))\n\nEmpirical correlation from demo sample: 0.904 (target rho = 0.90)\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\nReflection\nHow does correlation between assets influence the portfolio mean‚Äôs variability? Write 3‚Äì4 sentences interpreting for a diversified vs concentrated investment decision.",
    "crumbs": [
      "Home",
      "Basics",
      "Lab 1: Probability & Descriptive Statistics"
    ]
  },
  {
    "objectID": "01_Lab-1.html#ind",
    "href": "01_Lab-1.html#ind",
    "title": "Lab 1: Probability & Descriptive Statistics",
    "section": "3 Ind",
    "text": "3 Ind\n\n# ---- Data generation ----\nskewed_sample &lt;- rlnorm(1000, meanlog = 2, sdlog = 0.5)  # log-normal (positively skewed)\nnormal_sample &lt;- rnorm(1000, mean = 20, sd = 5)\n\ncat('Preview: first 6 rows of skewed (right-tailed) vs approximately normal sample.\\n')\nhead(data.frame(skewed_sample, normal_sample))\n\nPreview: first 6 rows of skewed (right-tailed) vs approximately normal sample.\n\n\n\nA data.frame: 6 √ó 2\n\n\n\nskewed_sample\nnormal_sample\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n16.108614\n19.910099\n\n\n2\n7.654198\n19.339124\n\n\n3\n7.882490\n7.253286\n\n\n4\n17.418494\n25.202867\n\n\n5\n9.304127\n21.248629\n\n\n6\n3.925404\n32.081037\n\n\n\n\n\n\n# ---- Descriptive statistics ----\nsummary_skewed &lt;- quick_summary(skewed_sample)\nsummary_normal &lt;- quick_summary(normal_sample)\n\ncat('Comparison of key statistics: note higher skewness & kurtosis for log-normal sample.\\n')\nbind_rows(\n  skewed = summary_skewed,\n  normal = summary_normal,\n  .id = \"distribution\"\n)\n\nComparison of key statistics: note higher skewness & kurtosis for log-normal sample.\n\n\n\nA data.frame: 2 √ó 7\n\n\ndistribution\nn\nmean\nsd\nvar\nskewness\nkurtosis\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nskewed\n1000\n8.428385\n4.496494\n20.21846\n1.64801605\n7.072802\n\n\nnormal\n1000\n20.221132\n5.043640\n25.43830\n-0.01362832\n2.942717\n\n\n\n\n\n\n# ---- Visualization: Histograms ----\ncombined &lt;- rbind(\n  data.frame(value = skewed_sample, type = \"Skewed (Log-normal)\"),\n  data.frame(value = normal_sample, type = \"Approx. Normal\")\n)\n\noptions(repr.plot.width = 13, repr.plot.height = 6)\n\nggplot(combined, aes(value)) +\n  geom_histogram(bins = 40, fill = \"#3182bd\", alpha = 0.8, color = \"white\") +\n  facet_wrap(~ type, scales = \"free\", nrow = 1) +\n  labs(title = \"Distribution Contrast\", x = \"Value\", y = \"Count\") +\n  theme(panel.spacing.x = unit(1.2, \"lines\"))\n\n\n\n\n\n\n\n\n\n# ---- Covariance & Correlation example ----\n# Suppose revenue depends on marketing spend with noise\nmarketing_spend &lt;- rnorm(100, mean = 50, sd = 10)\nrevenue &lt;- 5 + 1.8 * marketing_spend + rnorm(100, sd = 15)\n\ncat('Covariance & correlation between marketing spend and revenue (positive association expected).\\n')\nsummary_df &lt;- data.frame(cov = cov(marketing_spend, revenue), cor = cor(marketing_spend, revenue))\nsummary_df\n\nCovariance & correlation between marketing spend and revenue (positive association expected).\n\n\n\nA data.frame: 1 √ó 2\n\n\ncov\ncor\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n149.4054\n0.7856426\n\n\n\n\n\n\n# ---- Simulation: Sample vs Population variance ----\ntrue_var &lt;- 25  # sd^2 with sd=5\nsample_sizes &lt;- c(5, 10, 20, 50, 100, 250, 500)\nresults &lt;- lapply(sample_sizes, function(n){\n  x &lt;- rnorm(n, mean = 0, sd = 5)\n  data.frame(n = n, var_n = mean((x - mean(x))^2), var_n1 = var(x))\n}) %&gt;% dplyr::bind_rows()\ncat('Variance estimators: var_n (biased, divide by n) vs var_n1 (unbiased, divide by n-1).\\n')\nresults\n\nVariance estimators: var_n (biased, divide by n) vs var_n1 (unbiased, divide by n-1).\n\n\n\nA data.frame: 7 √ó 3\n\n\nn\nvar_n\nvar_n1\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n5\n15.17886\n18.97357\n\n\n10\n26.34624\n29.27360\n\n\n20\n16.53123\n17.40129\n\n\n50\n21.11229\n21.54315\n\n\n100\n23.98626\n24.22854\n\n\n250\n25.38000\n25.48193\n\n\n500\n25.58307\n25.63434\n\n\n\n\n\n\n# ---- Plot: Convergence of variance estimators ----\nresults_long &lt;- results %&gt;%\n  tidyr::pivot_longer(var_n:var_n1, names_to = \"estimator\", values_to = \"value\") %&gt;%\n  dplyr::mutate(estimator = dplyr::recode(estimator, var_n = \"Divide by n\", var_n1 = \"Divide by n-1\"))\n\noptions(repr.plot.width = 8, repr.plot.height = 6)\n\nggplot(results_long, aes(n, value, color = estimator)) +\n  geom_line() +\n  geom_point() +\n  geom_hline(yintercept = true_var, linetype = \"dashed\") +\n  labs(title = \"Convergence of Variance Estimators\", y = \"Estimated variance\", x = \"Sample size\")\n\n\n\n\n\n\n\n\n\nSampling Distributions of Two Asset Returns\nWe will simulate repeated samples of two (correlated) asset return series and compare the sampling distributions of their sample means and the distribution of the portfolio mean (equal weights).\n\n# ---- Stretch: parameters ----\nset.seed(2025)\nmu &lt;- c(0.0005, 0.0008)          # daily expected returns\nsd_vec &lt;- c(0.02, 0.03)\nrho &lt;- 0.9  # increased to 0.90 to demonstrate strong correlation\nSigma2 &lt;- matrix(c(sd_vec[1]^2, rho*prod(sd_vec), rho*prod(sd_vec), sd_vec[2]^2), 2, 2)\nB &lt;- chol(Sigma2)\n\n\n# ---- Stretch: demonstrate high correlation ----\n# Single large sample to visualize relationship\nN_demo &lt;- 2000\nZ_demo &lt;- matrix(rnorm(2*N_demo), N_demo, 2) %*% B\nX_demo &lt;- sweep(Z_demo, 2, mu, \"+\")\nassetA_demo &lt;- X_demo[,1]\nassetB_demo &lt;- X_demo[,2]\n\nemp_cor &lt;- cor(assetA_demo, assetB_demo)\ncat(sprintf('Empirical correlation from demo sample: %.3f (target rho = %.2f)\\n', emp_cor, rho))\n\noptions(repr.plot.width = 6, repr.plot.height = 5)\n\nggplot(data.frame(assetA_demo, assetB_demo), aes(assetA_demo, assetB_demo)) +\n  geom_point(alpha = 0.35, color = '#1b7837') +\n  geom_smooth(method='lm', se=FALSE, color='#762a83') +\n  labs(title = 'Scatter of Two Asset Returns (High Correlation)', x='Asset A Return', y='Asset B Return',\n       subtitle = paste0('Empirical r = ', round(emp_cor,3)))\n\nEmpirical correlation from demo sample: 0.904 (target rho = 0.90)\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\nReflection\nHow does correlation between assets influence the portfolio mean‚Äôs variability? Write 3‚Äì4 sentences interpreting for a diversified vs concentrated investment decision.",
    "crumbs": [
      "Home",
      "Basics",
      "Lab 1: Probability & Descriptive Statistics"
    ]
  },
  {
    "objectID": "01_Lab-1.html#independence-vs.-zero-correlation",
    "href": "01_Lab-1.html#independence-vs.-zero-correlation",
    "title": "Lab 1: Probability & Descriptive Statistics",
    "section": "3 Independence vs.¬†Zero Correlation",
    "text": "3 Independence vs.¬†Zero Correlation\nIndependence means no influence between events, while correlation measures linear relationship strength.\nIndependence implies zero correlation, but zero correlation does not imply independence.\nE.g., \\(X\\) and \\(Y = X^2\\) have zero correlation but are not independent.\n\n# ---- Zero Correlation but NOT Independence: Y = X^2 ----\n# Idea: X ~ N(0,1); define Y = X^2.  Then cor(X,Y) ~ 0 (symmetry cancels linear relation),\n# yet Y is a deterministic function of X so they are NOT independent.\n\nset.seed(2025)\nn &lt;- 5000\nX &lt;- rnorm(n)\nY &lt;- X^2\nr_xy &lt;- cor(X, Y)\ncat(sprintf(\"Sample correlation cor(X, Y=X^2) = %.4f (near 0)\\n\", r_xy))\n\nSample correlation cor(X, Y=X^2) = 0.0198 (near 0)\n\n\nWe got \\(\\rho_{X,Y}=0.0198\\). It seems to be close to zero, but we need a statistical test to confirm this formally.\n‚Üí Test for significance of the correlation coefficient\n\ncor.test(X,Y)\n\n\n    Pearson's product-moment correlation\n\ndata:  X and Y\nt = 1.4027, df = 4998, p-value = 0.1608\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.007887062  0.047529726\nsample estimates:\n       cor \n0.01983657 \n\n\nüí° Q: What is the p-value for the correlation test? What can you conclude about the correlation between the two variables? Write your answer in the cell below.\nA: [Type your answer here]\n\nlibrary(ggplot2)\noptions(repr.plot.width = 8, repr.plot.height = 7)\nscatter_df &lt;- data.frame(X = X, Y = Y)\np1 &lt;- ggplot(scatter_df, aes(X, Y)) +\n  geom_point(alpha = 0.25, color = '#2c7fb8') +\n  annotate('text', x = min(X)+0.2, y = max(Y)*0.95,\n           label = paste0('cor = ', sprintf('%.3f', r_xy)),\n           hjust = 0, size = 4) +\n  labs(title = 'Zero Linear Correlation but Nonlinear Dependence',\n       subtitle = 'Y = X^2 with X ~ N(0,1)',\n       x = 'X', y = 'Y = X^2') +\n  theme_minimal()\nprint(p1)",
    "crumbs": [
      "Home",
      "Basics",
      "Lab 1: Probability & Descriptive Statistics"
    ]
  },
  {
    "objectID": "01_Lab-1.html#descriptive-statistics-on-asset-returns",
    "href": "01_Lab-1.html#descriptive-statistics-on-asset-returns",
    "title": "Lab 1: Probability & Descriptive Statistics",
    "section": "4 Descriptive Statistics on Asset Returns",
    "text": "4 Descriptive Statistics on Asset Returns\n\nasset_df &lt;- read_csv(\"\")\n\n[1] \"Preview: first 6 rows of asset returns data frame:\"\n\n\n\nA data.frame: 6 √ó 3\n\n\n\nAsset_A\nAsset_B\nAsset_C\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n17.88517\n4.305714\n9.940056\n\n\n2\n10.30022\n4.165024\n10.549088\n\n\n3\n20.87020\n14.160931\n6.608859\n\n\n4\n18.20515\n22.326560\n9.580894\n\n\n5\n17.49377\n2.227112\n10.226785\n\n\n6\n29.87376\n15.721318\n8.969147\n\n\n\n\n\n\nquick_summary &lt;- function(x) {\n  # Function to compute basic descriptive statistics\n  data.frame(\n    n = length(x),\n    mean = mean(x),\n    sd = sd(x),\n    var = var(x),\n    skewness = moments::skewness(x),\n    kurtosis = moments::kurtosis(x),\n    row.names = NULL\n  )\n}\n\n\nlapply(asset_df, quick_summary) %&gt;% \n    do.call(rbind, .) %&gt;%\n  kable(digits = 3, caption = \"Descriptive Statistics of Asset Returns\")\n\n\n\nTable: Descriptive Statistics of Asset Returns\n\n|        |     n|   mean|    sd|    var| skewness| kurtosis|\n|:-------|-----:|------:|-----:|------:|--------:|--------:|\n|Asset_A | 10000| 19.679| 4.969| 24.695|    0.095|    2.919|\n|Asset_B | 10000|  8.237| 4.420| 19.533|    1.579|    7.023|\n|Asset_C | 10000| 10.001| 0.987|  0.975|   -0.825|    3.442|\n\n\n\n# ---- Descriptive statistics ----\nsummary_skewed &lt;- quick_summary(skewed_sample)\nsummary_normal &lt;- quick_summary(normal_sample)\n\ncat('Comparison of key statistics: note higher skewness & kurtosis for log-normal sample.\\n')\nbind_rows(\n  skewed = summary_skewed,\n  normal = summary_normal,\n  .id = \"distribution\"\n)\n\nComparison of key statistics: note higher skewness & kurtosis for log-normal sample.\n\n\n\nA data.frame: 2 √ó 7\n\n\ndistribution\nn\nmean\nsd\nvar\nskewness\nkurtosis\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nskewed\n1000\n8.428385\n4.496494\n20.21846\n1.64801605\n7.072802\n\n\nnormal\n1000\n20.221132\n5.043640\n25.43830\n-0.01362832\n2.942717\n\n\n\n\n\n\n# ---- Visualization: Histograms ----\ncombined &lt;- rbind(\n  data.frame(value = skewed_sample, type = \"Skewed (Log-normal)\"),\n  data.frame(value = normal_sample, type = \"Approx. Normal\")\n)\n\noptions(repr.plot.width = 13, repr.plot.height = 6)\n\nggplot(combined, aes(value)) +\n  geom_histogram(bins = 40, fill = \"#3182bd\", alpha = 0.8, color = \"white\") +\n  facet_wrap(~ type, scales = \"free\", nrow = 1) +\n  labs(title = \"Distribution Contrast\", x = \"Value\", y = \"Count\") +\n  theme(panel.spacing.x = unit(1.2, \"lines\"))\n\n\n\n\n\n\n\n\n\nNegatively Skewed Sample\nTo illustrate a left (negative) skew, we can reflect a right‚Äëskewed log-normal draw: neg_skew_sample = -rlnorm(...). This produces many moderate values with a few extreme negative tail values (long left tail). We‚Äôll summarize and visualize it next.\n\n# ---- Generate & Summarize Negatively Skewed Sample ----\nset.seed(321)\nneg_skew_sample &lt;- -rlnorm(1000, meanlog = 2, sdlog = 0.5)\nneg_summary &lt;- quick_summary(neg_skew_sample)\ncat('Negative skew sample summary (note skewness &lt; 0):\\n')\nneg_summary\n\n# Compare with original positively skewed (if present)\nif (exists('skewed_sample')) {\n  comp &lt;- dplyr::bind_rows(\n    pos_skew = quick_summary(skewed_sample),\n    neg_skew = neg_summary,\n    .id = 'type'\n  )\n  cat('\\nComparison vs original positive skew sample:\\n')\n  comp\n}\n\n\n# ---- Covariance & Correlation example ----\n# Suppose revenue depends on marketing spend with noise\nmarketing_spend &lt;- rnorm(100, mean = 50, sd = 10)\nrevenue &lt;- 5 + 1.8 * marketing_spend + rnorm(100, sd = 15)\n\ncat('Covariance & correlation between marketing spend and revenue (positive association expected).\\n')\nsummary_df &lt;- data.frame(cov = cov(marketing_spend, revenue), cor = cor(marketing_spend, revenue))\nsummary_df\n\nCovariance & correlation between marketing spend and revenue (positive association expected).\n\n\n\nA data.frame: 1 √ó 2\n\n\ncov\ncor\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n149.4054\n0.7856426\n\n\n\n\n\n\n# ---- Simulation: Sample vs Population variance ----\ntrue_var &lt;- 25  # sd^2 with sd=5\nsample_sizes &lt;- c(5, 10, 20, 50, 100, 250, 500)\nresults &lt;- lapply(sample_sizes, function(n){\n  x &lt;- rnorm(n, mean = 0, sd = 5)\n  data.frame(n = n, var_n = mean((x - mean(x))^2), var_n1 = var(x))\n}) %&gt;% dplyr::bind_rows()\ncat('Variance estimators: var_n (biased, divide by n) vs var_n1 (unbiased, divide by n-1).\\n')\nresults\n\nVariance estimators: var_n (biased, divide by n) vs var_n1 (unbiased, divide by n-1).\n\n\n\nA data.frame: 7 √ó 3\n\n\nn\nvar_n\nvar_n1\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n5\n15.17886\n18.97357\n\n\n10\n26.34624\n29.27360\n\n\n20\n16.53123\n17.40129\n\n\n50\n21.11229\n21.54315\n\n\n100\n23.98626\n24.22854\n\n\n250\n25.38000\n25.48193\n\n\n500\n25.58307\n25.63434\n\n\n\n\n\n\n# ---- Plot: Convergence of variance estimators ----\nresults_long &lt;- results %&gt;%\n  tidyr::pivot_longer(var_n:var_n1, names_to = \"estimator\", values_to = \"value\") %&gt;%\n  dplyr::mutate(estimator = dplyr::recode(estimator, var_n = \"Divide by n\", var_n1 = \"Divide by n-1\"))\n\noptions(repr.plot.width = 8, repr.plot.height = 6)\n\nggplot(results_long, aes(n, value, color = estimator)) +\n  geom_line() +\n  geom_point() +\n  geom_hline(yintercept = true_var, linetype = \"dashed\") +\n  labs(title = \"Convergence of Variance Estimators\", y = \"Estimated variance\", x = \"Sample size\")\n\n\n\n\n\n\n\n\n\n\nSampling Distributions of Two Asset Returns\nWe will simulate repeated samples of two (correlated) asset return series and compare the sampling distributions of their sample means and the distribution of the portfolio mean (equal weights).\n\n# ---- Stretch: parameters ----\nset.seed(2025)\nmu &lt;- c(0.0005, 0.0008)          # daily expected returns\nsd_vec &lt;- c(0.02, 0.03)\nrho &lt;- 0.9  # increased to 0.90 to demonstrate strong correlation\nSigma2 &lt;- matrix(c(sd_vec[1]^2, rho*prod(sd_vec), rho*prod(sd_vec), sd_vec[2]^2), 2, 2)\nB &lt;- chol(Sigma2)\n\n\n# ---- Stretch: demonstrate high correlation ----\n# Single large sample to visualize relationship\nN_demo &lt;- 2000\nZ_demo &lt;- matrix(rnorm(2*N_demo), N_demo, 2) %*% B\nX_demo &lt;- sweep(Z_demo, 2, mu, \"+\")\nassetA_demo &lt;- X_demo[,1]\nassetB_demo &lt;- X_demo[,2]\n\nemp_cor &lt;- cor(assetA_demo, assetB_demo)\ncat(sprintf('Empirical correlation from demo sample: %.3f (target rho = %.2f)\\n', emp_cor, rho))\n\noptions(repr.plot.width = 6, repr.plot.height = 5)\n\nggplot(data.frame(assetA_demo, assetB_demo), aes(assetA_demo, assetB_demo)) +\n  geom_point(alpha = 0.35, color = '#1b7837') +\n  geom_smooth(method='lm', se=FALSE, color='#762a83') +\n  labs(title = 'Scatter of Two Asset Returns (High Correlation)', x='Asset A Return', y='Asset B Return',\n       subtitle = paste0('Empirical r = ', round(emp_cor,3)))\n\nEmpirical correlation from demo sample: 0.904 (target rho = 0.90)\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\nReflection\nHow does correlation between assets influence the portfolio mean‚Äôs variability? Write 3‚Äì4 sentences interpreting for a diversified vs concentrated investment decision.",
    "crumbs": [
      "Home",
      "Basics",
      "Lab 1: Probability & Descriptive Statistics"
    ]
  }
]