[
  {
    "objectID": "07_Lab-2_dummy-variable.html",
    "href": "07_Lab-2_dummy-variable.html",
    "title": "Lab 2: Regression with Dummy Variables",
    "section": "",
    "text": "In this lab, we will run regressions with dummy variables (also known as binary variables or indicator variables).\n🎯 In this script, you will learn how to:",
    "crumbs": [
      "Home",
      "Basics",
      "Lab 2: Regression with Dummy Variables"
    ]
  },
  {
    "objectID": "07_Lab-2_dummy-variable.html#instructions",
    "href": "07_Lab-2_dummy-variable.html#instructions",
    "title": "Lab 2: Regression with Dummy Variables",
    "section": "1 Instructions",
    "text": "1 Instructions\nCopy and paste ALL code in this page into a new R script file in RStudio. Save the file as lab2_binary_variables.R (we call it a script or a source file). Then, run the code step by step, answering the questions.\nAlternatively, use Google Colab.\n\n# load packages and dataset\npkgs &lt;- c(\"tidyverse\", \"moments\", \"data.table\", \"stargazer\")\nmissing &lt;- setdiff(pkgs, rownames(installed.packages()))\nif (length(missing) &gt; 0) install.packages(missing)\ninvisible(lapply(pkgs, function(pkg) suppressPackageStartupMessages(library(pkg, character.only = TRUE))))",
    "crumbs": [
      "Home",
      "Basics",
      "Lab 2: Regression with Dummy Variables"
    ]
  },
  {
    "objectID": "07_Lab-2_dummy-variable.html#load-data-into-working-directory",
    "href": "07_Lab-2_dummy-variable.html#load-data-into-working-directory",
    "title": "Lab 2: Regression with Dummy Variables",
    "section": "2 Load data into working directory",
    "text": "2 Load data into working directory\nWe continue to use the same dataset California school dataset (CASchools). Refer to Data Visualization: Dataset Overview for detailed description of the dataset.\nPreviously, we loaded the data directly from an online data repository. Now, we will load the data from a local CSV file.\nDownload the CASchools_test_score.csv file from Canvas and placed it in the same folder as where you put lab2_binary_variables.R. This is to make sure that the script can find the data file.\n\nDepending on your computer setting, you may or may not see the file extension .csv.\nYou can rename the data file, note that the data file name needs to match the f_name used in the code below.\n\nTroubleshooting: If you get an error message like\n\nError: ‘CASchools_test_score.csv’ does not exist in current working directory (‘path-to-file’).\n\nIt means that your working directory is not set to the folder where you put the data file. You need to set your working directory to that folder. You can do this in RStudio by clicking on Session -&gt; Set Working Directory -&gt; Set to Source File Location. Then, run the code again.\n\n\n# load dataset\nf_name &lt;- \"CASchools_test_score.csv\"\ncas &lt;- read_csv(f_name)\n# data preview, first and last 5 rows\ncas %&gt;% as.data.table() %&gt;% print(topn = 5)\n\n     district                          school      county grades students\n        &lt;num&gt;                          &lt;char&gt;      &lt;char&gt; &lt;char&gt;    &lt;num&gt;\n  1:    75119              Sunol Glen Unified     Alameda  KK-08      195\n  2:    61499            Manzanita Elementary       Butte  KK-08      240\n  3:    61549     Thermalito Union Elementary       Butte  KK-08     1550\n  4:    61457 Golden Feather Union Elementary       Butte  KK-08      243\n  5:    61523        Palermo Union Elementary       Butte  KK-08     1335\n ---                                                                     \n416:    68957          Las Lomitas Elementary   San Mateo  KK-08      984\n417:    69518            Los Altos Elementary Santa Clara  KK-08     3724\n418:    72611          Somis Union Elementary     Ventura  KK-08      441\n419:    72744               Plumas Elementary        Yuba  KK-08      101\n420:    72751            Wheatland Elementary        Yuba  KK-08     1778\n     teachers calworks   lunch computer expenditure    income   english  read\n        &lt;num&gt;    &lt;num&gt;   &lt;num&gt;    &lt;num&gt;       &lt;num&gt;     &lt;num&gt;     &lt;num&gt; &lt;num&gt;\n  1:    10.90   0.5102  2.0408       67    6384.911 22.690001  0.000000 691.6\n  2:    11.15  15.4167 47.9167      101    5099.381  9.824000  4.583333 660.5\n  3:    82.90  55.0323 76.3226      169    5501.955  8.978000 30.000002 636.3\n  4:    14.00  36.4754 77.0492       85    7101.831  8.978000  0.000000 651.9\n  5:    71.50  33.1086 78.4270      171    5235.988  9.080333 13.857677 641.8\n ---                                                                         \n416:    59.73   0.1016  3.5569      195    7290.339 28.716999  5.995935 700.9\n417:   208.48   1.0741  1.5038      721    5741.463 41.734108  4.726101 704.0\n418:    20.15   3.5635 37.1938       45    4402.832 23.733000 24.263039 648.3\n419:     5.00  11.8812 59.4059       14    4776.336  9.952000  2.970297 667.9\n420:    93.40   6.9235 47.5712      313    5993.393 12.502000  5.005624 660.5\n      math\n     &lt;num&gt;\n  1: 690.0\n  2: 661.9\n  3: 650.9\n  4: 643.5\n  5: 639.9\n ---      \n416: 707.7\n417: 709.5\n418: 641.7\n419: 676.5\n420: 651.0\n\n\nPrepare dataset for analysis.\nAs done previously, we create the following variables:\n\nTestScore: average test score of students in a school (average of math and reading scores)\nstr: student-teacher ratio\n\n\ncas &lt;- cas %&gt;%\n    mutate(\n        TestScore = (read + math) / 2,\n        STR = students / teachers\n    )",
    "crumbs": [
      "Home",
      "Basics",
      "Lab 2: Regression with Dummy Variables"
    ]
  },
  {
    "objectID": "07_Lab-2_dummy-variable.html#regression-when-x-is-a-binary-variable",
    "href": "07_Lab-2_dummy-variable.html#regression-when-x-is-a-binary-variable",
    "title": "Lab 2: Regression with Dummy Variables",
    "section": "3 Regression When \\(X\\) is a Binary Variable",
    "text": "3 Regression When \\(X\\) is a Binary Variable\nInstead of using a continuous regressor \\(X\\) , we might be interested in running the regression\n\\[\nY_i = \\beta_0 + \\beta_1 D_i + u_i,\n\\tag{1}\\]\nwhere \\(D_i\\) is a binary variable, a so-called dummy variable. For example, we may defined \\(D_i\\) as follows:\n\\[\nD_i =\n\\begin{cases}\n    1 & \\text{if $STR$ in $i^{th}$ school district &lt; 20} \\\\\n    0 & \\text{if $STR$ in $i^{th}$ school district $\\geq$ 20}.\n\\end{cases}\n\\tag{2}\\]\nThe regression model now is\n\\[\nTestScore_i = \\beta_0 + \\beta_1 D_i + u_i.\n\\]\n\n3.1 Data Visualization\n\n# Create the dummy variable as defined above\n# D=1 (low STR); D=0 (high STR)\ncas$D &lt;- cas$STR &lt; 20\n\n\n# Compute group means\nmeans &lt;- aggregate(TestScore ~ D, data = cas, mean)\nmeans\n\n\nA data.frame: 2 × 2\n\n\nD\nTestScore\n\n\n&lt;lgl&gt;\n&lt;dbl&gt;\n\n\n\n\nFALSE\n650.0768\n\n\nTRUE\n657.2462\n\n\n\n\n\n\nggplot(cas, aes(x = factor(D), y = TestScore)) +\n    # scatter points, spreads the points horizontally so they don't overlap\n    geom_jitter(width = 0.1, height = 0, size = 0.5, color = \"steelblue\") +\n    geom_point(\n        data = means, aes(x = factor(D), y = TestScore),\n        color = \"red\", size = 2\n    ) + # group means\n    labs(\n        x = expression(D[i]),\n        y = \"Test Score\",\n    ) +\n    theme_minimal(base_size = 14)\n\n\n\n\n\n\nWe see that small size class has higher average test score.\n\n\n3.2 Regression with a Binary Variable\nWith \\(D\\) as the regressor, it is not useful to think of \\(\\beta_1\\) as a slope parameter since since \\(D_i \\in \\{0,1\\},\\) i.e., we only observe two discrete values instead of a continuum of regressor values. There is no continuous line depicting the conditional expectation function \\(E(TestScore_i | D_i)\\) since this function is solely defined for \\(x\\)-positions \\(D_i = 0\\) and \\(D_i = 1\\).\nTherefore, the interpretation of the coefficients in this regression model is as follows:\n\n\\(E(Y_i | D_i = 0) = \\beta_0\\), so \\(\\beta_0\\) is the expected test score in districts where \\(D_i = 0\\) and \\(STR\\) is larger or equal to 20.\n\\(E(Y_i | D_i = 1) = \\beta_0 + \\beta_1\\), or \\(\\beta_1 = E(Y_i | D_i = 1) - E(Y_i | D_i = 0).\\) Thus, \\(\\beta_1\\) is the difference in group-specific expectations, i.e., the difference in expected test score between districts with \\(STR&lt;20\\) and those with \\(STR \\geq 20.\\)\n\nWe now estimate the dummy regression model as defined by Equations (1) and (2).\n\ndummy_model_slr &lt;- lm(TestScore ~ D, data = cas)\nstargazer(dummy_model_slr, type = \"text\", title = \"Dummy Variable Regression Results\", digits = 3)\n\n\nDummy Variable Regression Results\n===============================================\n                        Dependent variable:    \n                    ---------------------------\n                             TestScore         \n-----------------------------------------------\nD                            7.169***          \n                              (1.847)          \n                                               \nConstant                    650.077***         \n                              (1.393)          \n                                               \n-----------------------------------------------\nObservations                    420            \nR2                             0.035           \nAdjusted R2                    0.032           \nResidual Std. Error      18.741 (df = 418)     \nF Statistic           15.073*** (df = 1; 418)  \n===============================================\nNote:               *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\n💡 Q: Based on the regression output, answer the following questions:\n\nWhat is the average test score in school districts with \\(STR \\geq 20\\)?\nWhat is the average test score in school districts with \\(STR &lt; 20\\)?\nWhat is the difference in average test score between school districts with \\(STR &lt; 20\\) and those with \\(STR \\geq 20\\)?\nIs the difference statistically significant at the 5% significance level?",
    "crumbs": [
      "Home",
      "Basics",
      "Lab 2: Regression with Dummy Variables"
    ]
  },
  {
    "objectID": "07_Lab-2_dummy-variable.html#mlr-with-a-binary-variable",
    "href": "07_Lab-2_dummy-variable.html#mlr-with-a-binary-variable",
    "title": "Lab 2: Regression with Dummy Variables",
    "section": "4 MLR with a Binary Variable",
    "text": "4 MLR with a Binary Variable\n\ndummy_model_mlr &lt;- lm(TestScore ~ computer + english + lunch + D, data = cas)\nstargazer(dummy_model_slr, dummy_model_mlr,\n    type = \"text\",\n    title = \"Regression Results\",\n    column.labels = c(\"SLR\", \"MLR\"),\n    digits = 3\n)\n\n\nRegression Results\n====================================================================\n                                  Dependent variable:               \n                    ------------------------------------------------\n                                       TestScore                    \n                              SLR                     MLR           \n                              (1)                     (2)           \n--------------------------------------------------------------------\ncomputer                                             0.001          \n                                                    (0.001)         \n                                                                    \nenglish                                            -0.139***        \n                                                    (0.034)         \n                                                                    \nlunch                                              -0.544***        \n                                                    (0.022)         \n                                                                    \nD                          7.169***                 2.876***        \n                            (1.847)                 (0.952)         \n                                                                    \nConstant                  650.077***               678.590***       \n                            (1.393)                 (1.164)         \n                                                                    \n--------------------------------------------------------------------\nObservations                  420                     420           \nR2                           0.035                   0.770          \nAdjusted R2                  0.032                   0.768          \nResidual Std. Error    18.741 (df = 418)        9.176 (df = 415)    \nF Statistic         15.073*** (df = 1; 418) 347.915*** (df = 4; 415)\n====================================================================\nNote:                                    *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\n💡 Q: Comparing the regression results of the simple regression with a binary variable and the multiple regression with additional control variables, answer the following questions:\n\nHow does the coefficient of the binary variable change when we add more control variables?\nWhy does the estimated effect of small classes shrink when other control variables are included?\nDoes the smaller coefficient in the multiple linear regression model mean small classes are unimportant, or does it mean we now have a less biased estimate of their effect? Explain.",
    "crumbs": [
      "Home",
      "Basics",
      "Lab 2: Regression with Dummy Variables"
    ]
  },
  {
    "objectID": "07_Lab-2_dummy-variable.html#anova-test",
    "href": "07_Lab-2_dummy-variable.html#anova-test",
    "title": "Lab 2: Regression with Dummy Variables",
    "section": "5 ANOVA test",
    "text": "5 ANOVA test\n\n5.1 One-way ANOVA test\nThe purpose of a one-way ANOVA (analysis of variance) test is to determine the existence of a statistically significant difference among the means of three or more populations. The test actually uses variances to help determine if the population means are equal or not.\nIn what follows, we will use the one-way ANONA test to examine whether the average test scores differ across schools with different sizes of student-teacher ratios (STR).\nWe first create a new categorical variable str_cat based on the variable str (student-teacher ratio):\n\nSmall size class: STR &lt; 18\nMedium size class: 18 ≤ STR &lt; 21\nLarge size class: STR ≥ 21\n\nThen, we run the one-way ANOVA test to see if the average test scores differ across schools with different sizes of student-teacher ratios (STR).\nNull and alternative hypotheses:\n\n\\(H_0\\): The average test scores are the same across schools with different sizes of student-teacher ratios (STR).\n\\(H_1\\): At least one pair of average test scores are different.\n\n\ncas &lt;- cas %&gt;%\n    mutate(\n        str_cat = case_when(\n            STR &lt; 18 ~ \"Small\",\n            STR &gt;= 18 & STR &lt; 21 ~ \"Medium\",\n            STR &gt;= 21 ~ \"Large\"\n        )\n    )\ncas$str_cat %&gt;% table()\n\n.\n Large Medium  Small \n    92    253     75 \n\n\n\n# ANOVA test\nanova_result &lt;- aov(TestScore ~ str_cat, data = cas)\nsummary(anova_result)\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nstr_cat       2   9101    4551   13.27 2.59e-06 ***\nResiduals   417 143008     343                     \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n\nGiven the significance level \\(\\alpha = 0.05\\), we reject the null hypothesis (p-value &lt; 0.05) and conclude that at least one pair of average test scores are different across schools with different sizes of student-teacher ratios (STR).\n\n# Box plot of TestScore by str_cat\np_anova &lt;- ggplot(cas, aes(x = str_cat, y = TestScore)) +\n    geom_boxplot(fill = \"lightblue\") +\n    labs(\n        x = \"Student-Teacher Ratio Category\",\n        y = \"Test Score\",\n        title = \"Box Plot of Test Scores by Student-Teacher Ratio Category\"\n    ) +\n    theme_minimal(base_size = 14)\np_anova\n\n\n\n\n\n\n\n\n\n\n5.2 Two-way ANOVA test\nThe two-way ANOVA test is used to compare the fits of two regression models.\n\nTest for Improvement: It tests whether adding more predictors (variables) to a model significantly improves the model’s ability to explain the variability in the response variable.\nModel Selection: Helps in deciding between a simpler model with fewer variables and a more complex one with more variables.\n\nFor instance, consider we have two models:\n\nModel 1: \\(y = \\beta_0 + \\beta_1 x_{1} + \\varepsilon\\), and\nModel 2: \\(y = \\beta_0 + \\beta_1 x_{1} + \\beta_2 x_2 + \\beta_3 x_3 + \\varepsilon\\).\n\nWe can form a test by comparing the \\(R^2\\) from the two models.\n\\[\n\\begin{aligned}\n&\\text{H}_0: \\beta_2 = \\beta_3 = 0 \\\\\n&\\text{H}_1: \\beta_2 \\ne 0 \\text{ or } \\beta_3\\ne 0 ,\n\\end{aligned}\n\\] which imposes two exclusion restrictions.\nComing back to our California school TestScore example, we can use the two-way ANOVA test to see if the multiple regression model with additional control variables (Model 2) significantly improves the model’s ability to explain the variability in TestScore compared to the simple regression model (Model 1).\n\nanova_result2 &lt;- anova(dummy_model_slr, dummy_model_mlr)\nanova_result2\n\n\nA anova: 2 × 6\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n418\n146815.42\nNA\nNA\nNA\nNA\n\n\n2\n415\n34940.45\n3\n111875\n442.9261\n6.146085e-129\n\n\n\n\n\nThe p-value is way less than 0.05, indicating that we reject the null hypothesis and conclude that adding the control variables (computer, english, and lunch) significantly improves the model’s ability to explain the variability in TestScore.",
    "crumbs": [
      "Home",
      "Basics",
      "Lab 2: Regression with Dummy Variables"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Business Analytics (Fall 2025)",
    "section": "",
    "text": "What Business Analytics (BA) is about?\nThe aim of this course is to provide students advanced knowledge, skills and competencies they can use to make data driven decisions for organizations.\nData-driven decision making\n\nDescribe the data: what happened\nUsing introductory statistics to identify patterns, trends, and insights embedded in historical data. E.g., summary statistics.\nPredictive: what will happen\nUsing statistical models on historical data and forecast future outcomes. E.g., regression, time series.\nPrescriptive: what should we do\nUsing optimization, simulation, and decision analysis techniques to suggest the best course of action based on data, predictions, and constraints.\nAutonomous: automated decision-making using Machine Learning (ML) and Artificial Intelligence (AI) techniques.\n\nOutline of models and applications we will cover in this course:1\n\n\n\n\n\n\n\n\nCategory\nTopics\nApplications in Business\n\n\n\n\nDescriptive\nData Visualization, Descriptive Analysis\nAsset return → Finance\n\n\nPredictive\nLinear regression\nAsset return prediction → FinanceTest Score Determinants → Sociology\n\n\n\nHypothesis testing\nTest Score Determinants → Sociology\n\n\n\nClassification, logistic regression\nAsset return increasing/decreasing prediction → FinanceEmployee satisfaction → Operation\n\n\nPrescriptive\nOptimization models\nPricing optimization → OperationSensitivity analysis, Scenario analysis → Accounting\n\n\n\n[1] This is a preliminary plan. Topics and applications are subject to change as the course moves forward.\n\nSoftware: R programming\nR is an open-source programming language widely used for statistical computing and data analysis. It provides a rich ecosystem of packages and libraries for data manipulation, visualization, and modeling.\nWhy open-source stands out in the competition?\nAI is the game changer here. AI significantly improves coding efficiency and productivity. You don’t need to memorize every function or syntax anymore. The current role for humans is to communicate your needs to AI, and AI will generate the code for you.\n\nOpen-source software integrates cutting-edge AI tools, faster and more efficiently than paid software.\nBy contrast, paid software is often slower to implement new features due to development cycles and licensing restrictions.\n\n Demonstration in VS Code. \nBy typing # create a function taking the n-th power of a number, AI will generate the code for you.\nHere is how to use GitHub Copilot in RStudio: RStudio User Guide: Tools, GitHub Copilot."
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "Business Analytics (Fall 2025)",
    "section": "",
    "text": "What Business Analytics (BA) is about?\nThe aim of this course is to provide students advanced knowledge, skills and competencies they can use to make data driven decisions for organizations.\nData-driven decision making\n\nDescribe the data: what happened\nUsing introductory statistics to identify patterns, trends, and insights embedded in historical data. E.g., summary statistics.\nPredictive: what will happen\nUsing statistical models on historical data and forecast future outcomes. E.g., regression, time series.\nPrescriptive: what should we do\nUsing optimization, simulation, and decision analysis techniques to suggest the best course of action based on data, predictions, and constraints.\nAutonomous: automated decision-making using Machine Learning (ML) and Artificial Intelligence (AI) techniques.\n\nOutline of models and applications we will cover in this course:1\n\n\n\n\n\n\n\n\nCategory\nTopics\nApplications in Business\n\n\n\n\nDescriptive\nData Visualization, Descriptive Analysis\nAsset return → Finance\n\n\nPredictive\nLinear regression\nAsset return prediction → FinanceTest Score Determinants → Sociology\n\n\n\nHypothesis testing\nTest Score Determinants → Sociology\n\n\n\nClassification, logistic regression\nAsset return increasing/decreasing prediction → FinanceEmployee satisfaction → Operation\n\n\nPrescriptive\nOptimization models\nPricing optimization → OperationSensitivity analysis, Scenario analysis → Accounting\n\n\n\n[1] This is a preliminary plan. Topics and applications are subject to change as the course moves forward.\n\nSoftware: R programming\nR is an open-source programming language widely used for statistical computing and data analysis. It provides a rich ecosystem of packages and libraries for data manipulation, visualization, and modeling.\nWhy open-source stands out in the competition?\nAI is the game changer here. AI significantly improves coding efficiency and productivity. You don’t need to memorize every function or syntax anymore. The current role for humans is to communicate your needs to AI, and AI will generate the code for you.\n\nOpen-source software integrates cutting-edge AI tools, faster and more efficiently than paid software.\nBy contrast, paid software is often slower to implement new features due to development cycles and licensing restrictions.\n\n Demonstration in VS Code. \nBy typing # create a function taking the n-th power of a number, AI will generate the code for you.\nHere is how to use GitHub Copilot in RStudio: RStudio User Guide: Tools, GitHub Copilot."
  },
  {
    "objectID": "index.html#course-materials",
    "href": "index.html#course-materials",
    "title": "Business Analytics (Fall 2025)",
    "section": "2 Course Materials",
    "text": "2 Course Materials\nThe course materials will be self-contained.\n\nLecture notes: lecture notes will be provided in the course website.\nR code: R code will be provided in form of Lab Jupyter Notebooks.\nYou may copy and paste the code into RStudio to run it.\n\nIf you want to learn in-depth, you can refer to the following textbooks and resources.\nTextbooks\n\nEvans, J.R. (2021) Business analytics: methods, models, and decisions. Third edition. Harlow: Pearson.\nStatistics primer. Basic introduction to business analytics at the undergraduate level.\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An Introduction to Statistical Learning with Applications in R. 2nd edition. Springer. Online version\nMain textbook for the course. It covers the fundamental concepts and methods of statistical learning, with practical applications in R.\nHuntsinger, R. (2025). Business Analytics: Methods and Cases for Data-Driven Decisions. Cambridge: Cambridge University Press. eTextbook available through Cambridge University Press.\nAdvanced methods and case studies for data-driven decision-making. We might use case studies from this book in the course.\n\nResources:\n\nDistribution Tables\nFormula Sheet"
  },
  {
    "objectID": "index.html#course-evaluation",
    "href": "index.html#course-evaluation",
    "title": "Business Analytics (Fall 2025)",
    "section": "3 Course Evaluation",
    "text": "3 Course Evaluation\nCompud Assessment\n❗️ In case of any discrepancies between the dates listed on the course website and StudentWeb, use the dates on StudentWeb as the reference.\n\nOppgave (home assignment): 40%\nGroup work (1-3 students) is possible; including a case study with data analysis and visualization; a report will be submitted;\nDate: week 42 (preliminary date: 14.10.2025)\nDuration: 2 weeks.\nThe assignment will be released in Inspera, and you will have two weeks to complete it.\nMore details will be provided as the course progresses.\nEksamen (school exam): 60%\nDigitally in Inspera;\nDate: 18.11.2025 (Refer to Studentweb for the exact time)"
  },
  {
    "objectID": "index.html#study-objectives",
    "href": "index.html#study-objectives",
    "title": "Business Analytics (Fall 2025)",
    "section": "4 Study Objectives",
    "text": "4 Study Objectives\n\nKnowledge:\n\nFamiliarity with statistical methods and models used in business analytics.\nFocus on descriptive analytics and predictive analytics. Prescriptive analytics will be introduced if time allows.\nKnowledge of data visualization techniques and tools.\n\nSkills:\n\nAbility to analyze and interpret data using statistical methods.\nProficiency in using R for data analysis and visualization.\nCompetence in applying business analytics techniques to real-world problems.\n\nCompetencies:\n\nDevelop critical thinking skills to evaluate data-driven decisions.\nAbility to communicate findings effectively through reports and presentations."
  },
  {
    "objectID": "index.html#how-to-reach-me",
    "href": "index.html#how-to-reach-me",
    "title": "Business Analytics (Fall 2025)",
    "section": "5 How to Reach Me",
    "text": "5 How to Reach Me\n\n\n\nInstructor:\n\n\nMenghan Yuan\n\n\n\n\nEmail:\n\n\nmenghan.yuan@nord.no\n\n\n\n\nOffice Hours:\n\n\nBy appointment\n\n\n\n\nOffice:\n\n\nHovedbygning A257"
  },
  {
    "objectID": "09_Logistic_Regression.html",
    "href": "09_Logistic_Regression.html",
    "title": "Regression with a Binary Dependent Variable",
    "section": "",
    "text": "🎯 Study Objectives\n\nUnderstand why linear regression is inappropriate for binary dependent variables.\nLearn the logit and probit models for analyzing binary outcomes.\nInterpret coefficients in logistic regressions.\nApply logit/probit models using real-world data (HMDA mortgage application dataset).\nUnderstand the concept of predicted probabilities.\nCompare linear probability model (LPM), logit, and probit approaches.",
    "crumbs": [
      "Home",
      "Basics",
      "Regression with a Binary Dependent Variable"
    ]
  },
  {
    "objectID": "09_Logistic_Regression.html#why-binary-dependent-variables-require-special-treatment",
    "href": "09_Logistic_Regression.html#why-binary-dependent-variables-require-special-treatment",
    "title": "Regression with a Binary Dependent Variable",
    "section": "1 Why Binary Dependent Variables Require Special Treatment",
    "text": "1 Why Binary Dependent Variables Require Special Treatment\nIn many economic and financial applications, the dependent variable is binary (takes only two values: 0 or 1). Examples include:\n\nMortgage approval: Did the bank approve the mortgage application? (Yes = 1, No = 0)\nLabor force participation: Is the individual employed? (Yes = 1, No = 0)\nDefault risk: Did the borrower default on the loan? (Yes = 1, No = 0)\nMarket entry: Does the firm enter the market? (Yes = 1, No = 0)\nProduct choice: Does the consumer purchase the product? (Yes = 1, No = 0)\n\n\n1.1 Why Linear Regression Fails\nIf we try to use ordinary linear regression (OLS) with a binary dependent variable \\(Y_i \\in \\{0,1\\}\\):\n\\[\nY_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\cdots + \\beta_k X_{ki} + u_i\n\\]\nThis approach is called the Linear Probability Model (LPM) because it estimates the probability that \\(Y_i = 1\\) as a linear function of the predictors.\nProblems with LPM:\n\nPredicted probabilities outside [0,1]: OLS can produce fitted values \\(\\hat{Y}_i &lt; 0\\) or \\(\\hat{Y}_i &gt; 1\\), which makes no sense for probabilities.\nHeteroskedasticity: The error term variance depends on \\(X\\), violating the homoskedasticity assumption. This means standard errors are incorrect.\nNonlinear relationship: The effect of \\(X\\) on the probability of \\(Y=1\\) is unlikely to be constant across all values of \\(X\\). For example, increasing income from $20,000 to $30,000 likely has a different effect on mortgage approval than increasing from $200,000 to $210,000.\n\n\n\n1.2 The Solution: Logit and Probit Models\nTo address these problems, we use nonlinear models that ensure predicted probabilities stay within [0,1]:\n\nLogit model: Uses the logistic cumulative distribution function (CDF)\nProbit model: Uses the standard normal cumulative distribution function (CDF)",
    "crumbs": [
      "Home",
      "Basics",
      "Regression with a Binary Dependent Variable"
    ]
  },
  {
    "objectID": "09_Logistic_Regression.html#the-hmda-dataset-mortgage-lending-discrimination",
    "href": "09_Logistic_Regression.html#the-hmda-dataset-mortgage-lending-discrimination",
    "title": "Regression with a Binary Dependent Variable",
    "section": "2 The HMDA Dataset: Mortgage Lending Discrimination",
    "text": "2 The HMDA Dataset: Mortgage Lending Discrimination\nWe will use data from the Federal Reserve Bank of Boston collected under the Home Mortgage Disclosure Act (HMDA). The dataset contains information on mortgage applications in the Boston area in 1990. The dataset contains 2380 observations with 14 variables.\nResearch Question: Do banks discriminate against minority applicants when making mortgage lending decisions, after controlling for economic factors?\n\n2.1 Load the Data\n\n# Load required packages\npkgs &lt;- c(\"AER\", \"tidyverse\", \"stargazer\", \"margins\", \"data.table\")\nmissing &lt;- setdiff(pkgs, rownames(installed.packages()))\nif (length(missing) &gt; 0) install.packages(missing)\ninvisible(lapply(pkgs, function(pkg) suppressPackageStartupMessages(library(pkg, character.only = TRUE))))\n\n# Load HMDA data\ndata(\"HMDA\")\n\n# Preview the data\ndata.table(HMDA)\n\n        deny pirat hirat     lvrat  chist  mhist  phist unemp selfemp insurance\n      &lt;fctr&gt; &lt;num&gt; &lt;num&gt;     &lt;num&gt; &lt;fctr&gt; &lt;fctr&gt; &lt;fctr&gt; &lt;num&gt;  &lt;fctr&gt;    &lt;fctr&gt;\n   1:     no 0.221 0.221 0.8000000      5      2     no   3.9      no        no\n   2:     no 0.265 0.265 0.9218750      2      2     no   3.2      no        no\n   3:     no 0.372 0.248 0.9203980      1      2     no   3.2      no        no\n   4:     no 0.320 0.250 0.8604651      1      2     no   4.3      no        no\n   5:     no 0.360 0.350 0.6000000      1      1     no   3.2      no        no\n  ---                                                                          \n2376:     no 0.310 0.250 0.8000000      1      1     no   3.2     yes        no\n2377:     no 0.300 0.300 0.7770492      1      2     no   3.2      no        no\n2378:     no 0.260 0.200 0.5267606      2      1     no   3.1      no        no\n2379:    yes 0.320 0.260 0.7538462      6      1    yes   3.1      no        no\n2380:    yes 0.350 0.260 0.8135593      2      2     no   4.3      no        no\n      condomin   afam single hschool\n        &lt;fctr&gt; &lt;fctr&gt; &lt;fctr&gt;  &lt;fctr&gt;\n   1:       no     no     no     yes\n   2:       no     no    yes     yes\n   3:       no     no     no     yes\n   4:       no     no     no     yes\n   5:       no     no     no     yes\n  ---                               \n2376:       no     no     no     yes\n2377:      yes     no    yes     yes\n2378:       no     no     no     yes\n2379:      yes    yes    yes     yes\n2380:      yes     no    yes     yes\n\n\n\n\n2.2 Variable Definitions\nThe key variables in the HMDA dataset are:\n\n\n\n\n\n\n\nVariable\nDefinition\n\n\n\n\ndeny\nWas the mortgage application denied? (yes/no) — dependent variable\n\n\npirat\nPayments-to-income ratio (monthly loan payments / monthly income)\n\n\nafam\nIs the applicant African American? (yes/no)\n\n\nhirat\nHousing expense-to-income ratio (monthly housing expenses / monthly income)\n\n\nlvrat\nLoan-to-value ratio (loan amount / assessed property value)\n\n\nchist\nCredit history: consumer credit score\n\n\nmhist\nMortgage history: mortgage credit score\n\n\nphist\nPublic record history: coded as “no” or “yes” (any bankruptcies, tax liens, etc.)\n\n\ninsurance\nWas mortgage insurance denied? (yes/no)\n\n\nselfemp\nIs the applicant self-employed? (yes/no)\n\n\nsingle\nIs the applicant single? (yes/no)\n\n\nhschool\nDoes the applicant have high school education? (yes/no)\n\n\nunemp\n1989 Massachusetts unemployment rate in applicant’s industry (%).\n\n\ncondominium\nIs the property a condominium? (yes/no)\n\n\n\n\n\n2.3 Data Exploration\n\nFrequency Tables and Contingency Tables\nLet’s examine the denial rates overall and by race.\nWe first look at the overall denial counts and rates.\n\n# Overall denial counts\ncat(\"Overall Denial Counts:\\n\")\nwith(HMDA, table(deny))\n\n# Overall denial rate\ncat(\"===========================\\n\")\ncat(\"Overall Denial Rate:\\n\")\nwith(HMDA, prop.table(table(deny))) %&gt;% round(3)\n\nOverall Denial Counts:\ndeny\n  no  yes \n2095  285 \n===========================\nOverall Denial Rate:\ndeny\n  no  yes \n0.88 0.12 \n\n\nThe table shows that about 12% of mortgage applications were denied overall. The majority of applications (88%) were approved.\nNext, we examine denial counts and rates by race.\nA contingency table is an effective method to see the association between two categorical variables. It counts the number of observations in each of the four possible scenarios. When dealing with just one categorical variable, this is referred to as a frequency table, which count the number of observations for each category.\nThe following gives a 2x2 contingency table for mortgage denial by African-American or not.\n\n# Denial rate by race\ncat(\"Denail counts by race:\\n\")\nwith(HMDA, table(deny, afam))\ncat(\"===========================\\n\")\ncat(\"Denial rate by race:\\n\")\nwith(HMDA, prop.table(table(deny, afam), margin = 2)) %&gt;% round(3)\n\nDenail counts by race:\n     afam\ndeny    no  yes\n  no  1852  243\n  yes  189   96\n===========================\nDenial rate by race:\n     afam\ndeny     no   yes\n  no  0.907 0.717\n  yes 0.093 0.283\n\n\nSome observations from the table:\n\nThe majority of applicants are non-African American.\nAfrican American applicants have a higher denial rate (about 28%) compared to non-African American applicants (about 9%).\n\n\n\n\n\n\n\nThis descriptive evidence suggests that the likelihood of denial may be systematically higher for African American applicants. However, these simple proportions do not control for other relevant factors such as income, credit history, or loan-to-value ratio. Logistic regression will allow us to model this relationship more rigorously while accounting for these additional variables.\n\n\n\n\n\nSummary Statistics\n\nsummary(HMDA)\n\n  deny          pirat            hirat            lvrat        chist   \n no :2095   Min.   :0.0000   Min.   :0.0000   Min.   :0.0200   1:1353  \n yes: 285   1st Qu.:0.2800   1st Qu.:0.2140   1st Qu.:0.6527   2: 441  \n            Median :0.3300   Median :0.2600   Median :0.7795   3: 126  \n            Mean   :0.3308   Mean   :0.2553   Mean   :0.7378   4:  77  \n            3rd Qu.:0.3700   3rd Qu.:0.2988   3rd Qu.:0.8685   5: 182  \n            Max.   :3.0000   Max.   :3.0000   Max.   :1.9500   6: 201  \n mhist    phist          unemp        selfemp    insurance  condomin  \n 1: 747   no :2205   Min.   : 1.800   no :2103   no :2332   no :1694  \n 2:1571   yes: 175   1st Qu.: 3.100   yes: 277   yes:  48   yes: 686  \n 3:  41              Median : 3.200                                   \n 4:  21              Mean   : 3.774                                   \n                     3rd Qu.: 3.900                                   \n                     Max.   :10.600                                   \n  afam      single     hschool   \n no :2041   no :1444   no :  39  \n yes: 339   yes: 936   yes:2341  \n                                 \n                                 \n                                 \n                                 \n\n\nSummary statistics by race.\n\n# Summary statistics by race\nHMDA %&gt;%\n    group_by(afam) %&gt;%\n    summarise(\n        n = n(),\n        denial_rate = mean(deny == \"yes\"),\n        mean_pirat = mean(pirat),\n        mean_lvrat = mean(lvrat)\n    )\n\n# A tibble: 2 × 5\n  afam      n denial_rate mean_pirat mean_lvrat\n  &lt;fct&gt; &lt;int&gt;       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 no     2041  0.09260167  0.3274625  0.7259712\n2 yes     339  0.2831858   0.3509891  0.8088478\n\n\n\n\n\n2.4 Visualizing the Contingency Table\nWe can visualize the contingency table using a Stacked Bar Plot.\n\n\n\n\n\n\n\n\n\nThe stacked bar graph shows:\n\nThe sample sizes of African American and non-African American applicants\nThe distribution of approved vs. denied applications within each racial group\nAfrican American applicants appear to have a higher proportion of denials\n\nIssue: When the groups have very different sizes, it can be hard to compare proportions using absolute frequencies.\nRemedy: Use a mosaic plot to visualize relative frequencies.\n\nMosaic Plot\nA mosaic plot replaces absolute frequencies with relative frequencies, making it easier to compare proportions across groups.\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nThe widths of the boxes are proportional to the percentage of each racial group in the sample.\nThe heights represent the denial rates within each group.\nWe can see that the “Denied” box for African American applicants is taller than for non-African American applicants, indicating a higher denial rate.\n\n\n\n\n2.5 Measures of Risk and Association for Binary Outcomes\nTo quantify the difference in mortgage denial rates between racial groups, we can calculate several measures of risk and association.\n\n# Calculate risk (denial rate) for each group\nrisk_table &lt;- contingency_table %&gt;%\n    prop.table(margin = 2) %&gt;%\n    as.data.frame.matrix()\nrisk_table %&gt;% round(3)\n\n         Non-African American African American\nApproved                0.907            0.717\nDenied                  0.093            0.283\n\n\n\n# Extract probabilities\np_non_afam &lt;- risk_table[\"Denied\", \"Non-African American\"]  # P(deny=1 | afam=0)\np_afam &lt;- risk_table[\"Denied\", \"African American\"]          # P(deny=1 | afam=1)\n\ncat(\"Risk (Denial Rate) by Race:\\n\")\ncat(\"===========================\\n\")\ncat(sprintf(\"Non-African American: %.4f (%.2f%%)\\n\", p_non_afam, p_non_afam * 100))\ncat(sprintf(\"African American:     %.4f (%.2f%%)\\n\", p_afam, p_afam * 100))\n\nRisk (Denial Rate) by Race:\n===========================\nNon-African American: 0.0926 (9.26%)\nAfrican American:     0.2832 (28.32%)\n\n\n\nRisk Difference (Excess Risk)\nThe risk difference or excess risk (ER) is the difference in denial rates between the two groups:\n\\[\nER = P(\\text{deny}=1|\\text{afam}=1) - P(\\text{deny}=1|\\text{afam}=0)\n\\]\n\nER &lt;- p_afam - p_non_afam\ncat(\"\\nRisk Difference (Excess Risk):\\n\")\ncat(\"==============================\\n\")\ncat(sprintf(\"ER = %.4f - %.4f = %.4f (%.2f percentage points)\\n\", p_afam, p_non_afam, ER, ER * 100))\n\n\nRisk Difference (Excess Risk):\n==============================\nER = 0.2832 - 0.0926 = 0.1906 (19.06 percentage points)\n\n\nInterpretation: African American applicants have a denial rate that is 19.06 percentage points higher than non-African American applicants.\n\nIf \\(ER = 0\\): no difference in risk between groups\nIf \\(ER &gt; 0\\): higher risk for the treatment group (African Americans, afam=1)\nBy contrast, the control group is non-African Americans (afam=0).\nIf \\(ER &lt; 0\\): lower risk for the treatment group\n\nRisk Ratio (Relative Risk)\nThe risk ratio or relative risk (RR) is the ratio of denial rates:\n\\[\nRR = \\frac{P(\\text{deny}=1|\\text{afam}=1)}{P(\\text{deny}=1|\\text{afam}=0)}\n\\]\n\nRR &lt;- p_afam / p_non_afam\ncat(\"\\nRisk Ratio (Relative Risk):\\n\")\ncat(\"===========================\\n\")\ncat(sprintf(\"RR = %.4f / %.4f = %.4f\\n\", p_afam, p_non_afam, RR))\n\n\nRisk Ratio (Relative Risk):\n===========================\nRR = 0.2832 / 0.0926 = 3.0581\n\n\nInterpretation: African American applicants have a denial rate that is 3.06 times higher than non-African American applicants.\n\nIf \\(RR = 1\\): no difference in risk\nIf \\(RR &gt; 1\\): higher risk for the treatment group (African Americans, afam=1)\nIf \\(RR &lt; 1\\): lower risk for the treatment group\n\nOdds Ratio\nThe odds ratio (OR) compares the odds of denial between the two groups.\nFirst, calculate the odds for each group:\n\\[\n\\text{odds} = \\frac{P(\\text{deny}=1)}{P(\\text{deny}=0)} = \\frac{P(\\text{deny}=1)}{1 - P(\\text{deny}=1)}\n\\]\n\n# Calculate odds for each group\nodds_non_afam &lt;- p_non_afam / (1 - p_non_afam)\nodds_afam &lt;- p_afam / (1 - p_afam)\n\ncat(\"\\nOdds by Race:\\n\")\ncat(\"=============\\n\")\ncat(sprintf(\"Non-African American: %.4f\\n\", odds_non_afam))\ncat(sprintf(\"African American:     %.4f\\n\", odds_afam))\n\n\nOdds by Race:\n=============\nNon-African American: 0.1021\nAfrican American:     0.3951\n\n\nThis means that the odds of being denied a mortgage are approximately 0.10 for non–African American applicants and 0.40 for African American applicants.\nThe odds ratio is:\n\\[\nOR = \\frac{\\text{odds}(\\text{afam}=1)}{\\text{odds}(\\text{afam}=0)} = \\frac{P(\\text{deny}=1|\\text{afam}=1)/[1-P(\\text{deny}=1|\\text{afam}=1)]}{P(\\text{deny}=1|\\text{afam}=0)/[1-P(\\text{deny}=1|\\text{afam}=0)]}\n\\]\n\nOR &lt;- odds_afam / odds_non_afam\ncat(\"\\nOdds Ratio:\\n\")\ncat(\"===========\\n\")\ncat(sprintf(\"OR = %.4f/%.4f = %.4f\\n\", odds_afam, odds_non_afam, OR))\n\n\nOdds Ratio:\n===========\nOR = 0.3951/0.1021 = 3.8712\n\n\nInterpretation: The odds of mortgage denial for African American applicants are 3.87 times higher than the odds for non-African American applicants.\n\nIf \\(OR = 1\\): no difference in odds\nIf \\(OR &gt; 1\\): higher odds for the treatment group (African Americans, afam=1)\nIf \\(OR &lt; 1\\): lower odds for the treatment group\n\n\n\n\n\n\n\n\nImportant Distinction\n\n\n\n\nRisk difference measures the absolute difference in probabilities (additive scale)\nRisk ratio and odds ratio measure relative differences (multiplicative scale)\nThe odds ratio is particularly useful because it directly relates to the coefficient in logistic regression:\nWhen we estimate a logistic model with afam as the predictor, \\(e^{\\beta_{\\text{afam}}}\\) will equal the odds ratio we calculated here.",
    "crumbs": [
      "Home",
      "Basics",
      "Regression with a Binary Dependent Variable"
    ]
  },
  {
    "objectID": "09_Logistic_Regression.html#model-specification-the-boston-mortgage-example",
    "href": "09_Logistic_Regression.html#model-specification-the-boston-mortgage-example",
    "title": "Regression with a Binary Dependent Variable",
    "section": "3 Model Specification: The Boston Mortgage Example",
    "text": "3 Model Specification: The Boston Mortgage Example\nWe want to model the probability that a mortgage application is denied as a function of applicant characteristics.\n\n3.1 Preparing the Data\nFirst, we create a binary numeric variable for denial (1 = denied, 0 = approved) and recode some factors:\n\n# Create binary dependent variable\nHMDA$deny_binary &lt;- ifelse(HMDA$deny == \"yes\", 1, 0)\n\n# Create binary variable for African American\nHMDA$afam_binary &lt;- ifelse(HMDA$afam == \"yes\", 1, 0)\n\n# Check the variables\nhead(HMDA[, c(\"deny\", \"deny_binary\", \"pirat\", \"afam\", \"afam_binary\")])\n\n  deny deny_binary pirat afam afam_binary\n1   no           0 0.221   no           0\n2   no           0 0.265   no           0\n3   no           0 0.372   no           0\n4   no           0 0.320   no           0\n5   no           0 0.360   no           0\n6   no           0 0.240   no           0\n\n\n\n\n3.2 Model 1: Linear Probability Model\nLet’s start with a simple linear regression to see its limitations.\nThe OLS regression of the binary dependent variable, \\(deny\\) against the payment-to-income ratio (pirat), is estimated as:\n\\[\ndeny = \\beta_0 + \\beta_1 \\cdot pirat + \\varepsilon\n\\]\n\nlpm_simple &lt;- lm(deny_binary ~ pirat, data = HMDA)\ncoeftest(lpm_simple, vcov. = vcovHC, type = \"HC1\")\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) -0.079910   0.031967 -2.4998   0.01249 *  \npirat        0.603535   0.098483  6.1283 1.036e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\\[\n\\widehat{deny} = -0.080 + 0.604 \\cdot pirat\n\\]\nThe estimated coefficient on P/I ratio is positive, and the population coefficient is statistically significantly different from 0 at the 1% level (the t-statistic is 6.12). If P/I ratio increases by 0.1, the probability of denial increases by approximately \\(0.604\\times 0.1 \\approx 0.060,\\) that is, by 6 percentage points. Thus applicants with higher debt payments as a fraction of income are more likely to have their application denied.\nNow we plot the data and the regression line to visualize the model.\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\n\n\n\nShortcomings with the linear probability model\n\n\n\n\nThe linear probability model can predict probabilities outside the \\([0,1]\\) range. For example, for high values of P/I ratio (pirat), the predicted probability exceeds 1. However, probabilities must lie between 0 and 1.\nThe relationship between P/I ratio and the probability of denial may NOT be linear in reality.\nIt is reasonable to expect the marginal effects of P/I ratio on denial probability to diminish as P/I ratio increases. Although a change in P/I ratio from 0.4 to 0.4 might have a large effect on the probability of denial, once the P/I ratio is already very high (e.g., 0.9 to 1.0), increasing P/I ratio further will have litte effect.\nThe error term in the linear probability model is heteroskedastic, violating OLS assumptions.\nThis means that standard errors and hypothesis tests based on OLS are invalid. → This issue can be addressed using heteroskedasticity robust standard errors.\n\n\n\n\n\n3.3 Model 2: Logit Regression with a single predictor\nThe estimated model is \\[\n\\mathrm P(deny_i = 1 | pirat_i) = F(\\beta_0 + \\beta_1 \\cdot pirat_i) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 \\cdot pirat_i)}} ,\n\\]\nwhere \\(F\\) is the logistic distribution function.\nNote that the left-hand side is the probability that the \\(i\\)-th mortgage application is denied (\\(deny_i = 1\\)), given the payment-to-income ratio (\\(pirat_i\\)).\nThe strength of the logit model is that it ensures predicted probabilities are always between 0 and 1.\n\n# Estimate logit model\nlogit_simple &lt;- glm(deny_binary ~ pirat,\n    family = binomial(link = \"logit\"),\n    data = HMDA\n)\ncoeftest(logit_simple, vcov. = vcovHC, type = \"HC1\")\n\n\nz test of coefficients:\n\n            Estimate Std. Error  z value  Pr(&gt;|z|)    \n(Intercept) -4.02843    0.35898 -11.2218 &lt; 2.2e-16 ***\npirat        5.88450    1.00015   5.8836 4.014e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\\[\n\\widehat{\\mathrm P}(deny_i = 1 | pirat_i) = F(-4.02 + 5.88 \\cdot pirat_i)= \\frac{1}{1 + e^{-(-4.02 + 5.88 \\cdot pirat_i)}}\n\\]\nVisualizing the logit model:\n\n\n\n\n\n\n\n\nFigure 2: The logit model uses the logistic distribution function to model the probability of denial as a function of the payment-to-income ratio. Unlike the linear probability model, the logit model ensures predicted probabilities remain within the [0,1] range.\n\n\n\n\n\nThe estimated probit regression function has a stretched “S” shape: It is nearly 0 and flat for small values of P/I ratio, it turns and increases for intermediate values, and it flattens out again and is nearly 1 for large values.\n\nExample 1 Predicted probability.\nWhat is the probability of denial given a P/I ratio of 0.3? What about for P/I ratio being 0.4 and 0.5?\n\n\nSolution\n\n\nSolution 1. For P/I ratio of 0.3, the estimated probability of denial based on the estimated logit model is: \\[\n\\widehat{P}(deny = 1 | pirat = 0.3) = \\frac{1}{1 + e^{-(-4.02 + 5.88 \\cdot 0.3)}} \\approx 9.4\\%\n\\] That is, the probability of denial is approximately 9.4%.\nFor P/I ratio of 0.4: \\[\n\\widehat{P}(deny = 1 | pirat = 0.4) = \\frac{1}{1 + e^{-(-4.02 + 5.88 \\cdot 0.4)}} \\approx 15.7\\%\n\\] That is, the probability of denial is approximately 15.7%.\nFor P/I ratio of 0.5: \\[\n\\widehat{P}(deny = 1 | pirat = 0.5) = \\frac{1}{1 + e^{-(-4.02 + 5.88 \\cdot 0.5)}} \\approx 25.2\\%\n\\]\nMain takeaway: As P/I ratio increases, the probability of denial increases, but not linearly.\n\nInterpretation on the logit coefficients:\nThe estimated model is\n\\[\n\\widehat{P}(deny_i = 1 \\mid pirat_i) = \\frac{1}{1 + e^{-(-4.02 + 5.88 \\cdot pirat_i)}}\n\\]\nwhich can be written in log-odds form as\n\\[\n\\log\\left(\\frac{P(deny_i = 1)}{1 - P(deny_i = 1)}\\right) = -4.02 + 5.88 \\cdot pirat_i\n\\]\n\nInterpreting coefficients (log-odds scale):\nThe slope coefficient, \\(\\hat{\\beta}_1 = 5.88\\), means that a one-unit increase in pirat increases the log-odds of mortgage denial by 5.88, holding all else constant.\nInterpreting odds ratios (exponentiating coefficients):\nTo obtain a more intuitive interpretation, we exponentiate the coefficient: \\[\ne^{\\hat{\\beta}_1} = e^{5.88} \\approx 357.7\n\\] This means that for a one-unit increase in the payment-to-income ratio, the odds of mortgage denial are about 357.7 times larger.\n\nBecause a one-unit increase in pirat is very large in practice, it is often more meaningful to interpret smaller changes.\n\nFor example, for a 0.1 increase in pirat: \\[\ne^{5.88 \\times 0.1} \\approx 1.80\n\\] This means that a 0.1 increase in the payment-to-income ratio increases the odds of denial by about 80%.\n\nHypothesis Testing:\nUsing the normal distribution of parameter estimates, we can use the standard normal table rather than the \\(t\\) table for critical points to test hypotheses about the coefficients.\nThe \\(z\\)-statistic for testing \\(H_0: \\beta_1 = 0\\) is:\n\\[\nz = \\frac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)} = \\frac{5.88 - 0}{1} \\approx 5.88\n\\]\nSince \\(z &gt; 1.96\\), we reject the null hypothesis at the 5% significance level and conclude that there is a statistically significant positive relationship between payment-to-income ratio and the probability of mortgage denial.\n\n\n3.4 Model 3: Linear Probability Model with Multiple Predictors\n\\[\ndeny_i = \\beta_0 + \\beta_1 \\cdot pirat_i + \\beta_2 \\cdot afam_i\n\\]\n\n# Estimate LPM\nlpm_multi &lt;- lm(deny_binary ~ pirat + afam_binary, data = HMDA)\nsummary(lpm_multi)\n\n\nCall:\nlm(formula = deny_binary ~ pirat + afam_binary, data = HMDA)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.62526 -0.11772 -0.09293 -0.05488  1.06815 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.09051    0.02079  -4.354 1.39e-05 ***\npirat        0.55919    0.05987   9.340  &lt; 2e-16 ***\nafam_binary  0.17743    0.01837   9.659  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3123 on 2377 degrees of freedom\nMultiple R-squared:  0.076, Adjusted R-squared:  0.07523 \nF-statistic: 97.76 on 2 and 2377 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretation:\n\n\\(\\hat{\\beta}_1\\): A one-unit increase in the payment-to-income ratio increases the probability of denial by approximately \\(\\hat{\\beta}_1\\).\n\\(\\hat{\\beta}_2\\): African American applicants have a probability of denial that is \\(\\hat{\\beta}_2\\) percentage points higher than non-African American applicants, holding pirat constant.\n\n\n\n3.5 Model 4: Logit Model with Multiple Predictors\nNow we estimate a logit model including both P/I ratio (pirat) and African-American binary (afam_binary) as predictors:\n\\[\nP(deny_i = 1 | pirat_i, afam_i) = F(\\beta_0 + \\beta_1 \\cdot pirat_i + \\beta_2 \\cdot afam_i) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 \\cdot pirat_i + \\beta_2 \\cdot afam_i)}} .\n\\]\n\n# Estimate logit model\nlogit_multi &lt;- glm(deny_binary ~ pirat + afam_binary,\n    family = binomial(link = \"logit\"),\n    data = HMDA\n)\ncoeftest(logit_multi, vcov. = vcovHC, type = \"HC1\")\n\n\nz test of coefficients:\n\n            Estimate Std. Error  z value  Pr(&gt;|z|)    \n(Intercept) -4.12556    0.34597 -11.9245 &lt; 2.2e-16 ***\npirat        5.37036    0.96376   5.5723 2.514e-08 ***\nafam_binary  1.27278    0.14616   8.7081 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe estimated logit model is: \\[\n\\begin{split}\n\\widehat{P}(deny_i = 1 | pirat_i, afam_i) &= F(-4.13 + 5.37 \\cdot pirat_i + 1.27 \\cdot afam_i) \\\\\n&= \\frac{1}{1 + e^{-(-4.13 + 5.37 \\cdot pirat_i + 1.27 \\cdot afam_i)}}\n\\end{split}\n\\]\nInterpretation of Logit Coefficients\nThe coefficients in a logit model represent the change in the log-odds of denial:\n\\[\n\\log\\left(\\frac{P(deny = 1)}{1 - P(deny = 1)}\\right) = -4.13 + 5.37 \\cdot pirat_i + 1.27 \\cdot afam_i\n\\]\n1. Interpreting coefficients (log-odds scale):\n\n\\(\\hat{\\beta}_1 &gt; 0\\): A one-unit increase in pirat increases the log-odds of denial by \\(\\hat{\\beta}_1\\), holding other variables constant.\n\\(\\hat{\\beta}_2 &gt; 0\\): Being African American increases the log-odds of denial by \\(\\hat{\\beta}_2\\) compared to non-African Americans, holding other variables constant.\n\n2. Interpreting odds ratios (exponentiating coefficients):\nAlternatively, we can interpret the odds ratio by taking the exponential of the coefficients:\n\nFor pirat: \\(e^{\\hat{\\beta}_1}\\) represents the multiplicative change in odds for a one-unit increase in the payment-to-income ratio.\nFor example, if the P/I ratio increases by 0.2, then \\(e^{5.37\\times 0.2} = 2.93\\), that is, the odds of mortgage denial are approximately 2.94 times higher, holding afam constant.\nFor afam: \\(e^{\\hat{\\beta}_2}\\) is the odds ratio comparing African American applicants to non-African American applicants.\nBased on the estimates, \\(e^{1.2} = 3.56\\), that is, African American applicants have odds of denial that are 3.56 times higher than non-African American applicants, holding pirat constant.\n\n\nExample 2 Suppose we have:\n\nOdds of denial for a non-African American applicant with pirat = 0.3 is 0.15\nThe odds ratio for African Americans compared to non-African American is \\(e^{\\hat{\\beta}_2} = 3.56\\)\n\nCalculate the expected odds for an African American applicant with the same pirat = 0.3.\n\n\nSolution\n\n\nSolution 2. \\[\n\\text{odds}(\\text{afam}=1) = \\text{odds}(\\text{afam}=0) \\times e^{\\hat{\\beta}_2} = 0.15 \\times 3.56 = 0.534\n\\] The expected odds of denial for an African American applicant with pirat = 0.3 is 0.534.\n\n\n\n3.6 Model 5: Probit Model\nIn contrast to the logit model, which uses the logistic CDF, the probit model uses the standard normal distribution function to model the probability of denial. \\[\nP(deny_i = 1 | X_i) = \\Phi(\\beta_0 + \\beta_1 \\cdot pirat_i + \\beta_2 \\cdot afam_i)\n\\]\nwhere \\(\\Phi(\\cdot)\\) is the standard normal CDF.\n\n# Estimate probit model\nprobit_multi &lt;- glm(deny_binary ~ pirat + afam_binary,\n    family = binomial(link = \"probit\"),\n    data = HMDA\n)\ncoeftest(probit_multi, vcov. = vcovHC, type = \"HC1\")\n\n\nz test of coefficients:\n\n             Estimate Std. Error  z value  Pr(&gt;|z|)    \n(Intercept) -2.258787   0.176608 -12.7898 &lt; 2.2e-16 ***\npirat        2.741779   0.497673   5.5092 3.605e-08 ***\nafam_binary  0.708155   0.083091   8.5227 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n3.7 Comparison: Logit, and Probit\n\n\n\n\n\n\n\n\n\nPractical Recommendation:\n\nLogit and probit typically give very similar results.\nLogit is more common in economics and finance due to the convenient odds ratio interpretation.\n\n\n\n3.8 Predicted Probabilities\nTo compare effects across models, we calculate predicted probabilities at specific values of the predictors.\n\nExample 3 For the three multivariate models (models 3–5), what is the predicted probability of denial for:\n\nAn African American applicant with pirat = 0.3?\nA non-African American applicant with pirat = 0.3?\n\n\n\nSolution\n\n\nSolution 3. For linear probability model (LPM): - African American applicant (afam_binary = 1): \\[\n  \\widehat{deny} = -0.09 + 0.56 \\times 0.3 + 0.18 \\times 1 = 0.258\n  \\] - Non-African American applicant (afam_binary = 0): \\[\n  \\widehat{deny} = -0.09 + 0.56 \\times 0.3 + 0.18 \\times 0 = 0.078\n  \\]\nFor logit model: - African American applicant: \\[\n  \\widehat{P}(deny = 1 | pirat = 0.3, afam = 1) = \\frac{1}{1 + e^{-(-4.13 + 5.37 \\times 0.3 + 1.27 \\times 1)}} \\approx 0.22\n  \\] - Non-African American applicant: \\[\n  \\widehat{P}(deny = 1 | pirat = 0.3, afam = 1) = \\frac{1}{1 + e^{-(-4.13 + 5.37 \\times 0.3 + 1.27 \\times 0)}} \\approx 0.07\n  \\]\nFor probit model: - African American applicant: \\[\n  \\widehat{P}(deny = 1 | pirat = 0.3, afam = 1) = \\Phi(-2.26 + 2.74 \\times 0.3 + 0.71 \\times 1) \\approx 0.23\n  \\] - Non-African American applicant: \\[\n  \\widehat{P}(deny = 1 | pirat = 0.3, afam = 0) = \\Phi(-2.26 + 2.74 \\times 0.3 + 0.71 \\times 0) \\approx 0.08\n  \\]",
    "crumbs": [
      "Home",
      "Basics",
      "Regression with a Binary Dependent Variable"
    ]
  },
  {
    "objectID": "09_Logistic_Regression.html#summary-logit-vs.-probit-vs.-lpm",
    "href": "09_Logistic_Regression.html#summary-logit-vs.-probit-vs.-lpm",
    "title": "Regression with a Binary Dependent Variable",
    "section": "4 Summary: Logit vs. Probit vs. LPM",
    "text": "4 Summary: Logit vs. Probit vs. LPM\n\n\n\n\n\n\n\n\n\n\nLinear\nLogit\nProbit\n\n\n\n\nFunction\nLinear\nLogistic CDF\nNormal CDF\n\n\nPredicted probabilities\nCan be outside [0,1]\nAlways in [0,1]\nAlways in [0,1]\n\n\nInterpretation\nDirect (probability)\nLog-odds / Odds ratio\nZ-score\n\n\nHeteroskedasticity\nAlways present\nAccounted for\nAccounted for\n\n\nWhen to use\nQuick approximation\nPreferred for most applications\nSimilar to logit; standard in some fields",
    "crumbs": [
      "Home",
      "Basics",
      "Regression with a Binary Dependent Variable"
    ]
  },
  {
    "objectID": "06_Multiple_Linear_Regression.html",
    "href": "06_Multiple_Linear_Regression.html",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "🎯 Study Objectives\n\nUnderstand the concept of multiple linear regression (MLR) and how it extends simple linear regression (SLR).\nPerform multiple regression analysis using R and interpret the results.\nLearn how to interpret the coefficients in a multiple regression model.\nUnderstand the importance of controlling for confounding variables in regression analysis.\nConduct and interpret F-tests for the overall significance of a regression model.\nExplore alternative model specifications and their implications on regression results.\nThe multiple regression model extends the basic concept of the simple regression model to include multiple explanatory variables. This allows us to analyze the relationship between a dependent variable and several independent variables simultaneously.\nA multiple regression model enables us to estimate the effect on \\(Y_i\\) of changing a regressor \\(X_{1i}\\) if the remaining regressors \\(X_{2i}, X_{3i}, \\ldots, X_{ki}\\) are held constant.\nJust like in the simple regression model, we assume the true relationship between \\(Y_i\\) and \\(X_{1i}, X_{2i}, X_{3i}, \\ldots, X_{ki}\\) to be linear. The relation is given by the population regression function:\n\\[\nY_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{3i} + \\ldots + \\beta_k X_{ki} + \\varepsilon_i .\n\\]\nNow we go back to the example of test scores of students in California schools. Recall in the simple regression model, we regression test score on expenditure per student and found statistically significant positive effect. However, the R-squared (3.7%) was rather low and residuals plot shows substantial noise, indicating that there are other factors affecting test scores.\n💡 We can improve the model by including more explanatory variables.\nWe expect that smaller class sizes lead to better test scores, as teachers can give more individual attention to each student. Therefore, we introduce class size (number of students per teacher) as an additional regressor.\nThe multiple regression model is then given by:\n\\[\n\\text{TestScore}_i = \\beta_0 + \\beta_1 \\times \\text{expenditure}_i + \\beta_2 \\times \\text{STR} + \\varepsilon_i\n\\]\nwhere \\(\\text{STR}\\) is the student-teacher ratio (representing class size).",
    "crumbs": [
      "Home",
      "Basics",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "06_Multiple_Linear_Regression.html#summary-statistics",
    "href": "06_Multiple_Linear_Regression.html#summary-statistics",
    "title": "Multiple Linear Regression",
    "section": "1.1 Summary Statistics",
    "text": "1.1 Summary Statistics\n\n# Data Preparation\nlibrary(tidyverse)\n\nf_name &lt;- \"https://raw.githubusercontent.com/my1396/FIN5005-Fall2025/refs/heads/main/data/CASchools_test_score.csv\"\ncas &lt;- read_csv(f_name,\n    col_types = cols(\n        county = col_factor(), # read as factor\n        grades = col_factor()\n    )\n)\ncas &lt;- cas %&gt;%\n    mutate(\n        TestScore = (read + math) / 2,\n        STR = students / teachers\n    )\n# summary statistics\ncas %&gt;%\n    select(TestScore, expenditure, STR) %&gt;%\n    summary()\n\n   TestScore      expenditure        STR       \n Min.   :605.6   Min.   :3926   Min.   :14.00  \n 1st Qu.:640.0   1st Qu.:4906   1st Qu.:18.58  \n Median :654.5   Median :5215   Median :19.72  \n Mean   :654.2   Mean   :5312   Mean   :19.64  \n 3rd Qu.:666.7   3rd Qu.:5601   3rd Qu.:20.87  \n Max.   :706.8   Max.   :7712   Max.   :25.80",
    "crumbs": [
      "Home",
      "Basics",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "06_Multiple_Linear_Regression.html#scatter-plot",
    "href": "06_Multiple_Linear_Regression.html#scatter-plot",
    "title": "Multiple Linear Regression",
    "section": "1.2 Scatter Plot",
    "text": "1.2 Scatter Plot\nScatter plot of TestScore against expenditure and STR:\n\n\n\n\n\n\n\n\nFigure 1: Scatter plots of TestScore vs expenditure and STR. STR stands for students-teach ratio, proxying class size.\n\n\n\n\n\nThere seems to be a negative correlation between class size (STR) and test scores, as expected.\nUse cor() to compute the correlation matrix.\n\ncor(cas %&gt;% select(TestScore, expenditure, STR)) %&gt;% round(2)\n\n            TestScore expenditure   STR\nTestScore        1.00        0.19 -0.23\nexpenditure      0.19        1.00 -0.62\nSTR             -0.23       -0.62  1.00",
    "crumbs": [
      "Home",
      "Basics",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "06_Multiple_Linear_Regression.html#interpretation-of-the-coefficients",
    "href": "06_Multiple_Linear_Regression.html#interpretation-of-the-coefficients",
    "title": "Multiple Linear Regression",
    "section": "2.1 Interpretation of the Coefficients",
    "text": "2.1 Interpretation of the Coefficients\nThe estimated model is\n\\[\n\\widehat{\\text{TestScore}} = 675.58 + 0.0025 \\cdot \\text{expenditure} - 1.76 \\cdot \\text{STR}.\n\\]\n\nSTR (–1.76): Holding expenditure constant, an additional student per teacher lowers the test score on average by 1.76 points. This effect is statistically significant at the 1% level.\nExpenditure (0.0025): Holding STR constant, a one-unit increase in expenditure is associated with an increase of 0.0025 points in the test score. This effect is very small and statistically insignificant (p-value = 0.17).",
    "crumbs": [
      "Home",
      "Basics",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "06_Multiple_Linear_Regression.html#compare-with-simple-regression",
    "href": "06_Multiple_Linear_Regression.html#compare-with-simple-regression",
    "title": "Multiple Linear Regression",
    "section": "2.2 Compare with Simple Regression",
    "text": "2.2 Compare with Simple Regression\n\nlibrary(stargazer)\nmodel_simple &lt;- lm(TestScore ~ expenditure, data = cas)\nstargazer(model_simple, model_multiple,\n    digits = 4,\n    type = \"html\",\n    dep.var.labels = \"Test Score\",\n    column.labels = c(\"SLR\", \"MLR\"),\n    covariate.labels = c(\"Expenditure\", \"STR\"),\n    notes = \"&lt;span&gt;&#42;&lt;/span&gt;: p&lt;0.1; &lt;span&gt;&#42;&#42;&lt;/span&gt;: &lt;strong&gt;p&lt;0.05&lt;/strong&gt;; &lt;span&gt;&#42;&#42;&#42;&lt;/span&gt;: p&lt;0.01 &lt;br&gt; Standard errors in parentheses.\",\n    notes.append = F\n)\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nTest Score\n\n\n\n\n\n\nSLR\n\n\nMLR\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nExpenditure\n\n\n0.0057***\n\n\n0.0025\n\n\n\n\n\n\n(0.0014)\n\n\n(0.0018)\n\n\n\n\n\n\n\n\n\n\n\n\nSTR\n\n\n\n\n-1.7632***\n\n\n\n\n\n\n\n\n(0.6109)\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n623.6165***\n\n\n675.5772***\n\n\n\n\n\n\n(7.7197)\n\n\n(19.5622)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n420\n\n\n420\n\n\n\n\nR2\n\n\n0.0366\n\n\n0.0555\n\n\n\n\nAdjusted R2\n\n\n0.0343\n\n\n0.0509\n\n\n\n\nResidual Std. Error\n\n\n18.7239 (df = 418)\n\n\n18.5619 (df = 417)\n\n\n\n\nF Statistic\n\n\n15.8734*** (df = 1; 418)\n\n\n12.2409*** (df = 2; 417)\n\n\n\n\n\n\n\n\nNote:\n\n\n*: p&lt;0.1; **: p&lt;0.05; ***: p&lt;0.01  Standard errors in parentheses.\n\n\n\n\nIn the simple regression of TestScore on expenditure:\n\\[\n\\widehat{\\text{TestScore}} = 623.6 + 0.0057 \\times \\text{expenditure},\n\\]\n\nExpenditure had a positive and statistically significant effect (p &lt; 0.001).\nThe effect size (0.0057 vs. 0.0025) was larger than in the multiple regression.\nThe overall explanatory power was improved (Adjusted \\(R^2\\) ≈ 0.0343 in the SLR vs. \\(R^2\\) ≈ 0.0509 in the MLR).\n\nThus, once STR is included, the apparent effect of expenditure becomes much smaller and loses statistical significance.\n\nQ: Why does the effect of expenditure change when STR is included?\nA: Expenditure is no longer significant in the multiple regression because expenditure and STR are correlated. Districts that spend more on education often also have smaller class sizes (\\(\\rho = -0.62\\)).\nIn the simple regression, the positive effect of expenditure partly reflected the fact that higher spending was associated with lower STR, which itself improves test scores. Once STR is explicitly controlled for, the “true” partial effect of expenditure (holding class size constant) is close to zero.\nThis illustrates the importance of multiple regression: it separates the effect of each regressor while holding others fixed, and it reveals that the real driver of test scores here is class size, not expenditure.",
    "crumbs": [
      "Home",
      "Basics",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "06_Multiple_Linear_Regression.html#regression-diagnostics",
    "href": "06_Multiple_Linear_Regression.html#regression-diagnostics",
    "title": "Multiple Linear Regression",
    "section": "2.3 Regression diagnostics",
    "text": "2.3 Regression diagnostics\n\nResidual Standard Error (RSE):\n\nSLR: 18.72 (df = 418) → MLR: 18.56 (df = 417)\nThe MLR has a slightly smaller RSE, meaning it predicts test scores a bit more accurately.\n\nMultiple \\(R^2\\):\n\nSLR: 0.037 → MLR: 0.055\nAdding STR increases the proportion of variation explained, though the overall fit remains modest.\n\nAdjusted \\(R^2\\):\n\nSLR: 0.034 → MLR: 0.051\nThe adjusted \\(R^2\\) also improves, showing STR provides genuine explanatory power beyond expenditure.\n\nF-statistic (overall significance):\n\nSLR: 15.87, p &lt; 0.001 → MLR: 12.24, p &lt; 0.001\nBoth models are statistically significant overall. The MLR has a smaller F-statistic because it includes more regressors, but still clearly improves explanatory power.",
    "crumbs": [
      "Home",
      "Basics",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "06_Multiple_Linear_Regression.html#example-f-test-for-the-multiple-regression-model",
    "href": "06_Multiple_Linear_Regression.html#example-f-test-for-the-multiple-regression-model",
    "title": "Multiple Linear Regression",
    "section": "3.1 Example: F-test for the Multiple Regression Model",
    "text": "3.1 Example: F-test for the Multiple Regression Model\n\n\nNull and alternative hypotheses \\[\n\\begin{aligned}\n&\\text{H}_0: \\beta_1 = \\beta_2 = 0 \\\\\n&\\text{H}_1: \\text{At least one of the } \\beta_1, \\beta_2,\\text{is not zero.}\n\\end{aligned}\n\\] In plain language,\n\n\\(\\text{H}_0\\): Expenditure and STR have no effect on test scores.\n\\(\\text{H}_1\\): At least one of expenditure and STR has an effect on test scores.\n\nCalculate the test statistic \\[\nF=\\frac{(R^{2}/k)}{(1-R^{2})/(n-k-1)}\n\\]\nwhere \\(k\\) is the number of regressors (not counting the intercept) and \\(n\\) is the sample size.\nFrom the regression output, we have\n\n\\(R^{2}/k = 0.05545/2 = 0.027725.\\)\n\\((1-R^{2})/(n-k-1) = (1-0.05545)/417 \\approx 0.00226511\\)\n\nHence, the \\(F\\) statistic is \\[\nF = \\frac{0.027725}{0.00226511} \\approx 12.24 .\n\\]\nCritical value at \\(\\alpha=5\\%\\)\nDegrees of freedom for the \\(F\\) distribution are \\((df_{1}, df_{2})=(k, n-k-1)=(2, 417)\\).\nReferring to the \\(F\\)-distribution table to find the critical value.\nAccording to the \\(F\\) table, there is no exact value for \\(df_2=417\\), we use the value for \\(df_2=\\infty\\) as an approximation.\nWe use the The critical value \\(F_{0.95}(2, \\infty) = 3.00\\). The exact value is \\(F_{0.95}(2, 417)\\) is expected to be slightly larger than \\(3.00\\). But since our calculated \\(F\\) statistic is much larger than \\(3.00\\), we can safely reject the null hypothesis. It does not matter if the exact critical value is \\(3.01\\) or \\(3.06\\).\nDecision rule and conclusion\nDecision rule at 5 percent significance level: reject \\(H_{0}\\) if \\(F&gt;F_{\\text{crit}}.\\)\nSince \\(F_{\\text{obs}}=12.24&gt;F_{\\text{crit}}=3.00\\), we reject \\(H_{0}\\) and conclude that at least one of expenditure and STR has a statistically significant effect on test scores.\nInterpretation in context\nStatistically the regression is significant overall. That means at least one of the regressors, expenditure or STR, is linearly related to test scores.\nFrom the coefficient t tests in your summary we know STR is the significant predictor while expenditure is not significant once STR is included. In plain language this suggests that class size is driving the overall significance of the model rather than expenditure.",
    "crumbs": [
      "Home",
      "Basics",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "06_Multiple_Linear_Regression.html#interpretations",
    "href": "06_Multiple_Linear_Regression.html#interpretations",
    "title": "Multiple Linear Regression",
    "section": "4.1 Interpretations",
    "text": "4.1 Interpretations\n\nAdding control variables roughly halves the coefficient on STR.\n\nThe estimate is sensitive to the set of control variables used.\n\nConclusion: decreasing the student–teacher ratio ceteris paribus by one unit leads to an estimated average increase in test scores of about 1 point.\n“Ceteris Paribus” means “all other things being equal”, i.e., holding all other regressors constant.\n\n\n\nAdding student characteristics as controls increases\n\n\\(R^{2}\\) and \\(\\bar{R}^{2}\\) from 0.049 (spec (1)) up to 0.773 (spec (3) and spec (5)).\n\nThese variables can therefore be considered suitable predictors of test scores.\n\n\n\n\ncalworks is not statistically significant in all models.\n\nExample: in spec (5), the coefficient on calworks is not significantly different from zero at the 5% level since \\[\n\\left|\\frac{-0.048}{0.059}\\right| = 0.81 &lt; 1.96.\n\\] For \\(df = 415\\), we can safely use the normal distribution table to find the critical value for a two-sided t-test at the 5% significance level as the sample size is large (\\(df&gt;30\\)).\nCommonly used critical values under normal distribution:\n\n1.96 for 5% significance level;\n1.64 for 10% significance level; and\n2.58 for 1% significance level.\n\n\nThe effect of adding calworks to the base specification (spec (3)) on the coefficient of size and its standard error is negligible.\nWe can therefore consider calworks as a superfluous control variable, given the inclusion of lunch in this model.\nThe two variables are highly correlated (\\(\\rho = 0.74\\)). See Figure 2 for a scatter plot.\n\ncat(\"Correlation between lunch and calworks: \\n\\n\")\ncor(cas %&gt;% select(lunch, calworks)) %&gt;% round(2)\n\nCorrelation between lunch and calworks: \n\n         lunch calworks\nlunch     1.00     0.74\ncalworks  0.74     1.00\n\n\n\n\n\n\n\n\n\n\nFigure 2: Scatter plots of lunch vs. calworks. Positive linear relationship between the two variables.\n\n\n\n\n\n\n\nRef:\n\nSection 7.6 Analysis of the Test Score Data Set, Introduction to Econometrics with R, https://www.econometrics-with-r.org/7-6-analysis-of-the-test-score-data-set.html",
    "crumbs": [
      "Home",
      "Basics",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "04_Theoretical_Framework_LR.html",
    "href": "04_Theoretical_Framework_LR.html",
    "title": "Theoretical Framework of Linear Regression",
    "section": "",
    "text": "🎯 Study Objectives\n\nUnderstand the key assumptions underlying the linear regression model.\nUnderstand the theoretical framework of linear regression, including the OLS estimator, its properties, and its distribution.\nCentral Limit Theorem (CLT)\n\nApply the CLT to approximate the distribution of sample mean and calculating probabilities using distribution table.\nUnderstand the implications of CLT for the sampling distribution of the OLS estimator.\n\nDistinguish the difference between exact distribution and sampling distribution of the OLS estimator.\nUnderstand R squared and adjusted R squared as measures of fit.\nBe able to identify violations of the linear regression assumptions using diagnostic plots.\nUnderstand how to address heteroskedasticity.\nIn this section, we will discuss the theoretical framework of linear regression, including the assumptions underlying the linear regression model and essential hypothesis tests.\nQ: Why the assumptions matter?\nA: In short, violation of the assumptions may lead to biased or inefficient estimates, incorrect inferences, and unreliable predictions.",
    "crumbs": [
      "Home",
      "Basics",
      "Theoretical Framework of Linear Regression"
    ]
  },
  {
    "objectID": "04_Theoretical_Framework_LR.html#assumptions-of-linear-regression",
    "href": "04_Theoretical_Framework_LR.html#assumptions-of-linear-regression",
    "title": "Theoretical Framework of Linear Regression",
    "section": "1 Assumptions of Linear Regression",
    "text": "1 Assumptions of Linear Regression\nQ: What are the assumptions of linear regression?\nA: The Gauss-Markov Assumptions ensure that the Ordinary Least Squares (OLS) estimator is the Best Linear Unbiased Estimator (BLUE).\n\nLinearity: The relationship between the independent variable(s) and the dependent variable is linear in parameters.\nNo perfect multicollinearity: The independent variables are NOT perfectly correlated.\nError has zero conditional mean\nError is homoskedastic\n\nHomoskedasticity: The variance of the error term is constant across all levels of the independent variable(s).\n\nIn time series settings (we won’t cover in this course), we also assume:\n\nNo autocorrelation: The error terms are not correlated with each other.\n\nResiduals are normally distributed\nThis assumption is useful for conducting hypothesis tests and constructing confidence intervals.\nLarge outliers are unlikely\nThis assumption requires that observations with values of \\(X_i\\), \\(Y_i\\), or both that are far outside the usual range of the data are unlikely.\nLarge outliers can make OLS regression results misleading as OLS are sensitive to outliers.\nMathematically, this assumption can be stated as: \\(X\\) and \\(Y\\) have finite kurtosis.",
    "crumbs": [
      "Home",
      "Basics",
      "Theoretical Framework of Linear Regression"
    ]
  },
  {
    "objectID": "04_Theoretical_Framework_LR.html#regression-framework-and-ols-estimator",
    "href": "04_Theoretical_Framework_LR.html#regression-framework-and-ols-estimator",
    "title": "Theoretical Framework of Linear Regression",
    "section": "2 Regression Framework and OLS Estimator",
    "text": "2 Regression Framework and OLS Estimator\nMultiple linear regression models is defined as\n\\[\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + ... + \\beta_k X_{ik} + \\varepsilon_i\n\\]\nwhere\n\n\\(i=1,\\ldots, n\\) indexes the observations,\n\\(Y_i\\) is the dependent variable, and\n\\(X_{ij}\\) represents the \\(j\\)-th independent variable for observation \\(i\\), with \\(j = 1, \\ldots, k\\) and \\(k\\) denoting the total number of independent variables.\nThe coefficients \\(\\beta_j\\) are to be estimated, and\n\\(\\varepsilon_i\\) is the error term.\n\nThe OLS estimator \\(\\hat{\\beta}^{OLS}\\) minimizes the sum of squared residuals (SSR):\n\\[\n\\hat{\\beta}^{OLS} = \\arg\\min_{\\beta} \\sum_{i=1}^{n} e_i^2\n\\]\nwhere \\(e_i = Y_i - \\hat{Y}_i\\) is the residual for observation \\(i\\).\n\\(\\hat{Y}_i\\) is the predicted value of \\(Y_i\\) given the estimated coefficients.\n\\[\n\\hat{\\beta}^{OLS} = \\arg\\min_{\\beta} \\sum_{i=1}^{n} (Y_i - \\beta_0 - \\sum_{j=1}^{k} \\beta_j X_{ij})^2\n\\]\n\\(\\hat{\\beta}^{OLS}\\) can be obtained by the first order condition (setting the derivative of SSR with respect to each \\(\\beta_j\\) to zero and solving the resulting system of equations).\nRead OLS in Matrix Form for more details. We won’t cover the OLS derivation in this course.\nHere we give the direct formula for the OLS estimator:\n\\[\n\\hat\\beta = \\left(\\sum_{i=1}^n X_iX_i'\\right)^{-1} \\left(\\sum_{i=1}^n X_iY_i\\right)\n\\]\nwhere\n\\[\nX_i = \\begin{pmatrix}1 \\\\ X_{i1} \\\\ X_{i2} \\\\ \\vdots \\\\ X_{ik}\\end{pmatrix}\n\\]\nis a \\((k+1) \\times 1\\) vector of independent variables including the intercept for observation \\(i\\).\nProperties of OLS estimator:\n\nUnbiasedness: \\(E[\\hat{\\beta}^{OLS}] = \\beta\\) (if Gauss-Markov assumptions hold)\nEfficiency: Among all linear unbiased estimators, OLS has the smallest variance\nConsistency: As sample size \\(n \\to \\infty\\), \\(\\hat{\\beta}^{OLS} \\to \\beta\\) in probability\n\n\n2.1 Exact Distribution of OLS Estimator\nUnder the assumption of normally distributed errors, \\(\\boldsymbol{\\varepsilon} \\vert \\boldsymbol{X} \\sim N(\\boldsymbol{0}, \\sigma^2\\boldsymbol{I})\\), we have in matrix form\n\\[\n\\hat{\\boldsymbol{\\beta}} \\sim N(\\boldsymbol{\\beta}, \\sigma^2 (\\boldsymbol{X}'\\boldsymbol{X})^{-1}).\n\\tag{1}\\]\nwhere\n\n\\(\\sigma^2 = \\var (\\varepsilon_i \\mid \\bX)\\) is the variance of the error term.\n\\(\\bX\\) is the matrix of independent variables. \\[\n\\bX = \\begin{pmatrix}\n   X_1' \\\\ X_2' \\\\ \\vdots \\\\ X_n'\n\\end{pmatrix}_{n \\times (k+1)}\n\\]\n\nEquation 1 is called the exact (finite-sample) distribution of the OLS estimator.\n\\(\\var(\\hat\\bbeta) = \\sigma^2 (\\boldsymbol{X}'\\boldsymbol{X})^{-1}\\) is called the variance-covariance matrix of the OLS estimator, often denoted as \\(V_{\\hat\\beta}.\\)\nWe estimate \\(\\sigma^2\\) with \\(\\hat{\\sigma}^2:\\) \\[\n\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^ne_i^2}{n-k-1}.\n\\]\nThe standard errors of \\(\\hat{\\beta}_j\\), \\(j=1,\\ldots,k,\\) are given by the square root of the \\((j,j)\\) element of the variance-covariance matrix, \\(V_{\\hat\\beta}\\).\nThe individual coefficient estimates \\(\\hat{\\beta}_j\\) are normally distributed:\n\\[\n\\hat{\\beta}_j \\sim N(\\beta_j, \\sigma^2 v_{jj})\n\\]",
    "crumbs": [
      "Home",
      "Basics",
      "Theoretical Framework of Linear Regression"
    ]
  },
  {
    "objectID": "04_Theoretical_Framework_LR.html#sampling-distribution-of-ols-estimator",
    "href": "04_Theoretical_Framework_LR.html#sampling-distribution-of-ols-estimator",
    "title": "Theoretical Framework of Linear Regression",
    "section": "3 Sampling Distribution of OLS Estimator",
    "text": "3 Sampling Distribution of OLS Estimator\nBecausethe OLS estimator \\(\\hat\\beta_0, \\hat\\beta_1, \\ldots\\) are computed from a randomly drawn sample of data, the estimators themselves are random variables with a probability distribution.\nThe sampling distribution describes the values they could take over different possible random samples. In small samples, theses sampling distributions are complicated, but in large samples, they are normal because of the Central Limit Theorem (CLT).\n\n\n\n\n\n\nSampling Distribution vs. Exact Distribution\n\n\n\n\nThe distribution depends on the sample data \\((X_i, Y_i).\\)\nBased on Central Limit Theorem (CLT), we can establish the sampling distribution of \\(\\hat\\bbeta\\) when sample size is large, i.e., \\(n\\to\\infty\\).\n\n\n\n\n3.1 Central Limit Theorem (CLT)\n\nTheorem 1 Suppose that \\(X_1, \\ldots, X_n\\) is an independent and identically distributed (iid) sequence with a finite mean \\(\\mathbb{E}(X_i)=\\mu\\) and variance \\(\\text{Var}(X_i)=\\sigma^2\\), where \\(0&lt;\\sigma^2&lt;\\infty\\).\nDefine a sample mean: \\(\\overline{X}=\\frac{1}{n}\\sum_{i=1}^n X_i.\\)\nThe Lindeberg-Lévy CLT states that\n\\[\n\\frac{\\overline{X}-\\mathbb{E}[\\overline{X}]}{\\sqrt{\\text{Var}[\\overline{X}]}}\n= \\frac{\\overline{X}-\\mu}{\\sqrt{\\sigma^2/n}}\n= {\\color{#00CC66} \\sqrt{n} \\cdot \\frac{\\overline{X}-\\mu}{\\sigma} }\n\\xrightarrow{d} N(0,1)\n\\] where \\(\\xrightarrow{d}\\) denotes convergence in distribution.\nAlternative expressions of CLT: \\[\n\\begin{split}\n\\sqrt{n} (\\overline{X}-\\mu)  & \\xrightarrow{d} N(0,\\sigma^2) \\\\\n\\red{\\overline{X}} & \\red{\\xrightarrow{d} N(\\mu, \\frac{\\sigma^2}{n})}\n\\end{split}\n\\]\n\nQ: Why does CLT matter?\nA:\n\nThe central limit theorem implies that if \\(N\\) is large enough, we can approximate the distribution of \\(\\overline{X}\\) by the standard normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2 / N\\) regardless of the underlying distribution of \\(X.\\)\nThe scaling by \\(n\\) is crucial. \\[\n\\sigma_{\\overline{X}}^2 = \\text{Var}[\\overline{X}] = \\frac{\\sigma^2}{n}\n\\] This means as sample size increases, the sample variance shrinks at the rate of \\(1/n\\).\n\n\n\n\n\n\nTip\n\n\n\nInterpretation: As the sample size \\(n\\) increases,the mean becomes more concentrated around the true population mean \\(\\mu\\).\n\n\nHow large must \\(n\\) be for the distribution of \\(\\overline{X}\\) to be approximately normal?\nThere is no definitive answer, but a common rule of thumb is that \\(n \\geq 30\\) is often sufficient for the CLT to provide a good approximation.\nBecause the distribution of \\(\\overline{X}\\) approaches the normal distribution as \\(n\\) increases, \\(\\overline{X}\\) is said to have an asymptotic normal distribution.\n\n\n\n3.2 CLT Example\nThe amount of time customers spend in a grocery store is a random variable with mean \\(\\mu = 40\\) minutes and standard deviation \\(\\sigma = 15\\) minutes. Assume each customer’s time in the store is independent of others.\n\\[\n\\mathbb{E}[X] = \\mu = 40, \\quad \\text{and} \\quad \\text{SD}(X) = \\sigma = 15\n\\]\nConsider the following probabilities:\n\nExample 1 Assuming \\(X\\) is normally distributed, what is the probability that a randomly selected customer spends more than 42 minutes in the store, i.e., compute \\(P(X &gt; 42)\\)?\nUse the normal distribution table to look up the probabilities.\n\n\nSolution\n\n\nIndividual Customer\nWe define the standard normal variable \\(Z\\) as:\n\\[\nZ = \\frac{X - \\mu}{\\sigma} \\sim N(0,1)\n\\]\nwhere \\(\\mu = 40\\) and \\(\\sigma = 15\\).\nTo compute \\(P(X &gt; 42)\\), we convert to the standard normal:\n\\[\nP(X &gt; 42) = P(\\frac{X-40}{15} &gt; \\frac{42-40}{15}) =  P(Z &gt; 0.1333)\n\\]\nBy symmetry of the normal distribution:\n\\[P(Z &gt; 0.1333) = P(Z &lt; -0.1333) = \\Phi(-0.1333) = 0.4483\\]\nwhere \\(\\Phi(\\cdot)\\) denotes the cumulative distribution function (CDF) of the standard normal distribution.\nInterpretation: There is approximately a 44.83% probability that a randomly selected customer will spend more than 42 minutes in the store.\nNote that the specific distribution for \\(X\\), i.e., normality here, is required to compute the probability for a single customer.\n\n\nExample 2 Given a random sample of \\(n=64\\) customers, what is the probability of the average time spent by the 64 customers exceeds 42 minutes, i.e., compute \\(P(\\overline{X}_{64} &gt; 42)\\)?\nHint: apply CLT and justify why CLT can be applied here.\n\n\nSolution\n\n\nSample Mean for \\(n = 64\\)\nBy the Central Limit Theorem, for large \\(n\\) with finite mean and variance, the sampling distribution of the sample mean \\(\\overline{X}_{64}\\) is approximately normal (regardless the distribution of \\(X\\)):\n\\[\n\\overline{X}_{64} \\sim N (\\mu, \\frac{\\sigma^2}{n})\n\\]\nPlug in \\(\\mu = 40\\) and \\(\\sigma = 15\\):\n\\[\n\\sigma_{\\overline{X}_{64}} = \\frac{\\sigma}{\\sqrt{n}} = \\frac{15}{\\sqrt{64}} = 1.875\n\\]\nAccording to CLT \\(\\overline{X}_{64}\\) is normally distributed:\n\\[\n\\overline{X}_{64} \\sim N\\left(40, 1.875^2\\right)\n\\]\nTo calculate \\(P(\\overline{X} &gt; 42)\\), we standardize the sample average to the standard normal variable \\(Z\\), defined as:\n\\[\nZ = \\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} = \\frac{\\overline{X} - 40}{1.875}\n\\]\nWe can rewrite the probability in terms of \\(Z\\):\n\\[\nP(\\overline{X} &gt; 42) = P(\\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} &gt; \\frac{42 - 40}{1.875}) = P(Z &gt; 1.067)\n\\]\nUsing the standard normal distribution table:\n\\[\nP(Z &gt; 1.067) = P(Z &lt; -1.067)\n\\]\nSince the standard normal distribution table is rounded at two decimal places, we look up \\(P(Z &lt; -1.07) = 0.1423\\):\nInterpretation: There is a 14.23% probability that the average time spent by a random sample of 64 customers exceeds 42 minutes.\n\n\nExample 3 Given a random sample of \\(n=81\\) customers, what is the probability of the average time spent by the 81 customers exceeds 42 minutes, i.e., compute \\(P(\\overline{X}_{81} &gt; 42)\\)?\n\n\nSolution\n\n\nSample Mean for \\(n = 81\\)\n\\[\n\\sigma_{\\overline{X}_{81}} = \\frac{\\sigma}{\\sqrt{n}} = \\frac{15}{\\sqrt{81}} = \\frac{15}{9} = 1.667\n\\]\nAccording to CLT \\(\\overline{X}_{81}\\) is normally distributed:\n\\[\n\\overline{X}_{81} \\sim N(\\mu= 40, \\sigma_{\\overline{X}_{81}} = 1.667^2)\n\\]\nStandardize the sample average\n\\[\nP(\\overline{X} &gt; 42) = P(\\frac{\\overline{X}-\\mu}{\\sigma_{\\overline{X}_{81}}} &gt; \\frac{42-40}{1.667})\n\\]\nRewrite the probability in terms of \\(Z\\):\n\\[\nz = \\frac{42 - 40}{1.667} = 1.20\n\\]\n\\[\nP(\\overline{X} &gt; 42) = P(Z &gt; 1.20)\n\\]\nLook up the normal distribution table \\[\nP(Z &gt; 1.20) = P(Z&lt;-1.2) \\approx 0.1151\n\\]\nInterpretation: A random sample of 16 customers has a 11.51% chance of yielding an average time above 42 minutes.\n\n\nExample 4 Discussion: Compare your results in the three probabilities. Provide a brief interpretation of how the probabilities differ and why.\n\n\nSolution\n\n\n\nFor a single customer, there is a 44.83% chance of spending more than 42 minutes.\nFor the average of 64 customers, the probability drops to 14.23%.\nFor the average of 81 customers, it drops further to 11.51%.\nWhy? As sample size increases, the variability (standard deviation) of the sample mean decreases, making it less likely to observe extreme values like an average over 42 minutes. This demonstrates the Central Limit Theorem and the stabilizing effect of larger samples. (See Figure 1 for a graphical illustration.)\n\nThe individual distribution is wider (larger standard deviation), so the chance of extreme values is higher.\nAs sample size increases, the standard error decreases, so the distribution of the sample mean becomes narrower.\nThis means it becomes less likely to observe a sample mean far from the population mean.\n\n\n\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\n\n\n\nKey Takeaways from CLT\n\n\n\nGenerally, as the sample size increases, the sampling distribution of the sample mean becomes more concentrated around the population mean.\nThis reflects a reduction in variability: larger samples yield more precise estimates of the population mean.\nConversely, smaller sample sizes result in a wider sampling distribution, indicating greater variability in the sample mean.\n\n\n\n\n3.3 Large Sample Distribution of OLS Estimator\nNow we can establish the large sample distribution of the OLS estimator \\(\\hat\\bbeta\\) based on CLT.\nUnder the Gauss-Markov assumptions, as \\(n\\to\\infty\\), \\[\n\\sqrt{n} (\\hat\\bbeta - \\bbeta) \\xrightarrow{d} N(0, \\sigma^2 E(\\bX_i\\bX_i')^{-1})\n\\]\n\nAgain, note the scaling by \\(\\sqrt{n}.\\) It shows how fast the estimator \\(\\hat\\bbeta\\) converges to the true parameter \\(\\bbeta\\) as sample size \\(n\\) increases.\nNote the difference of the variance between the exact distribution and the large sample distribution.\n\nThe exact distribution depends on the sample data \\(\\bX'\\bX\\)\nThe large sample distribution depends on the population moment \\(E(\\bX_i\\bX_i')\\).\n\nIt suggests that \\(\\hat\\bbeta\\) is asymptotically normal and consistent.\n“Consistency” means that as the sample size \\(n\\) increases, the estimator \\(\\hat\\bbeta\\) converges in probability to the true parameter \\(\\bbeta\\).",
    "crumbs": [
      "Home",
      "Basics",
      "Theoretical Framework of Linear Regression"
    ]
  },
  {
    "objectID": "04_Theoretical_Framework_LR.html#measure-of-fit",
    "href": "04_Theoretical_Framework_LR.html#measure-of-fit",
    "title": "Theoretical Framework of Linear Regression",
    "section": "4 Measure of Fit",
    "text": "4 Measure of Fit\nWe often use \\(R^2\\) to measure the goodness of fit of a regression model.\n\n\\(R^2\\) is called “the coefficient of determination.”\nIt represents the proportion of variance in the dependent variable that is predictable by the regression model.\n\\(0\\le R^2\\le 1\\)\n\n\\(R^2\\) is defined as:\n\\[\nR^2 = 1 - \\frac{SSR}{SST} = \\frac{SST - SSR}{SST} = \\frac{SSE}{SST}\n\\]\nwhere\n\n\\(SSR = \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2 = \\sum_{i=1}^{n} e_i^2\\) is the sum of squared residuals (unexplained variation)\n\\(SST = \\sum_{i=1}^{n} (Y_i - \\bar{Y})^2\\) is the total sum of squares (total variation)\n\\(SSE = \\sum_{i=1}^{n} (\\hat{Y}_i - \\bar{Y})^2\\) is the explained sum of squares (explained variation)\n\\(\\bar{Y}= \\frac{1}{n}\\sum_{i=1}^n Y_i\\) is the mean of the dependent variable\n\nNote that adding variables always increases \\(R^2\\), even if the new variables are not statistically significant. → To address this, we use the adjusted \\(R^2\\).\nIn a regression model with multiple explanatory variables, we often use adjusted \\(R^2\\) that adjusts the number of explanatory variables.\n“adjusted \\(R^2\\)” or “R-bar-squared”, defined by \\[\n\\overline{R}^2 = 1- \\frac{n-1}{n-k}(1-R^2) ,\n\\] which imposes a penalty as \\(k\\) increases in a given sample size \\(n\\).",
    "crumbs": [
      "Home",
      "Basics",
      "Theoretical Framework of Linear Regression"
    ]
  },
  {
    "objectID": "04_Theoretical_Framework_LR.html#homoskedasticity",
    "href": "04_Theoretical_Framework_LR.html#homoskedasticity",
    "title": "Theoretical Framework of Linear Regression",
    "section": "5 Homoskedasticity",
    "text": "5 Homoskedasticity\nHomoskedasticity assumption requires constant variance of the error term across all levels of the independent variable(s).\n\\[\n\\var(\\varepsilon_i | X) = \\sigma^2\n\\]\nWhen the assumption is violated, we have heteroskedasticity:\n\\[\n\\var(\\varepsilon_i | X) = \\sigma^2_i\n\\]\nthat is, the variance of the error term varies with the level of the independent variable(s).\nIf the error has heteroskedasticity, the standard error under homoskedasticity assumption will be underestimated, leading to invalid t statistics, affecting hypothesis tests and confidence intervals.\n\n5.1 Detection of Heteroskedasticity\n\n\n5.2 Graphical Method\nA common graphical method to detect heteroskedasticity is to plot the residuals against the fitted values or one of the independent variables.\nRecall the CASchools example, we regress TestScore on expenditure.\n\n\n\n\n\n\n\n\n\nWe recap the figure summary below:\n\nThe residuals appear reasonably balanced around zero, though the spread suggests substantial unexplained variation.\nThe spread of residuals seems to increase slightly with fitted values, indicating potential heteroskedasticity (non-constant variance of errors). This suggests that the assumption of homoskedasticity may be violated, which could affect the reliability of our coefficient estimates standard errors.\n\n\n\n5.3 Fix Heteroskedasticity\nA quick fix is to use heteroskedasticity-robust standard errors, which are valid even when the homoskedasticity assumption is violated.\n\nlibrary(tidyverse)\nlibrary(sandwich) # for robust standard errors\nlibrary(lmtest) # for coeftest()\nlibrary(stargazer)\n\nf_name &lt;- \"https://raw.githubusercontent.com/my1396/FIN5005-Fall2025/refs/heads/main/data/CASchools_test_score.csv\"\ncas &lt;- read_csv(f_name,\n    col_types = cols(\n        county = col_factor(), # read as factor\n        grades = col_factor()\n    )\n)\ncas &lt;- cas %&gt;%\n    mutate(TestScore = (read + math) / 2)\nmodel &lt;- lm(TestScore ~ expenditure, data = cas)\n\n\n# OLS standard errors\nse_ols &lt;- sqrt(diag(vcov(model)))\n\n# HC1 heteroskedasticity-robust standard errors\nse_hc1 &lt;- sqrt(diag(vcovHC(model, type = \"HC1\")))\n\n# Stargazer output: same model, different SEs\nstargazer(model, model,\n          se = list(se_ols, se_hc1),\n          digits = 4,\n          column.labels = c(\"OLS SE\", \"Robust SE\"),\n          type = \"text\"\n          )\n\n\n===========================================================\n                                   Dependent variable:     \n                               ----------------------------\n                                        TestScore          \n                                   OLS SE       Robust SE  \n                                    (1)            (2)     \n-----------------------------------------------------------\nexpenditure                      0.0057***      0.0057***  \n                                  (0.0014)      (0.0016)   \n                                                           \nConstant                        623.6165***    623.6165*** \n                                  (7.7197)      (8.4664)   \n                                                           \n-----------------------------------------------------------\nObservations                        420            420     \nR2                                 0.0366        0.0366    \nAdjusted R2                        0.0343        0.0343    \nResidual Std. Error (df = 418)    18.7239        18.7239   \nF Statistic (df = 1; 418)        15.8734***    15.8734***  \n===========================================================\nNote:                           *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nUnder the heteroskedasticity-robust standard errors:\n\nNo effects on the coefficient estimates.\nThe Std. Error is slightly larger (0.001620 vs. 0.001443). The coefficient remains statistically significant at the 5% level.\n\nInterpretation\nThe heteroskedasticity-robust results suggest that the OLS standard errors may have been too optimistic (underestimated variability). While the statistical significance of the expenditure effect is slightly reduced, it remains highly significant, indicating a robust positive relationship between expenditure and test scores.",
    "crumbs": [
      "Home",
      "Basics",
      "Theoretical Framework of Linear Regression"
    ]
  },
  {
    "objectID": "02_Data_Visualization.html",
    "href": "02_Data_Visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "🎯 Study Objectives\n\nLoads and prepares the California school dataset (CASchools) for analysis.\n\nDemonstrates how to visualize data using scatter plots, line charts, histograms, box plots.\nExplores expenditure per student across different grade spans.\nInvestigates correlations between expenditure and other variables.\n\nQQ plots to assess normality and interpret skewness and kurtosis in the data.\n\nGiven a QQ plot, you should be able to identify how does the distribution compare to normal: positively skewed, negatively skewed, has fat tails, or thin tails.",
    "crumbs": [
      "Home",
      "Basics",
      "Data Visualization"
    ]
  },
  {
    "objectID": "02_Data_Visualization.html#descriptive-analysis-and-data-visualization-with-caschools",
    "href": "02_Data_Visualization.html#descriptive-analysis-and-data-visualization-with-caschools",
    "title": "Data Visualization",
    "section": "1 Descriptive Analysis and Data Visualization with CASchools",
    "text": "1 Descriptive Analysis and Data Visualization with CASchools\nData visualization is a powerful tool for understanding patterns, relationships, and distributions in datasets. In this session, we will explore the California Schools dataset using various visualization techniques including histograms, box plots, scatter plots, and Q-Q plots. These visualizations help us answer important questions about educational spending, student performance, and the relationships between various school characteristics.\n\n1.1 Dataset Overview\nThe following prints the first and last 5 rows of the dataset, together with data types of each column:\n\n# load packages and dataset\npkgs &lt;- c(\"tidyverse\", \"moments\", \"data.table\", \"ggsci\")\nmissing &lt;- setdiff(pkgs, rownames(installed.packages()))\nif (length(missing) &gt; 0) install.packages(missing)\ninvisible(lapply(pkgs, function(pkg) suppressPackageStartupMessages(library(pkg, character.only = TRUE))))\nf_name &lt;- \"https://raw.githubusercontent.com/my1396/FIN5005-Fall2025/refs/heads/main/data/CASchools_test_score.csv\"\ncas &lt;- read_csv(f_name, \n  col_types = cols(\n    county = col_factor(), # read as factor\n    grades = col_factor()\n  ))\n\nThe following prints the first and last 5 rows of the dataset, together with data types of each column:\n\ncas %&gt;% as.data.table()\n\n     district                          school      county grades students\n        &lt;num&gt;                          &lt;char&gt;      &lt;fctr&gt; &lt;fctr&gt;    &lt;num&gt;\n  1:    75119              Sunol Glen Unified     Alameda  KK-08      195\n  2:    61499            Manzanita Elementary       Butte  KK-08      240\n  3:    61549     Thermalito Union Elementary       Butte  KK-08     1550\n  4:    61457 Golden Feather Union Elementary       Butte  KK-08      243\n  5:    61523        Palermo Union Elementary       Butte  KK-08     1335\n ---                                                                     \n416:    68957          Las Lomitas Elementary   San Mateo  KK-08      984\n417:    69518            Los Altos Elementary Santa Clara  KK-08     3724\n418:    72611          Somis Union Elementary     Ventura  KK-08      441\n419:    72744               Plumas Elementary        Yuba  KK-08      101\n420:    72751            Wheatland Elementary        Yuba  KK-08     1778\n     teachers calworks   lunch computer expenditure    income   english  read\n        &lt;num&gt;    &lt;num&gt;   &lt;num&gt;    &lt;num&gt;       &lt;num&gt;     &lt;num&gt;     &lt;num&gt; &lt;num&gt;\n  1:    10.90   0.5102  2.0408       67    6384.911 22.690001  0.000000 691.6\n  2:    11.15  15.4167 47.9167      101    5099.381  9.824000  4.583333 660.5\n  3:    82.90  55.0323 76.3226      169    5501.955  8.978000 30.000002 636.3\n  4:    14.00  36.4754 77.0492       85    7101.831  8.978000  0.000000 651.9\n  5:    71.50  33.1086 78.4270      171    5235.988  9.080333 13.857677 641.8\n ---                                                                         \n416:    59.73   0.1016  3.5569      195    7290.339 28.716999  5.995935 700.9\n417:   208.48   1.0741  1.5038      721    5741.463 41.734108  4.726101 704.0\n418:    20.15   3.5635 37.1938       45    4402.832 23.733000 24.263039 648.3\n419:     5.00  11.8812 59.4059       14    4776.336  9.952000  2.970297 667.9\n420:    93.40   6.9235 47.5712      313    5993.393 12.502000  5.005624 660.5\n      math\n     &lt;num&gt;\n  1: 690.0\n  2: 661.9\n  3: 650.9\n  4: 643.5\n  5: 639.9\n ---      \n416: 707.7\n417: 709.5\n418: 641.7\n419: 676.5\n420: 651.0\n\n\nThe cas dataset contains information on 420 public school districts in California, specifically those that operate either kindergarten through 6th grade (K–6) or kindergarten through 8th grade (K–8) schools, with data collected for the years 1998 and 1999.\nThe dataset includes:\n\nStudent performance measures (average reading and math scores).\nSchool characteristics (such as enrollment, number of teachers, student–teacher ratio, number of computers, expenditure per student).\nStudent demographics (including income, percentage qualifying for reduced-price lunch, percentage of English learners, etc.).\n\ncas is a data frame containing 420 observations on 14 variables. Refer to the following table for the full definition.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ndistrict\nCharacter. District code. Unique ID.\n\n\nschool\nCharacter. School name.\n\n\ncounty\nFactor indicating county.\n\n\ngrades\nFactor indicating grade span covered by the district’s schools. It takes on two values: - KK-06: means the district’s schools serve kindergarten through 6th grade.- KK-08: means the district’s schools serve kindergarten through 6th grade.\n\n\nstudents\nTotal enrollment.\n\n\nteachers\nNumber of teachers.\n\n\ncalworks\nPercent qualifying for CalWorks (income assistance program).\n\n\nlunch\nPercent qualifying for reduced-price lunch.\n\n\ncomputer\nNumber of computers.\n\n\nexpenditure\nExpenditure per student.\n\n\nincome\nDistrict average income (in USD 1,000).\n\n\nenglish\nPercent of English learners (i.e., students for whom English is a second language).\n\n\nread\nAverage reading score.\n\n\nmath\nAverage math score.\n\n\n\nData source: Stock, J. H. and Watson, M. W. (2020). Introduction to Econometrics, 4th ed. Pearson.\n\n\n\n1.2 Summary Statistics\nThe following displays the summary for each variable in the dataset:\n\nsummary(cas)\n\n    district        school                  county      grades   \n Min.   :61382   Length:420         Sonoma     : 29   KK-08:359  \n 1st Qu.:64308   Class :character   Kern       : 27   KK-06: 61  \n Median :67760   Mode  :character   Los Angeles: 27              \n Mean   :67473                      Tulare     : 24              \n 3rd Qu.:70419                      San Diego  : 21              \n Max.   :75440                      Santa Clara: 20              \n                                    (Other)    :272              \n    students          teachers          calworks          lunch       \n Min.   :   81.0   Min.   :   4.85   Min.   : 0.000   Min.   :  0.00  \n 1st Qu.:  379.0   1st Qu.:  19.66   1st Qu.: 4.395   1st Qu.: 23.28  \n Median :  950.5   Median :  48.56   Median :10.520   Median : 41.75  \n Mean   : 2628.8   Mean   : 129.07   Mean   :13.246   Mean   : 44.71  \n 3rd Qu.: 3008.0   3rd Qu.: 146.35   3rd Qu.:18.981   3rd Qu.: 66.86  \n Max.   :27176.0   Max.   :1429.00   Max.   :78.994   Max.   :100.00  \n                                                                      \n    computer       expenditure       income          english      \n Min.   :   0.0   Min.   :3926   Min.   : 5.335   Min.   : 0.000  \n 1st Qu.:  46.0   1st Qu.:4906   1st Qu.:10.639   1st Qu.: 1.941  \n Median : 117.5   Median :5215   Median :13.728   Median : 8.778  \n Mean   : 303.4   Mean   :5312   Mean   :15.317   Mean   :15.768  \n 3rd Qu.: 375.2   3rd Qu.:5601   3rd Qu.:17.629   3rd Qu.:22.970  \n Max.   :3324.0   Max.   :7712   Max.   :55.328   Max.   :85.540  \n                                                                  \n      read            math      \n Min.   :604.5   Min.   :605.4  \n 1st Qu.:640.4   1st Qu.:639.4  \n Median :655.8   Median :652.4  \n Mean   :655.0   Mean   :653.3  \n 3rd Qu.:668.7   3rd Qu.:665.8  \n Max.   :704.0   Max.   :709.5  \n                                \n\n\nInterpreting the Summary Statistics\nThis summary provides key insights into our dataset:\n\nContinuous variables (students, teachers, expenditure, income, etc.) show their distribution through quartiles, mean, and range\nCategorical variables (county, grades) show frequency counts\nNotice the range of expenditure per student: from around \\$3,900 to over \\$7,700 - this substantial variation will be a key focus of our analysis\nTest scores (reading and math) also show considerable variation, suggesting potential relationships with school resources\n\n\n\n1.3 Expenditure Variation Analysis\nQ: To what extent does expenditure per student vary?\nUnderstanding the distribution of expenditure per student is crucial for education policy analysis. Let’s examine detailed descriptive statistics:\n\nquick_summary &lt;- function(x, full=FALSE) {\n    # Function to compute basic descriptive statistics\n    if (!full){\n        # Basic summary\n        data.frame(\n          n = length(x),\n          min = min(x),\n          mean = mean(x),\n          median = median(x),\n          max = max(x),\n          sd = sd(x),\n          skewness = moments::skewness(x),\n          kurtosis = moments::kurtosis(x),\n          row.names = NULL\n        )\n    } else {\n        # Full summary with quartiles and IQR\n        data.frame(\n          n = length(x),\n          min = min(x),\n          Q1 = quantile(x, 0.25),\n          mean = mean(x),\n          median = median(x),\n          Q3 = quantile(x, 0.75),\n          max = max(x),\n          IQR = IQR(x),\n          sd = sd(x),\n          skewness = moments::skewness(x),\n          kurtosis = moments::kurtosis(x),\n          row.names = NULL\n        )\n    }\n}\nquick_summary(cas$expenditure) %&gt;% round(2)\n\n    n     min    mean  median     max     sd skewness kurtosis\n1 420 3926.07 5312.41 5214.52 7711.51 633.94     1.07     4.88\n\n\nInterpreting the Results:\n\nRange: Expenditure varies from about \\$3,926 to \\$7,712 per student - a difference of nearly \\$4,000\nCentral Tendency: The mean (\\$5,312) is slightly higher than the median (\\$5,196), suggesting a slight right skew\nVariability: Standard deviation of \\$633 indicates moderate spread around the mean\nSkewness (1.07): Positive skewness confirms some districts spend considerably more than average\nKurtosis (4.88): Larger than 3 indicates a distribution with fat tails relative to the normal distribution.\n\n\n# Basic histogram\nggplot(cas, aes(x = expenditure)) +\n  geom_histogram(binwidth = 500, boundary = 0, fill = \"lightblue\", color = \"black\") +\n  labs(title = \"Histogram of Expenditure per Student\",\n       x = \"Expenditure per Student (USD)\",\n       y = \"Frequency\") +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\nWhat the Histogram Reveals:\n\nMost districts cluster around the \\$5,000-\\$5,500 range\nVariability is moderate\nThe distribution has a long right tail, indicating positive skewness. That suggests that expenditure per student is concentrated at lower values for most districts, while a smaller number of districts spend substantially more, pulling the mean above the median\nThere are few extreme positive outliers, indicating some districts do spend significantly more than average\n\n\n\n\n1.4 Group Comparison: K-6 vs K-8 Schools\nQ: Are there any differences in expenditure per student between K-6 and K-8 schools?\nThis comparison helps us understand whether grade span affects resource allocation. Let’s examine the statistics for each group:\n\ncas %&gt;%\n    group_by(grades) %&gt;%\n    group_map(~ {\n        s &lt;- quick_summary(.x$expenditure, full=TRUE)\n        s &lt;- cbind(\n            grades = .y$grades,    # add group info\n            s)\n        return(s)\n    }) %&gt;%\n    bind_rows()\n\n  grades   n      min       Q1     mean   median       Q3      max      IQR\n1  KK-08 359 3926.070 4881.139 5267.365 5195.919 5539.380 7711.507 658.2412\n2  KK-06  61 4715.446 5092.917 5577.493 5399.383 5990.794 7614.379 897.8770\n        sd skewness kurtosis\n1 618.6415 1.094514 5.284716\n2 662.8033 1.019435 3.309185\n\n\nKey Findings:\n\nK–6 schools (61 districts): Mean expenditure of \\$5,577, slightly higher than K–8 schools\nK–8 schools (359 districts): Mean expenditure of \\$5,267, with a larger sample size\nK–6 schools indicating higher variability, with a standard deviation of \\$662 compared to \\$618 for K-8 schools\nSkewness is positive for both groups, with K–8 schools showing slightly more skewness\nThe difference suggests that smaller grade span districts may have slightly higher per-pupil spending\n\n\nBox plot by grades span\n\nggplot(cas, aes(x = grades, y = expenditure, fill = grades)) +\n  geom_boxplot() +\n  scale_fill_npg() +\n  labs(x = \"Grade Span\",\n       y = \"Expenditure per Student (USD)\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nUnderstanding the Box Plot:\n1. Box and middle bar (median)\n\nThe box itself spans from the first quartile (Q1, 25th percentile) to the third quartile (Q3, 75th percentile).\nThis range is called the interquartile range (IQR = Q3 - Q1).\nThe middle bar inside the box represents the median (50th percentile).\n\nIf the median is closer to the bottom of the box, the lower half of the data is more concentrated.\nIf closer to the top, the upper half is more concentrated.\n\n\n2. Whiskers\n\nThe whiskers extend from the box to show the range of the data excluding outliers.\nA whisker extends to the largest/smallest value within 1.5 × IQR from the box:\n\nUpper whisker = largest value ≤ Q3 + 1.5 × IQR\nLower whisker = smallest value ≥ Q1 - 1.5 × IQR\n\n\n3. Outliers\n\nPoints beyond the whiskers are plotted individually as outliers.\nThese are values that are unusually high or low relative to the bulk of the data.\n\n\n\n\n\n\n\nSummary of boxplot\n\nBox: middle 50% of the data\nMiddle line: median\nWhiskers: typical range, excluding extreme values\nOutliers: extreme observations\n\n\n\n\n\nInterpreting the Box Plot:\n\nMedian lines: K–6 districts show a higher median expenditure than K-8 districts\nBox sizes: K–6 district show a larger interquartile ranges (IQR), indicating greater middle 50% spread\nOutliers: K–8 groups show more positive outliers (points beyond the whiskers), representing districts with unusually high expenditure\n\n\n\n\n\n1.5 Correlation Analysis\nQ: What predicts expenditure per student?\nUnderstanding which factors correlate with expenditure helps identify patterns in educational resource allocation. Let’s examine correlations:\nCorrelation coefficients in ascending order:\n\n# Get the set of numeric variables\nv &lt;- setdiff(\n    names(cas),\n    c(\"district\", \"school\", \"county\", \"grades\")\n)\ncorExp &lt;- cor(cas[\"expenditure\"], cas[setdiff(v, \"expenditure\")])\nt(corExp) %&gt;%\n    as.data.frame() %&gt;%\n    rownames_to_column(var = \"variable\") %&gt;%\n    rename(correlation = expenditure) %&gt;% \n    arrange(correlation)\n\n  variable correlation\n1 students -0.11228455\n2 teachers -0.09519483\n3  english -0.07139604\n4 computer -0.07131050\n5    lunch -0.06103871\n6 calworks  0.06788857\n7     math  0.15498949\n8     read  0.21792682\n9   income  0.31448448\n\n\nKey Correlation Insights:\n\nNo strong correlations (e.g., \\(&gt;|0.7|\\)) with expenditure\nPositive correlations: Income (0.31), math scores (0.22), and reading scores (0.15)\nModerate Negative correlations: Students(\\(-0.11\\)), indicating districts with a larger amount of students tend to spend less per student\nInterpretation:\n\nWealthier districts tend to spend more per student\nHigher spending correlates with better test scores\nDistricts with more students may face budget constraints leading to lower per-student expenditure\n\n\nPlot scatter plots for the top three correlated variables\n\ntop_vars &lt;- c(\"income\", \"math\", \"read\")\ncas_long &lt;- cas %&gt;%\n    select(expenditure, all_of(top_vars)) %&gt;%\n    pivot_longer(cols = all_of(top_vars), names_to = \"variable\", values_to = \"value\")\nggplot(cas_long, aes(x = value, y = expenditure)) +\n    geom_point(alpha = 0.6, color = \"#1976d2\") +\n    facet_wrap(~variable, scales = \"free_x\") +\n    labs(\n        title = \"Scatter Plots of Expenditure per Student vs. Top Correlated Variables\",\n        x = \"Value\",\n        y = \"Expenditure per Student (USD)\"\n    ) +\n    theme_minimal(base_size = 14)   \n\n\n\n\n\n\n\n\nWhat These Scatter Plots Reveal:\n\nIncome vs. Expenditure: Positive relationship - wealthier districts consistently spend more per student; the trend is relatively clear with some variability;\nMath Scores vs. Expenditure: Positive trend suggests higher spending correlates with better math performance; the points are more scattered, indicating other factors also play a role;\nReading Scores vs. Expenditure: Similar pattern to math scores, showing the relationship between resources and achievement\nData Distribution: Points are fairly scattered, indicating that while correlations exist, other factors also influence these relationships\n\n\n\n\n1.6 Academic Performance Relationship\nQ: what is the relationship between district level maths and reading scores?\nAcademic performance across different subjects often correlates, as they reflect overall educational quality and student preparation. Let’s examine this relationship:\n\nwith(cas, cor.test(math, read))\n\n\n    Pearson's product-moment correlation\n\ndata:  math and read\nt = 49.005, df = 418, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9073417 0.9359362\nsample estimates:\n      cor \n0.9229015 \n\n\n\nggplot(cas, aes(x = read, y = math)) +\n    geom_point(alpha = 0.6, color = \"#1976d2\") +\n    geom_smooth(method = \"lm\", color = \"#d32f2f\") +\n    labs(\n        title = \"Scatter Plot of Math vs. Reading Scores\",\n        x = \"Average Reading Score\",\n        y = \"Average Math Score\"\n    ) +\n    theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\nAnalysis of the Relationship:\n\nStrong positive correlation: Districts with higher reading scores typically have higher math scores\nLinear trend: The smooth line shows a clear linear relationship between the two subjects\nTight clustering: Most points cluster closely around the trend line, indicating a strong relationship\nEducational insights:\n\nThis suggests that factors affecting academic performance impact multiple subjects similarly\nDistricts that excel in one area tend to excel in others",
    "crumbs": [
      "Home",
      "Basics",
      "Data Visualization"
    ]
  },
  {
    "objectID": "02_Data_Visualization.html#q-q-plot",
    "href": "02_Data_Visualization.html#q-q-plot",
    "title": "Data Visualization",
    "section": "2 Q-Q Plot",
    "text": "2 Q-Q Plot\nThe Q-Q plot, or quantile-quantile plot, is a graphical tool to help us assess if a set of data plausibly came from some theoretical distribution such as a normal distribution.\nUnderstanding Q-Q Plots:\nA QQ plot is a scatter plot created by plotting two sets of quantiles against one another.\n\nIf both sets of quantiles came from the same distribution, we should see the points forming a line that’s roughly straight.\nFor example, if we run a statistical analysis that assumes our residuals are normally distributed, we can use a normal QQ plot to check that assumption.\nWhen the theoretical quantiles are from a normal distribution, the plot is called a normal QQ plot.\n\nWhy Q-Q Plots Matter:\n\nMany statistical tests assume normality; deviations from normality can affect the validity of statistical inferences\nQ-Q plots provide a visual method to assess these assumptions\nThey help identify the type of non-normality (skewness, heavy tails, etc.)\n\n\n2.1 QQ plot visualization tool\nhttps://xiongge.shinyapps.io/QQplots/\n\nAdjust Skewness and Kurtosis to see how the QQ plot changes.\nNote the website is not able to host many users at the same time, so if it does not load, please try again later.\n\n\n\n\n2.2 Skewness\n\nPositive skew\nPositively skewed (mean &gt; median) data have a J-shaped pattern in the Q-Q plot.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary statistics:\n\n\n      n   min mean median  max sd skewness kurtosis\n1 10000 -1.89    0   -0.2 5.34  1     0.89     3.63\n\n\n💡 Note that for a positive skewed distribution, the mean is larger than the median because the long right tail pulls the mean above the median.\nUnderstanding Positive Skewness:\n\nShape: Long tail extending to the right\nMean vs. Median: Mean &gt; Median (tail pulls the mean upward)\nQ-Q Plot Pattern: J-shaped curve, points above the line at both ends\nReal-world examples: Income distribution, housing prices\nStatistical implications: May indicate floor effects or bounded distributions\n\n\n\nNegative skew\nNegatively skewed (mean &lt; median) data have Q-Q plots that display an inverted J-shape.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary statistics:\n\n\n      n   min  mean median  max   sd skewness kurtosis\n1 10000 -5.16 -0.03   0.16 1.98 1.02    -0.85     3.47\n\n\n💡 Note that for a negatively skewed distribution, the mean is smaller than the median because the long left tail pulls the mean above the median.\nUnderstanding Negative Skewness:\n\nShape: Long tail extending to the left\nMean vs. Median: Mean &lt; Median (tail pulls the mean downward)\nQ-Q Plot Pattern: Inverted J-shaped curve, points below the line at both ends\nReal-world examples:\n\nTest scores (when most students perform well): Most students score high (80-100), with fewer low scores creating a left tail\nAge at retirement: Most people retire around 65-67, but some retire much earlier due to health or financial reasons\n\nStatistical implications: May indicate ceiling effects or bounded distributions\n\n\n\n\n2.3 Kurtosis\nKurtosis measures the “tailedness” of a distribution compared to a normal distribution. Understanding kurtosis helps identify distributions with unusually many or few extreme values.\n\nFat tails\nThis plot shows a t-distribution with \\(3\\) degrees of freedom, which has heavier tails than a normal distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary statistics:\n\n\n     n   min  mean median  max   sd skewness kurtosis\n1 1000 -6.17 -0.02  -0.01 8.61 1.44     0.13     6.33\n\n\nUnderstanding Fat Tails (High Kurtosis):\n\nQ-Q Plot Pattern: Reverse S-shaped curve - points below the line at low quantiles, above at high quantiles.\nKurtosis &gt; 3: Indicates heavier tails than normal distribution\nPractical meaning: More extreme values (both high and low) occur than expected under normality\nReal-world examples:\n\nFinancial returns: Stock market crashes and booms occur more frequently than normal distribution predicts\nMeasurement errors: Occasionally very large errors occur due to equipment malfunction or human mistakes\nResponse times: Most responses are quick, but some take much longer due to system overload or user distraction\n\nStatistical implications: Higher risk of extreme events; standard confidence intervals may be too narrow\n\n\n\nThin tails\nThis example shows a distribution with lighter tails than normal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary statistics:\n\n\n     n   min  mean median  max   sd skewness kurtosis\n1 1000 -1.17 -0.01      0 1.09 0.58    -0.02      1.8\n\n\nUnderstanding Thin Tails (Low Kurtosis):\n\nQ-Q Plot Pattern: S-shaped curve - points above the line at low quantiles, below at high quantilees.\nKurtosis &lt; 3: Indicates lighter tails than normal distribution\nPractical meaning: Fewer extreme values occur than expected under normality\nReal-world examples: Bounded measurements, standardized test scores, some quality control data\nStatistical implications: Lower risk of extreme events; data more predictable than normal assumption suggests\n\n\nSummary of Distribution Shapes\nUnderstanding these patterns helps in:\n\nChoosing appropriate statistical methods\nIdentifying data quality issues\nMaking informed assumptions about uncertainty\nSelecting proper transformations when needed\nInterpreting results correctly in context",
    "crumbs": [
      "Home",
      "Basics",
      "Data Visualization"
    ]
  },
  {
    "objectID": "00_environment_setup.html",
    "href": "00_environment_setup.html",
    "title": "Seting Up Your Environment",
    "section": "",
    "text": "This guide will help you set up your coding environment for FIN5005, with both cloud-based and local options available (you may choose either).\n\nYou will learn how to use RStudio Cloud for quick, browser-based access and RStudio Desktop for the full set of features on your computer.\nIt is strongly recommended to install the local version (RStudio Desktop) on your computer.\n\nFollow the instructions below to get started.\nEstimated reading time: 6 minutes",
    "crumbs": [
      "Home",
      "Environment Setup"
    ]
  },
  {
    "objectID": "00_environment_setup.html#cloud-solution",
    "href": "00_environment_setup.html#cloud-solution",
    "title": "Seting Up Your Environment",
    "section": "1 Cloud Solution",
    "text": "1 Cloud Solution\nCloud solution means that you don’t need to install anything on your computer. You can run R code directly in your web browser. This is the easiest way to get started without worrying about installation issues.\nBut soon enough, you will realize that the cloud-based solution is limited in:\n\ncompute power: 1 CPU core, 1 GB RAM\nexecution hours: 25 hours per month;\nslow performance\nAI integration is better in local environment\nGitHub Copilot is a powerful AI tool that offers autocomplete-style suggestions as you code. The tool can give you suggestions based on the code you want to use or by simply inquiring about what you want the code to do.\nIt is developed by GitHub in partnership with OpenAI, and it is designated to best assist developers in writing code more efficiently.\nSome of its highlight features include:\n\nCode autocompletion: generating suggestions while typing the code.\nCode generation: Copilot will use the context of the active document to generate suggestions for code that might be useful.\nAnswering questions: it can also be used to ask simple questions while you are coding (e.g., “What is the definition of mean?”).\nLanguage support: supports multiple programming languages, including R, Python, SQL, HTML, and JavaScript.\n\nTo enable Copilot in RStudio: click on Tools → Global Options → Copilot → tick the box saying “Enable GitHub Copilot” → sign in to your GitHub account, and there you go; you are ready to start!\nGiHub Copilot free plan has limited usage: 2000 code autocompletions per month, 30 code generations per month, and 30 question answers per month.\nSoon enough, you will realize that you need more than that. Because every character you type counts as usage, regless of whether you accept the suggestion or not.\n🎈 The good news is: You can use Copilot Pro for free if you are a student!\nSee Apply to GitHub Education as a student.\nIf you wonder anything, look up in GitHub Copilot Documentation.\n\n\nNonetheless, here are two popular cloud solutions:\n\nGoogle Colab: good support for Jupyter Notebooks .ipynb.\nWe will use Jupyter Notebooks in this course.\nRStudio Cloud: basic emulator for RStudio IDE in the cloud.\n\n\n1.1 RStudio Cloud\nOnce in the dashboard, you can click on the New Project - New RStudio Project to get started:\n\nOnce the new project is created, you will see the RStudio IDE interface, which is similar to the one you would find in RStudio Desktop. You can write and execute R code, create scripts, and manage your projects directly in the cloud.",
    "crumbs": [
      "Home",
      "Environment Setup"
    ]
  },
  {
    "objectID": "00_environment_setup.html#local-solution",
    "href": "00_environment_setup.html#local-solution",
    "title": "Seting Up Your Environment",
    "section": "2 Local Solution",
    "text": "2 Local Solution\n\n2.1 RStudio Desktop\nRStudio Desktop is a free and open-source integrated development environment (IDE) for R. It provides a user-friendly interface for writing and executing R code, making it easier to work with R scripts, data analysis, visualization.\nTo install RStudio Desktop, follow these steps:\n\nInstall R\nInstall RStudio Desktop\n\nGo to RStudio Desktop Download and download the installer for your operating system (Windows, macOS, or Linux).\n\nref:\n\nRStudio Cloud - How to Get Started For Free",
    "crumbs": [
      "Home",
      "Environment Setup"
    ]
  },
  {
    "objectID": "01_introduction.html",
    "href": "01_introduction.html",
    "title": "Probability and Data in Business Analytics",
    "section": "",
    "text": "🎯 Study Objectives\n\nApply probability rules to real business events.\nDistinguish clearly between population parameters and sample estimators.\nCompute and explain expectation, variance, and standard deviation in business contexts.\nInterpret higher moments: skewness and kurtosis for tail and asymmetry assessment.\nCombine random variables (e.g., portfolio or multi‑channel forecast) and decompose variance with covariance terms.\nCalculate and interpret covariance and correlation; relate independence vs. zero correlation.\nProbability is the foundation for making data-driven decisions in business. This section covers the essential concepts, formulas, and business examples you’ll use in analytics.",
    "crumbs": [
      "Home",
      "Basics",
      "Probability and Data in Business Analytics"
    ]
  },
  {
    "objectID": "01_introduction.html#probability-axioms",
    "href": "01_introduction.html#probability-axioms",
    "title": "Probability and Data in Business Analytics",
    "section": "1 Probability Axioms",
    "text": "1 Probability Axioms\nProbability measures how likely an event is.\n\\[P(A) = \\frac{\\text{Number of ways A can occur}}{\\text{Total possible outcomes}}\\]\n\n\\(P(A) \\geq 0\\) for any event \\(A\\)\n\\(P(\\text{all possible outcomes}) = 1\\)\n\\(P(A \\text{ or } B) = P(A) + P(B) - P(A\\text{ and }B)\\)\nIf \\(A\\) and \\(B\\) are mutually exclusive, then \\[\nP(A \\text{ and } B) = 0\n\\] and \\[\nP(A \\text{ or } B) = P(A) + P(B)\n\\] Mutually exclusive means “cannot happen together.”\n\n\nExample 1 A retailer estimates a 30% chance of a supply chain disruption and a 50% chance of a demand spike. If these events are mutually exclusive, what is the probability of either event occurring?\n\n\nSolution\n\n\nSolution 1. Since the events are mutually exclusive, \\(P(A \\text{ and } B)=0,\\) then \\[\n\\begin{split}\nP(A \\text{ and } B)=P(A)+P(B)-P(A \\text{ and } B)=0.30+0.50=0.80.\n\\end{split}\n\\]\n\n\nExample 2 In a multi-channel marketing campaign, 35% of customers click an email offer (Event A) and 25% engage with a social media ad (Event B). Data shows 10% of customers do both. What is the probability a randomly selected customer engages with at least one of the two channels (email OR social)?\n\n\nSolution\n\n\nSolution 2. \n\\(P(A) + P(B) - P(A \\text{ and } B) = 0.35 + 0.25 - 0.10 = 0.50\\)",
    "crumbs": [
      "Home",
      "Basics",
      "Probability and Data in Business Analytics"
    ]
  },
  {
    "objectID": "01_introduction.html#sample-vs.-population-estimating-the-big-picture",
    "href": "01_introduction.html#sample-vs.-population-estimating-the-big-picture",
    "title": "Probability and Data in Business Analytics",
    "section": "2 Sample vs. Population: Estimating the Big Picture",
    "text": "2 Sample vs. Population: Estimating the Big Picture\nIn business, we use samples to estimate population characteristics.\nSample mean:\n\\[\n\\overline{X} = \\frac{1}{n} \\sum_{i=1}^n X_i\n\\]\n\nExample 3 A telecom company surveys a randomly selected sample of 500 customers about service quality. The sample mean satisfaction score is 4.2. How can this estimate help guide company-wide improvements?\n\n\nSolution\n\n\nSolution 3. \n\nThe sample mean (4.2) is based on 500 surveyed customers (the sample).\nThe population is all customers of the telecom company.\nTo guide company-wide improvements, we need to know if the sample is representative of the population.\nThere is uncertainty in the estimate; larger samples make the sample mean closer to the true population mean.\nThe sample mean is useful, but its reliability depends on sample representativeness and size.\n\n\n\nExample 4 Why is it important to distinguish between sample and population in business analytics?\n\n\nSolution\n\n\nSolution 4. \nBecause we use samples to estimate population parameters, and understanding the difference helps us make better decisions and avoid bias.",
    "crumbs": [
      "Home",
      "Basics",
      "Probability and Data in Business Analytics"
    ]
  },
  {
    "objectID": "01_introduction.html#moments",
    "href": "01_introduction.html#moments",
    "title": "Probability and Data in Business Analytics",
    "section": "3 Moments",
    "text": "3 Moments\n\n3.1 Expectation, Variance, and Standard Deviation\n\nExpectation (population): Average level of the outcome (true but unknown in practice). \\[\n\\mathbb{E}[X] =\n\\begin{cases}\n\\displaystyle \\sum_{i} x_i \\, P(X = x_i) & \\text{(discrete)} \\\\\n\\displaystyle \\int_{-\\infty}^{\\infty} x \\, f(x) \\, dx & \\text{(continuous)} \\\\\n\\end{cases}\n\\] Sample estimator: Replace the distribution by observed data: \\[\\bar{X} = \\dfrac{1}{n}\\sum_{i=1}^n X_i.\\] The sample mean \\(\\bar{X}\\) estimates the population mean \\(\\mathbb{E}[X]\\).\nVariance (population): How much values fluctuate around the true mean.\n\\[\\text{Var}(X) = E[(X - E[X])^2]\\]\nThe variance can also be expressed as:\n\\[\n\\begin{aligned}\n\\color{#00CC66}{\\text{Var}(X)} &= \\mathbb{E}\\left[\\left(X - \\mathbb{E}[X]\\right)^2\\right] \\\\\n&= \\color{#00CC66}{\\mathbb{E}[X^2] - (\\mathbb{E}[X])^2}.\n\\end{aligned}\n\\]\nThis second form is often faster to compute when you already have (or can easily get) \\(\\mathbb{E}[X^2]\\) (e.g., from grouped data or a probability model). In finance, we frequently estimate variance from simulated or historical returns by computing the average of squared returns (giving \\(\\mathbb{E}[X^2]\\)) and subtracting the square of the average return.\nSample estimators:\n\nBiased (population style) estimator: \\[ \\hat{\\sigma}^2 = \\dfrac{1}{n}\\sum_{i=1}^n (X_i - \\bar{X})^2 \\]\nUnbiased estimator: \\[s^2 = \\dfrac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X})^2 \\] The latter (\\(s^2\\)) subtracts 1 from \\(n\\) in the denominator, which is known as a degrees of freedom correction. This version has some desirable properties but we will not discuss these for now. Suffice to say that both versions are usually fine.\n\n\n\nExample 5 An operations planner models next-hour demand arrivals for a micro-warehouse (number of urgent orders) as \\(X \\in \\{0,1,2\\}\\) with probabilities 0.2, 0.5, 0.3. Compute the variability (variance) of order arrivals using the shortcut formula \\(\\mathbb{E}[X^2] - (\\mathbb{E}[X])^2\\) to inform staffing.\n\n\nSolution\n\n\nSolution 5. \\[\n\\begin{split}\n\\mathbb{E}[X] &= 0(0.2)+1(0.5)+2(0.3)=1.1 \\\\\n\\mathbb{E}[X^2] &= 0^2(0.2)+1^2(0.5)+2^2(0.3)=0+0.5+1.2=1.7 \\\\\n\\text{Var}(X) &= \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 = 1.7 - 1.1^2 = 1.7 - 1.21 = 0.49\n\\end{split}\n\\]\n\n\nExample 6 A SaaS company tracks daily change in Daily Active Users (DAU) (in %) over 5 days: \\(0.4\\), \\(0.6\\), \\(-0.2\\), \\(0.5\\), \\(0.7\\). Verify that the direct variance definition and the shortcut formula match; interpret what the variance implies for short‑term engagement volatility.\n\n\nSolution\n\n\nSolution 6. Daily % changes: \\(0.4,\\ 0.6,\\ -0.2,\\ 0.5,\\ 0.7\\). Let units be percentage points (pp). \\[\n\\mathbb{E}[X] = \\frac{0.4+0.6-0.2+0.5+0.7}{5} = 0.4 \\text{ pp}\n\\] \\[\n\\mathbb{E}[X^2] = \\frac{0.16+0.36+0.04+0.25+0.49}{5} = \\frac{1.30}{5} = 0.26 \\text{ (pp)}^2\n\\] Variance (percentage-point squared): \\[\n\\text{Var}(X) = 0.26 - (0.4)^2 = 0.26 - 0.16 = 0.10 \\text{ (pp)}^2\n\\] Standard deviation \\(= \\sqrt{0.10} \\approx 0.316\\) pp (about \\(0.32\\) percentage points). Direct (long) method matches (0.10).\nInterpretation: Moderate short-term volatility compared to the mean change of 0.4 pp. This suggests some fluctuations in user engagement, but not extreme.\n\n\nStandard Deviation (population): Square root of variance, showing average deviation from the true mean.\n\\[\\text{sd}(X) = \\sqrt{\\text{Var}(X)}\\]\nSample estimator: \\(s = \\sqrt{s^2}\\) (using the unbiased \\(s^2\\) above).\n\n\nExample 7 A hedge fund analyzes daily returns of two portfolios. Portfolio A has higher mean but also higher variance. How should risk-adjusted performance be compared?\n\n\nSolution\n\n\nSolution 7. \nCompare risk-adjusted metrics (e.g., Sharpe ratio = (mean - rf)/sd). Higher mean with disproportionate variance may yield lower Sharpe.\n\n\nExample 8 A business has daily profits of $200, $250, $180, $220, and $210. Calculate the mean and variance.\n\n\nSolution\n\n\nSolution 8. Mean: \\(\\bar{x}=\\dfrac{200+250+180+220+210}{5}=\\dfrac{1060}{5}=212\\).\nVariance calculations:\n\nSum of squared deviations\n\\[\n\\begin{aligned}\n(200-212)^2 &+ (250-212)^2 + (180-212)^2 + (220-212)^2 + (210-212)^2 \\\\\n&= 144 + 1444 + 1024 + 64 + 4 = 2680\n\\end{aligned}\n\\]\nPopulation variance (divide by \\(n=5\\))\n\\[ \\sigma^2 = \\frac{2680}{5} = 536 \\]\nSample variance (unbiased, divide by \\(n-1=4\\))\n\\[ s^2 = \\frac{2680}{4} = 670 \\]\n\nPopulation variance uses \\(n\\) when treating these 5 observations as the entire population; sample variance uses \\(n-1\\) to unbiasedly estimate \\(\\text{Var}(X)\\) of a larger process.\n\n\n\n3.2 Sums of Random Variables: Expectation & Variance\nPopulation linearity of expectation: \\[E[X + Y] = E[X] + E[Y]\\]\nSample counterpart: \\(\\overline{X+Y} = \\bar{X} + \\bar{Y}\\) (sample means add componentwise).\nIf \\(X\\) and \\(Y\\) are independent, the population variance of their sum is the sum of their variances: \\[\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\]\nIf not independent, add twice the covariance: \\[\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y) + 2\\text{Cov}(X, Y)\\]\nSample counterpart (unbiased style): \\[s_{X+Y}^2 = s_X^2 + s_Y^2 + 2 s_{XY},\\] where \\(s_{XY} = \\dfrac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar{X})(Y_i-\\bar{Y}).\\)\nIntuitive Example: In portfolio management, the total risk (variance) of a portfolio depends on the variances of individual assets and how they move together (covariance).\n\nExample 9 Two independent projects have profit variances of 100 and 150. What is the variance of total profit?\n\n\nSolution\n\n\nSolution 9. \n\\(100 + 150 = 250\\).\n\n\nVariance of Linear Transformations & Portfolios\nThe variance operator is the expectation of a quadratic function, so it is not linear. Instead, for constants \\(a, b\\):\n\\[\n\\text{Var}(a + bX) = b^2\\,\\text{Var}(X).\n\\]\nMore generally, for constants \\(a_i \\in \\mathbb{R}, i=1,\\ldots,n\\):\n\\[\n\\text{Var}\\left(\\sum_{i=1}^n a_i X_i \\right) = \\sum_{i=1}^n a_i^2 \\, \\text{Var}(X_i) + \\sum_{i=1}^n \\sum_{j=1}^n a_i a_j \\, \\text{Cov}(X_i, X_j).\n\\]\nIf the \\(X_i\\) are pairwise independent (or uncorrelated), the double sum of covariance terms drops out for \\(i\\neq j\\), leaving only the first sum.\n\nExample 10 A portfolio allocates 40% to Asset A and 60% to Asset B. \\(\\text{Var}(A)=0.04\\), \\(\\text{Var}(B)=0.09\\), and \\(\\text{Cov}(A,B)=0.01\\). Compute portfolio variance using the general formula.\n\n\nSolution\n\n\nSolution 10. \n\\(\\text{Var}(P)=0.4^2(0.04)+0.6^2(0.09)+2(0.4)(0.6)(0.01)=0.0064+0.0324+0.0048=0.0436\\).\n\n\nExample 11 A production planner combines forecasts: Final forecast \\(F = 2 + 0.7X_1 + 0.3X_2\\). If \\(\\text{Var}(X_1)=25\\), \\(\\text{Var}(X_2)=9\\), and \\(\\text{Cov}(X_1,X_2)=6\\), find \\(\\text{Var}(F)\\).\n\n\nSolution\n\n\nSolution 11. \nConstant 2 adds no variance. \\(\\text{Var}(F)=0.7^2(25)+0.3^2(9)+2(0.7)(0.3)(6)=12.25+0.81+2.52=15.58\\).\n\n\n\n\n3.3 Skewness\nSkewness measures asymmetry in a distribution.\n\nSkewness = 0: Symmetric (e.g., normal)\nSkewness &lt; 0: Longer left tail (large losses more likely than large gains)\nSkewness &gt; 0: Longer right tail (occasional big gains)\n\n\n\n\n\n\n\n\n\nFigure 1: Diagram of Skewness.\n\n\n\n\n\nBusiness reading: Positive skew in quarterly sales means you usually hit average numbers but sometimes land a very big contract. Negative skew in operational losses could mean rare but severe downside events that need contingency planning.\nLet \\(\\mu_3 = E[(X-\\mathbb{E}[X])^3]\\) denote the third central moment. Standardized (population) skewness: \\[\\gamma_1 = \\frac{\\mu_3}{\\text{sd}(X)^3}.\\]\nSample (bias‑adjusted) skewness estimator: \\[\\hat{\\gamma}_1 = \\frac{n}{(n-1)(n-2)} \\sum_{i=1}^n \\left(\\frac{X_i-\\bar{X}}{s}\\right)^3.\\]\n\n\n\n\n\n\nTip\n\n\n\nNote for skewness and kurtosis, it is NOT expected to hand calculate the values, the focus is on\n\nthe interpretation of given results.\nbe able to recognize skewness and kurtosis in data visualizations (e.g., histograms, box plots).\n\n\n\n\n\n3.4 Kurtosis\nKurtosis measures tail weight (propensity for outliers) relative to a normal distribution.\n\nKurtosis ≈ 3: Normal tail thickness\nKurtosis &gt; 3 (leptokurtic): Heavy / fat tails, more extreme outcomes\nKurtosis &lt; 3 (platykurtic): Light / thin tails, fewer extremes\n\n\n\n\n\n\n\n\n\nFigure 2: Examples of heavy-tailed distributions.\n\n\n\n\n\n\\(t\\)-distribution has higher kurtosis than normal distributions.\n\nMeaning that \\(t\\)-distribution has a higher probability of obtaining values that are far from the mean than a normal distribution.\nIt is less peaked in the center and higher in the tails than normal distribution.\nAs the degree of freedom increases, \\(t\\)-distribution approximates to normal distribution, kurtosis decreases and approximates to 3.\n\n\n\n\n\n\n\n\n\nFigure 3: The comparison between the t-distribution and the normal distribution at degrees of freedom ranging from 1 to 50. Figure source: from T.J. Kyner.\n\n\n\n\n\n\n\n\n\n\n\nFor the \\(t\\)-distribution:\n\n\n\n\nAs the degrees of freedom (df) increase, the \\(t\\)-distribution approaches the normal distribution.\nA common rule of thumb is that for \\(df &gt; 30\\), one can pretty safely use the normal distribution in place of a t-distribution unless you are interested in the extreme tails.\n\n\n\nLet \\(\\mu_4 = E[(X-\\mathbb{E}[X])^4]\\) be the fourth central moment. Standardized (population) kurtosis: \\[\\kappa = \\frac{\\mu_4}{\\text{sd}(X)^4}, \\qquad \\text{Excess kurtosis} = \\kappa - 3.\\] Sample (Fisher) excess kurtosis estimator: \\[\\hat{g}_2 = \\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\sum_{i=1}^n \\left(\\frac{X_i-\\bar{X}}{s}\\right)^4 - \\frac{3(n-1)^2}{(n-2)(n-3)}.\\] High excess kurtosis signals greater tail risk; low or negative excess indicates fewer extreme outcomes.\nInterpretation link: High positive skewness without high kurtosis means upside spikes with limited extra tail risk; high kurtosis (regardless of skew sign) means fatter tails and a need to focus on outlier management.\n\nExample 12 A company’s quarterly profits have excess kurtosis of 4. What does this mean for risk management?\n\n\nSolution\n\n\nSolution 12. Excess kurtosis is defined relative to a normal distribution (which has kurtosis 3).\n\nExcess kurtosis = Kurtosis − 3\nSo an excess kurtosis of 4 means the total kurtosis is 7, much higher than a normal distribution.\n\nThis implies the distribution of quarterly profits has fat tails — extreme profit or loss outcomes are more likely than under a normal distribution. Recommend to focus risk management on outliers.",
    "crumbs": [
      "Home",
      "Basics",
      "Probability and Data in Business Analytics"
    ]
  },
  {
    "objectID": "01_introduction.html#covariance-correlation-independence",
    "href": "01_introduction.html#covariance-correlation-independence",
    "title": "Probability and Data in Business Analytics",
    "section": "4 Covariance, Correlation & Independence",
    "text": "4 Covariance, Correlation & Independence\nPopulation covariance measures the direction of comovement between two variables:\n\nPositive covariance: Both variables increase or decrease together\nNegative covariance: One increases while the other decreases\nMagnitude: Does not indicate strength of relationship\n\nPopulation formulas: \\[\\text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])]\\] \\[\\text{Cov}(X, Y) = E[XY] - E[X]E[Y]\\]\nSample covariance (unbiased): \\[s_{XY} = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y}).\\]\nPopulation correlation standardizes covariance to a scale from -1 to 1: \\[\\text{Corr}(X, Y) = \\frac{\\text{Cov}(X, Y)}{\\text{sd}(X) \\cdot \\text{sd}(Y)}\\]\nSample correlation: \\[r_{XY} = \\frac{s_{XY}}{s_X s_Y}.\\]\n\nCorrelation = 1: Perfect positive relationship\nCorrelation = \\(-1\\): Perfect negative relationship\nCorrelation = 0: No linear relationship\n\nIndependence means knowing one event doesn’t affect the other: \\[P(A \\text{ and } B) = P(A) \\cdot P(B)\\]\n\n\n\n\n\n\nIndependence vs. Correlation\n\n\n\nIndependence means no influence between events, while correlation measures linear relationship strength.\nIndependence implies zero correlation, but zero correlation does not imply independence.\nE.g., \\(X\\) and \\(Y = X^2\\) have zero correlation but are not independent.\n\n\nBusiness Example: Suppose a retailer finds that sales and weather are correlated. This information can be used to adjust inventory planning for seasonal effects. If two marketing campaigns are independent, the outcome of one does not affect the other.\n\nExample 13 Suppose a customer visits a supermarket. Let:\n\n\\(A\\) = “customer buys a loaf of bread today” and \\(P(A)=30\\%\\).\n\\(B\\) = “customer buys laundry detergent today” and \\(P(B)=10\\%\\).\n\nWhat is the probability the customer buys both bread and detergent today? What does observing a bread purchase tell you about \\(P(B)\\)?\n\n\nSolution\n\n\nSolution 13. Bread (frequent staple) and detergent (infrequent refill) decisions are unrelated, so it is reasonable to assume independence. Hence \\[\nP(A \\text{ and } B) = 30\\% \\times 10\\% = 3\\%\n\\] Observing bread does not change \\(P(B)=10\\%\\).\n\n\nExample 14 If the correlation between two stocks is \\(-0.8,\\) what does this mean for portfolio diversification?\n\n\nSolution\n\n\nSolution 14. \nThey tend to move in opposite directions, aiding diversification and reducing portfolio variance.\n\n\n\n\n\n\n\nMutually Exclusive vs Independence\n\n\n\nMutually exclusive: Cannot happen together \\[P(A\\cap B)=0\\]\nIndependent: Knowledge of one does not change probability of the other \\[P(A\\cap B)=P(A)P(B)\\]\nThink: Disjoint = “never together”; Independent = “no influence”.",
    "crumbs": [
      "Home",
      "Basics",
      "Probability and Data in Business Analytics"
    ]
  },
  {
    "objectID": "01_introduction.html#population-vs.-sample-summary-estimators",
    "href": "01_introduction.html#population-vs.-sample-summary-estimators",
    "title": "Probability and Data in Business Analytics",
    "section": "5 Population vs. Sample: Summary & Estimators",
    "text": "5 Population vs. Sample: Summary & Estimators\nWhen analysing data we distinguish between:\n\nPopulation parameters (target): Fixed (but usually unknown) numerical characteristics of the underlying process (e.g., \\(\\mathbb{E}[X], \\text{Var}(X), \\gamma_1, \\kappa, \\text{Cov}(X,Y)\\)).\nSample statistics (estimators): Functions of observed data used to approximate population parameters (e.g., \\(\\bar{X}, s^2, s, \\hat{\\gamma}_1, \\hat{g}_2, s_{XY}, r_{XY}\\)). They vary from sample to sample.\n\nKey principles:\n\nReplace integrals / probability-weighted sums with empirical averages.\nCenter around the sample mean when the population mean is unknown.\nUse \\(n-1\\) (unbiased) denominators for second moments (variance, covariance) when estimating from an i.i.d. sample.\nAlways interpret sample statistics as estimates with sampling variability; risk management and forecasting should account for estimation error (e.g., standard deviation and confidence intervals).\n\n\n5.1 Quick Reference Table\n\n\n\n\n\n\n\n\n\nConcept\nPopulation Symbol / Definition\nSample Estimator\nNotes\n\n\n\n\nMean\n\\(\\mathbb{E}[X]\\)\n\\(\\bar{X}=\\tfrac{1}{n}\\sum X_i\\)\nUnbiased for mean (i.i.d.)\n\n\nVariance\n\\(\\text{Var}(X)=E[(X-\\mathbb{E}[X])^2]\\)\n\\(s^2=\\tfrac{1}{n-1}\\sum (X_i-\\bar{X})^2\\)\n\\(s_n^2\\) (divide by \\(n\\)) is biased low\n\n\nStd. Dev.\n\\(\\text{sd}(X)=\\sqrt{\\text{Var}(X)}\\)\n\\(s=\\sqrt{s^2}\\)\nPlug-in\n\n\nSkewness\n\\(\\gamma_1=\\mu_3/\\text{sd}^3\\), \\(\\mu_3=E[(X-\\mathbb{E}[X])^3]\\)\n\\(\\hat{\\gamma}_1=\\frac{n}{(n-1)(n-2)}\\sum (\\frac{X_i-\\bar{X}}{s})^3\\)\nMeasures asymmetry\n\n\nKurtosis (excess)\n\\(\\kappa-3\\), \\(\\kappa=\\mu_4/\\text{sd}^4\\)\n\\(\\hat{g}_2\\) (Fisher)\nTail heaviness vs. normal\n\n\nCovariance\n\\(\\text{Cov}(X,Y)=E[(X-\\mathbb{E}X)(Y-\\mathbb{E}Y)]\\)\n\\(s_{XY}=\\tfrac{1}{n-1}\\sum (X_i-\\bar{X})(Y_i-\\bar{Y})\\)\nSign = direction\n\n\nCorrelation\n\\(\\rho=\\dfrac{\\text{Cov}(X,Y)}{\\text{sd}(X)\\text{sd}(Y)}\\)\n\\(r_{XY}=\\dfrac{s_{XY}}{s_X s_Y}\\)\nScale-free ( -1 to 1 )\n\n\n\n\n\n\n\n\n\nInterpretation tip\n\n\n\nIf a sample statistic looks extreme (e.g., very high skewness or kurtosis), examine sample size and outliers; small samples amplify noise in higher-moment estimates.",
    "crumbs": [
      "Home",
      "Basics",
      "Probability and Data in Business Analytics"
    ]
  },
  {
    "objectID": "03_Linear_Regression.html",
    "href": "03_Linear_Regression.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "In this session, we will use the same CASchools dataset from the previous session and implement a simple linear regression model to explore a question of interest:",
    "crumbs": [
      "Home",
      "Basics",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "03_Linear_Regression.html#data-preparation",
    "href": "03_Linear_Regression.html#data-preparation",
    "title": "Simple Linear Regression",
    "section": "1 Data Preparation",
    "text": "1 Data Preparation\n\n# load packages and dataset\npkgs &lt;- c(\"tidyverse\", \"moments\", \"data.table\", \"ggsci\", \"stargazer\")\nmissing &lt;- setdiff(pkgs, rownames(installed.packages()))\nif (length(missing) &gt; 0) install.packages(missing)\ninvisible(lapply(pkgs, function(pkg) suppressPackageStartupMessages(library(pkg, character.only = TRUE))))\nf_name &lt;- \"https://raw.githubusercontent.com/my1396/FIN5005-Fall2025/refs/heads/main/data/CASchools_test_score.csv\"\ncas &lt;- read_csv(f_name,\n    col_types = cols(\n        county = col_factor(), # read as factor\n        grades = col_factor()\n    )\n)\ncas %&gt;% as.data.table()\n\n     district                          school      county grades students\n        &lt;num&gt;                          &lt;char&gt;      &lt;fctr&gt; &lt;fctr&gt;    &lt;num&gt;\n  1:    75119              Sunol Glen Unified     Alameda  KK-08      195\n  2:    61499            Manzanita Elementary       Butte  KK-08      240\n  3:    61549     Thermalito Union Elementary       Butte  KK-08     1550\n  4:    61457 Golden Feather Union Elementary       Butte  KK-08      243\n  5:    61523        Palermo Union Elementary       Butte  KK-08     1335\n ---                                                                     \n416:    68957          Las Lomitas Elementary   San Mateo  KK-08      984\n417:    69518            Los Altos Elementary Santa Clara  KK-08     3724\n418:    72611          Somis Union Elementary     Ventura  KK-08      441\n419:    72744               Plumas Elementary        Yuba  KK-08      101\n420:    72751            Wheatland Elementary        Yuba  KK-08     1778\n     teachers calworks   lunch computer expenditure    income   english  read\n        &lt;num&gt;    &lt;num&gt;   &lt;num&gt;    &lt;num&gt;       &lt;num&gt;     &lt;num&gt;     &lt;num&gt; &lt;num&gt;\n  1:    10.90   0.5102  2.0408       67    6384.911 22.690001  0.000000 691.6\n  2:    11.15  15.4167 47.9167      101    5099.381  9.824000  4.583333 660.5\n  3:    82.90  55.0323 76.3226      169    5501.955  8.978000 30.000002 636.3\n  4:    14.00  36.4754 77.0492       85    7101.831  8.978000  0.000000 651.9\n  5:    71.50  33.1086 78.4270      171    5235.988  9.080333 13.857677 641.8\n ---                                                                         \n416:    59.73   0.1016  3.5569      195    7290.339 28.716999  5.995935 700.9\n417:   208.48   1.0741  1.5038      721    5741.463 41.734108  4.726101 704.0\n418:    20.15   3.5635 37.1938       45    4402.832 23.733000 24.263039 648.3\n419:     5.00  11.8812 59.4059       14    4776.336  9.952000  2.970297 667.9\n420:    93.40   6.9235 47.5712      313    5993.393 12.502000  5.005624 660.5\n      math\n     &lt;num&gt;\n  1: 690.0\n  2: 661.9\n  3: 650.9\n  4: 643.5\n  5: 639.9\n ---      \n416: 707.7\n417: 709.5\n418: 641.7\n419: 676.5\n420: 651.0\n\n\nVariable of interest: Test scores\n\nread and math are average reading and math test scores for each district\nIn the last session, we show that these two variables are highly correlated (r=0.92)\nWe construct an average test score as the average of the test score for reading and the score of the math test.\n\n\n# compute TestScore and append it to CASchools\ncas &lt;- cas %&gt;%\n  mutate(TestScore = (read + math) / 2)",
    "crumbs": [
      "Home",
      "Basics",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "03_Linear_Regression.html#descriptive-statistics",
    "href": "03_Linear_Regression.html#descriptive-statistics",
    "title": "Simple Linear Regression",
    "section": "2 Descriptive Statistics",
    "text": "2 Descriptive Statistics\nRefer to the previous session for detailed summary statistics and visualizations.\nA quick preview\nv &lt;- setdiff(names(cas), c(\"district\", \"school\", \"county\", \"grades\"))\nstargazer(as.data.frame(cas[v]), type=\"html\", digits=2)\n\n\n\n\n\n\n\nStatistic\n\n\nN\n\n\nMean\n\n\nSt. Dev.\n\n\nMin\n\n\nMax\n\n\n\n\n\n\n\n\nstudents\n\n\n420\n\n\n2,628.79\n\n\n3,913.10\n\n\n81\n\n\n27,176\n\n\n\n\nteachers\n\n\n420\n\n\n129.07\n\n\n187.91\n\n\n4.85\n\n\n1,429.00\n\n\n\n\ncalworks\n\n\n420\n\n\n13.25\n\n\n11.45\n\n\n0.00\n\n\n78.99\n\n\n\n\nlunch\n\n\n420\n\n\n44.71\n\n\n27.12\n\n\n0.00\n\n\n100.00\n\n\n\n\ncomputer\n\n\n420\n\n\n303.38\n\n\n441.34\n\n\n0\n\n\n3,324\n\n\n\n\nexpenditure\n\n\n420\n\n\n5,312.41\n\n\n633.94\n\n\n3,926.07\n\n\n7,711.51\n\n\n\n\nincome\n\n\n420\n\n\n15.32\n\n\n7.23\n\n\n5.34\n\n\n55.33\n\n\n\n\nenglish\n\n\n420\n\n\n15.77\n\n\n18.29\n\n\n0.00\n\n\n85.54\n\n\n\n\nread\n\n\n420\n\n\n654.97\n\n\n20.11\n\n\n604.50\n\n\n704.00\n\n\n\n\nmath\n\n\n420\n\n\n653.34\n\n\n18.75\n\n\n605.40\n\n\n709.50\n\n\n\n\nTestScore\n\n\n420\n\n\n654.16\n\n\n19.05\n\n\n605.55\n\n\n706.75",
    "crumbs": [
      "Home",
      "Basics",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "03_Linear_Regression.html#scatter-plot",
    "href": "03_Linear_Regression.html#scatter-plot",
    "title": "Simple Linear Regression",
    "section": "3 Scatter Plot",
    "text": "3 Scatter Plot\nHypothesis: Higher spending per student leads to better test scores.\n\nggplot(cas, aes(x = expenditure, y = TestScore)) +\n    geom_point(alpha = 0.6, color = \"#1976d2\") +\n    geom_smooth(method = \"lm\", color = \"#d32f2f\", se = FALSE) +\n    labs(\n        title = \"Scatter Plot of Test Scores vs. Expenditure per Student\",\n        x = \"Expenditure per Student (USD)\",\n        y = \"Average Test Score\"\n    ) +\n    theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\nThere seems to be a positive relationship between expenditure per student and average test scores. Also note that the points are dispersed, indicating that other factors may also influence test scores. We will further explore this using multiple regression in future sessions.\nFor now, we will focus on a simple linear regression model with only one independent variable: expenditure per student.\nLet’s calculate the correlation coefficient between expenditure and test scores.\n\nwith(cas, cor.test(expenditure, TestScore))\n\n\n    Pearson's product-moment correlation\n\ndata:  expenditure and TestScore\nt = 3.9841, df = 418, p-value = 7.989e-05\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.09736861 0.28180139\nsample estimates:\n      cor \n0.1912728 \n\n\n\nWhat does the correlation coefficient tell us? Is it statistically significant? Justify your answer.\n\n\nSolution\n\n\n\nThe reported correlation coefficient is 0.19 between expenditure and TestScore.\nThis is a positive but weak correlation: higher expenditures are associated with higher test scores, but the relationship is not very strong.\nThe p-value = 7.989e-05, which is far below common significance levels (0.05, 0.01, or even 0.001).\nThis means we reject the null hypothesis that the true correlation is zero. The result is statistically significant.",
    "crumbs": [
      "Home",
      "Basics",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "03_Linear_Regression.html#simple-linear-regression",
    "href": "03_Linear_Regression.html#simple-linear-regression",
    "title": "Simple Linear Regression",
    "section": "4 Simple Linear Regression",
    "text": "4 Simple Linear Regression\n\n4.1 Model Specification\nWe will fit a simple linear regression model to examine the relationship between expenditure per student and average test scores.\n\\[\n\\text{TestScore}_i = \\beta_0 + \\beta_1 \\times \\text{expenditure}_i + \\varepsilon_i\n\\tag{1}\\]\nEquation 1 is the linear regression model with a single independent variable.\nWhere:\n\n\n\n\n\n\nKey Components of the Linear Regression Model\n\n\n\n\n\\(\\text{TestScore}_i\\) in the Left Hand Side (LHS) is the dependent variable (response variable). \\(i\\) indexes different school districts.\n\\(\\text{expenditure}_i\\) in the Right Hand Side (RHS) is the independent variable (explanatory variable or regressor).\n\\(\\beta_0\\) and \\(\\beta_1\\) are known as the parameters (coefficients) of the model.\n\n\\(\\beta_0\\) is the intercept, representing the expected value of TestScore when expenditure is zero.\n\\(\\beta_1\\) is the slope coefficient, representing the change in TestScore for a one dollar increase in expenditure.\n\n\\(\\varepsilon_i\\) is the error term, capturing all other factors affecting not included in the model, e.g, teacher quality, school facilities, parental involvement, etc.\n\n\n\nEquation 1 can be written more generally as:\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i\n\\]\n\n\\(\\beta_1\\) is the slope coefficient, which measures the change in the dependent variable \\(Y\\) associated with a one-unit increase in the independent variable \\(X.\\)\n\\(\\beta_0\\) is the intercept, which is the expected value of \\(Y\\) when \\(X=0;\\) it is the point at which the population regression line intersects the Y axis.\n\n📌 In some econometric applications, the intercept has a meaningful economic interpretation. In other applications, the intercept has no real-world meaning; for example, when \\(X\\) is the expenditure per student, strictly speaking the intercept is the expected value of test scores when there is no expenditure, which might be unrealistic.\nWhen the real-world meaning of the intercept is nonsensical, it is best to think of it simply as the coefficient that determines the level of the regression line.\n\n\n4.2 Estimating the Coefficients of the Linear Regression Model\nBased on the scatter plot and the correlation analysis, we expect a positive relationship between expenditure and test scores. We will use the Ordinary Least Squares (OLS) method to estimate the coefficients of the linear regression model.\nTo run this regression, we use the lm() function in R.\n\n# Fit the linear regression model\nmodel &lt;- lm(TestScore ~ expenditure, data = cas)\nsummary(model)\n\n\nCall:\nlm(formula = TestScore ~ expenditure, data = cas)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-50.146 -14.206   0.689  13.513  50.127 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 6.236e+02  7.720e+00  80.783  &lt; 2e-16 ***\nexpenditure 5.749e-03  1.443e-03   3.984 7.99e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.72 on 418 degrees of freedom\nMultiple R-squared:  0.03659,   Adjusted R-squared:  0.03428 \nF-statistic: 15.87 on 1 and 418 DF,  p-value: 7.989e-05\n\n\n\n\n4.3 Interpretation of the output\n\nA. Model Specification and Fitted Values\nThe regression estimates the effect of expenditure (per student, in USD) on test scores (combined reading and math).\nFrom Theory to Practice:\nWe started with the theoretical model: \\[\n\\text{TestScore}_i = \\beta_0 + \\beta_1 \\times \\text{expenditure}_i + \\varepsilon_i\n\\]\nUsing OLS estimation, we obtain the fitted model:\n\\[\n\\widehat{\\text{TestScore}} = 623.6 + 0.00575 \\times \\text{expenditure}\n\\]\nThe hat symbol (\\(\\widehat{\\text{TestScore}}\\)) indicates these are predicted values based on our sample data. Notice that we no longer have the error terms (\\(\\varepsilon_i\\)) because this equation gives us the predicted values from the regression line.\nUnderstanding Residuals:\nSince our model can’t perfectly predict every observation, we need to measure how far off our predictions are from reality:\n\\[\n\\text{Residual}_i = \\text{TestScore}_i - \\widehat{\\text{TestScore}}_i\n\\]\nResiduals represent the difference between actual and predicted values - essentially, they capture what our model couldn’t explain. In the original theoretical model, these correspond to the error terms (\\(\\varepsilon_i\\)) that we estimated.\n\n\n\nB. Coefficients\n\nIntercept (623.6): When expenditure = 0, the predicted test score is about 623.6. While not meaningful in practice (since expenditure cannot realistically be zero), it serves as the baseline of the regression line.\nExpenditure (0.00575): For every one-dollar increase in expenditure per student, the average test score is predicted to increase by 0.0057 points. Put differently: a $1,000 increase in expenditure is associated with a 5.75-point increase in test scores.\n\n\n\n\nC. Statistical Significance\nFor the slope coefficient (\\(\\beta_1\\)):\n\nThe t-value = 3.984 and p-value = 7.99e-05, which is well below 0.001.\nThis indicates that expenditure has a statistically significant positive effect on test scores.\n\n\n\n\nD. Model Fit\n\nR-squared = 0.0366 (about 3.7%).\n\nThis means expenditure explains only a small portion of the variation in test scores.\nMany other factors (teacher quality, socio-economic background, school resources, etc.) likely play a larger role.\n\nF-statistic = 15.87 with a p-value of 7.99e-05.\n\nThis tests the null hypothesis that all regression coefficients are equal to zero (no effect).\nThe low p-value indicates we reject the null hypothesis, confirming that the model has some explanatory power.\n\n\n\n\n\nE. Residuals\nThe investigation of residuals requires diagnostic test which checks the assumptions of linear regression (e.g., homoscedasticity, normality of errors).\nWe will cover them in future sessions.\nHere we provide a brief overview.\n\n\n\n\n\n\n\n\n\n\nThe first plot is “Residuals vs Fitted” plot.\nThe residuals appear reasonably balanced around zero, though the spread suggests substantial unexplained variation.\nThe spread of residuals seems to increase slightly with fitted values, indicating potential heteroskedasticity (non-constant variance of errors). This suggests that the assumption of homoskedasticity may be violated, which could affect the reliability of our coefficient estimates and standard errors.\nThe second plot is the normal Q-Q plot, which assesses whether the residuals are normally distributed.\n\nThe points generally follow the reference line, suggesting that the residuals are approximately normally distributed, although there shows thin tails.\nThis indicates that there are few extreme residuals. The model predictions don’t have large outliers.\n\n\nFrom the summary output:\n\nThe residual standard error = 18.72, which is the average size of prediction errors (in test score points).\n\n\n✅ Summary Interpretation:\nThe regression analysis shows a statistically significant positive relationship between school expenditure and test scores. On average, an additional \\$1,000 in per-student expenditure is associated with an increase of about 5.75 points in test scores. However, the explanatory power of the model is low (R² ≈ 3.7%), indicating that expenditure alone explains only a small fraction of the variation in test scores. This suggests that while spending matters, many other factors also influence student performance.\nTo gain a more comprehensive understanding of the factors affecting student achievement, future analyses should employ multiple regression models that incorporate additional explanatory variables such as student demographics, teacher qualifications, and school characteristics. We will cover multiple regression in subsequent sessions.",
    "crumbs": [
      "Home",
      "Basics",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "03_Linear_Regression.html#report-by-stargazer",
    "href": "03_Linear_Regression.html#report-by-stargazer",
    "title": "Simple Linear Regression",
    "section": "5 Report by Stargazer",
    "text": "5 Report by Stargazer\nstargazer produces well-formatted regression tables that can be easily included in reports or publications.\n\nstargazer(model, type = \"text\", digits = 2)\n\n\n===============================================\n                        Dependent variable:    \n                    ---------------------------\n                             TestScore         \n-----------------------------------------------\nexpenditure                   0.01***          \n                              (0.001)          \n                                               \nConstant                     623.62***         \n                              (7.72)           \n                                               \n-----------------------------------------------\nObservations                    420            \nR2                             0.04            \nAdjusted R2                    0.03            \nResidual Std. Error      18.72 (df = 418)      \nF Statistic           15.87*** (df = 1; 418)   \n===============================================\nNote:               *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\n\n\nReferences\n\n\nIntroduction to Econometrics with R",
    "crumbs": [
      "Home",
      "Basics",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "05_Hypothesis_Testing.html",
    "href": "05_Hypothesis_Testing.html",
    "title": "Hypothesis Testing and Confidence Intervals",
    "section": "",
    "text": "We have been using t-statistics and p-values to for testing hypotheses such as whether a correlation coefficient is significantly different from zero or if a regression coefficient is significantly different from zero.\nIn the following section, we will introduce the procedure of hypothesis testing formally.\nNote that we will mainly focus on two-sided hypothesis tests when testing for the significance of a single coefficient as they are most commonly used in practice.\nWe will use one-sided tests when introducing F-tests for the overall significance of a regression model and for model comparison.",
    "crumbs": [
      "Home",
      "Basics",
      "Hypothesis Testing and Confidence Intervals"
    ]
  },
  {
    "objectID": "05_Hypothesis_Testing.html#t-test-for-single-coefficient",
    "href": "05_Hypothesis_Testing.html#t-test-for-single-coefficient",
    "title": "Hypothesis Testing and Confidence Intervals",
    "section": "1 t-test for Single Coefficient",
    "text": "1 t-test for Single Coefficient\nUsing the simple linear regression model of California School Test Scores in the previous session as an example, test whether expenditure has a significant effect on test scores at the 5% significance level.\nCall:\nlm(formula = TestScore ~ expenditure, data = cas)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-50.146 -14.206   0.689  13.513  50.127 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 6.236e+02  7.720e+00  80.783  &lt; 2e-16 ***\nexpenditure 5.749e-03  1.443e-03   3.984 7.99e-05 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 18.72 on 418 degrees of freedom\nMultiple R-squared:  0.03659,   Adjusted R-squared:  0.03428 \nF-statistic: 15.87 on 1 and 418 DF,  p-value: 7.989e-05\nHere is the hypothesis test step by step for the expenditure coefficient in the simple linear regression of California school test scores.\n\n\nState Hypotheses\n\n\\(H_0:\\ \\beta_{\\text{exp}} = 0\\) (expenditure has no effect on test scores)\n\\(H_1:\\ \\beta_{\\text{exp}} \\neq 0\\) (two-sided)\n\nCalculate test statistic\n\\[\nt=\\frac{\\hat\\beta - \\beta}{\\text{SE}(\\hat\\beta)} \\sim t_{df}\n\\]\nwhere the test statistic follows a \\(t\\)-distribution with degrees of freedom (df) \\(= n - k - 1.\\) \\(k\\) is the number of predictors (excluding the intercept).\nComing back to the regression output, we have estimate \\(= 0.005749,\\) value to test against with \\(= 0,\\) standard error \\(= 0.001443.\\)\nThat is, \\(\\hat\\beta = 0.005749,\\) \\(\\beta = 0,\\) and \\(\\text{SE}(\\hat\\beta) = 0.001443,\\)\n\\[\nt=\\frac{\\hat\\beta - \\beta}{\\text{SE}(\\hat\\beta)}=\\frac{0.005749 - 0}{0.001443}=3.984\n\\]\nwhere \\(n=420\\) (number of observations) and \\(k=1\\) (number of predictors), so \\(df=420-1-1=418\\).\nFind critical value, \\(C_{\\alpha/2}.\\)\nFor significance level at 5%, i.e., \\(\\alpha=0.05\\), the critical value is given by\n\\[\nC_{\\alpha/2} = t_{0.975, df}\n\\]\nReferring to the \\(t\\)-distribution table, \\(t_{0.975,418}\\approx 1.96\\).\n\n\n\n\n\n\nFinding critical value\n\n\n\nNotice that when df is large, the critical value approaches that of the standard normal distribution, \\(z_{0.975}=1.96\\).\nThe rule-of-thumb is that when \\(df &gt; 30\\), you can use the standard normal critical values.\n\n\nDecision rule\n\nReject \\(H_0\\) if \\(|t|&gt;1.96\\).\nFail to reject \\(H_0\\) if \\(|t|\\leq 1.96\\).\n\nSince \\(3.984&gt;1.96\\), we reject \\(H_0\\) and conclude that \\(\\beta_{\\text{exp}}\\) is statistically significantly different from zero.\nInterpretation in context\nExpenditure has a statistically significant positive association with test scores at the 5% level. The point estimate implies that a $1,000 increase in per-student expenditure is associated with about \\(0.005749\\times 1000=5.75\\) points higher test scores.\n\n\n\n\n1.1 P-value Approach\nStatistical software often reports the p-value, which is the smallest significance level at which you would reject the null hypothesis.\n\nIf the p-value \\(&lt; \\alpha\\), reject \\(H_0\\).\nIf the p-value \\(\\geq \\alpha\\), fail to reject \\(H_0\\).\nCommon significance levels are 1%, 5%, and 10%.\nInterpretation: The probability of rejecting the null hypothesis when the null hypothesis is true.\n\nQ: How to find the p-value given test statistic?\nA: For two-sided test, the p-value is given by\n\\[\np\\text{-value} = 2\\;\\P(T &gt; |t|)\n\\]\n\nExample 1 A marketing manager wants to understand whether the number of social media posts influences monthly customer engagement for an online store. She runs a regression analysis and finds a t-statistic of \\(t = 2.457\\) for the coefficient on the number of posts, with \\(df = 30\\).\n\nCompute the two-sided p-value.\nBased on a 5% significance level, would you reject the null hypothesis that the number of posts has no effect on customer engagement?\n\n\n\nSolution\n\n\nThe p-value is obtained by: \\[p\\text{-value} = 2\\;\\P(T &gt; |t|) = 2 \\times 0.01 = 0.02\\] The p-value is 0.02, which is less than the common significance level of 0.05. Therefore, we reject the null hypothesis and conclude that the regression coefficient is statistically significant at the 5% level.\nInterpretation: There is statistically significant evidence that the number of social media posts affects customer engagement for the online store.\n\n\nExample 2 A financial analyst is studying the relationship between advertising expenditure and sales revenue for a chain of retail stores. She runs a regression and obtains an estimated coefficient for advertising spend. To test whether advertising has a statistically significant effect on sales, she computes a t-statistic of \\(t = 2.15\\) with \\(df = 25\\).\nAt the 5% significance level, should she reject the null hypothesis that advertising has no effect on sales?\n\n\nSolution\n\n\nThe critical value at 5% significance level is \\(t_{0.975,25}=2.060\\). Since \\(2.15 &gt; 2.060\\), we reject the null hypothesis at the 5% significance level.\nInterpretation: There is statistically significant evidence that advertising expenditure affects sales revenue for the retail chain.\n\n\nExample 3 A store manager wants to investigate whether the amount spent on in-store promotions affects weekly sales. She runs a regression and obtains a t-statistic of \\(t = 0.82\\) for the coefficient on promotion spending, with \\(df = 28\\).\nAt the 5% significance level, would you reject the null hypothesis that promotion spending has no effect on weekly sales?\n\n\nSolution\n\n\nThe critical value at 5% significance level is \\(t_{0.975,28}=2.048\\). Since \\(0.82 &lt; 2.048\\), we fail to reject the null hypothesis at the 5% significance level.\nInterpretation:\nThere is no statistically significant evidence that the amount spent on in-store promotions affects weekly sales.",
    "crumbs": [
      "Home",
      "Basics",
      "Hypothesis Testing and Confidence Intervals"
    ]
  },
  {
    "objectID": "05_Hypothesis_Testing.html#confidence-interval-for-single-coefficient",
    "href": "05_Hypothesis_Testing.html#confidence-interval-for-single-coefficient",
    "title": "Hypothesis Testing and Confidence Intervals",
    "section": "2 Confidence Interval for Single Coefficient",
    "text": "2 Confidence Interval for Single Coefficient\nA 95 percent confidence interval for the slope is given by\n\\[\n\\left(\\hat{\\beta}-C_{\\alpha/2}\\times SE(\\hat{\\beta}),\\; \\hat{\\beta}+C_{\\alpha/2}\\times SE(\\hat{\\beta})\\right)\n\\]\nwhere \\(C_{\\alpha/2}\\) is the critical value for a two-sided test at significance level \\(\\alpha\\).\nFor the expenditure coefficient in the simple linear regression of California school test scores, \\(\\hat{\\beta}=0.005749\\), \\(SE(\\hat{\\beta})=0.001443\\), and \\(C_{0.025}=1.96\\).\nA 95% confidence interval is therefore\n\\[\n0.005749 \\pm 1.96\\times 0.001443 \\approx [0.0029,\\ 0.0086],\n\\]\nwhich corresponds to roughly 2.9 to 8.6 points per $1,000.\nThe effect is statistically significant but modest in size, and the low \\(R^2\\) indicates that many other factors also influence test scores.",
    "crumbs": [
      "Home",
      "Basics",
      "Hypothesis Testing and Confidence Intervals"
    ]
  },
  {
    "objectID": "08_Interactions.html",
    "href": "08_Interactions.html",
    "title": "Interactions of Binary Variables",
    "section": "",
    "text": "🎯 Study Objectives\n\nInterpret interaction terms of two binary variables in regression models.\nUnderstand the importance of controlling for changes in interacting variables when interpreting coefficients.\nConduct and interpret a two-way ANOVA to compare nested regression models.\nInterpret interaction terms between a binary variable and a continuous variable in regression models.\nVisualize interaction effects using regression lines.\nFor this section of the course, we will be working with the Boston dataset, which contains 506 observations on housing values in various suburbs of Boston. This dataset provides detailed information on factors such as crime rates, property age, locations (proximity to the Charles River), and other socioeconomic variables.\nWe will use this data to explore and analyze the determinants of housing prices, applying regression techniques to understand how different factors influence the value of homes.\n# load packages and dataset\npkgs &lt;- c(\"tidyverse\", \"MASS\", \"data.table\", \"stargazer\", \"cowplot\")\nmissing &lt;- setdiff(pkgs, rownames(installed.packages()))\nif (length(missing) &gt; 0) install.packages(missing)\ninvisible(lapply(pkgs, function(pkg) suppressPackageStartupMessages(library(pkg, character.only = TRUE))))\ndata(Boston) # Boston housing data\nUse ?Boston to see the description of the dataset.\nHere are some variables in the dataset which we are going to use in this session.",
    "crumbs": [
      "Home",
      "Basics",
      "Interactions of Binary Variables"
    ]
  },
  {
    "objectID": "08_Interactions.html#interactions-of-two-binary-variables",
    "href": "08_Interactions.html#interactions-of-two-binary-variables",
    "title": "Interactions of Binary Variables",
    "section": "1 Interactions of Two Binary Variables",
    "text": "1 Interactions of Two Binary Variables\nConsider the following regression model\n\\[\nmedv_i = \\beta_0 + \\beta_1\\, chas_i + \\beta_2\\, old_i + \\beta_3\\, (chas_i \\times old_i) + u_i\n\\tag{1}\\]\nwhere \\(chas_i\\) and \\(old_i\\) are dummy variables defined as follows:\n\\[\n\\begin{aligned}\nchas_i &= \\begin{cases}\n1 & \\text{if the suburb is next to the Charles River} \\\\\n0 & \\text{otherwise}\n\\end{cases} \\\\\nold_i &= \\begin{cases}\n1 & \\text{if } age_i \\ge 95 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\end{aligned}\n\\]\nwhere \\(agi_i\\) being the proportion of owner-occupied units built prior to 1940.\nInstructions:\n\nGenerate and append the binary variable \\(old\\) to the dataset Boston.\nConduct the regression stated above and assign the result to mod_bb.\nObtain a robust coefficient summary of the model. How do you interpret the results?\n\n\nBoston &lt;- Boston %&gt;%\n    mutate(old = ifelse(age &gt;= 95, 1, 0))\nmod_bb &lt;- lm(medv ~ chas*old, data = Boston)\nsummary(mod_bb)\n\n\nCall:\nlm(formula = medv ~ chas * old, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.771  -4.765  -1.683   2.776  33.433 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  23.6989     0.4503  52.624  &lt; 2e-16 ***\nchas          4.0582     1.6872   2.405   0.0165 *  \nold          -7.1319     0.9493  -7.513 2.67e-13 ***\nchas:old     10.5462     3.7577   2.807   0.0052 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.604 on 502 degrees of freedom\nMultiple R-squared:  0.1301,    Adjusted R-squared:  0.1249 \nF-statistic: 25.02 on 3 and 502 DF,  p-value: 4.244e-15\n\n\nThe estimated regression model is given by\n\\[\n\\widehat{medv}_i = 23.70 + 4.06\\, chas_i - 7.13\\, old_i + 10.55\\, (chas_i \\times old_i)\n\\]\nInterpretation of the coefficients\nWe start with listing all possible combinations of the binary variables \\(chas_i\\) and \\(old_i\\).\n\n\n\n\n\n\n\n\n\nScenario\n\\(chas_i\\)\n\\(old_i\\)\nInterpretation\n\n\n\n\n1\n0\n0\nSuburb not next to Charles River and new\n\n\n2\n0\n1\nSuburb not next to Charles River and old\n\n\n3\n1\n0\nSuburb next to Charles River and new\n\n\n4\n1\n1\nSuburb next to Charles River and old\n\n\n\nBased on the estimated regression model, we can compute the expected value of medv for each of the four groups.\n\n\n\n\n\n\n\n\n\n\nScenario\n\\(chas_i\\)\n\\(old_i\\)\n\\(chas_i \\times old_i\\)\nExpected value of medv\n\n\n\n\n1\n0\n0\n0\n\\(\\color{#0099FF}{\\hat{\\beta}_0} = 23.70\\)\n\n\n2\n0\n1\n0\n\\({\\color{#0099FF}\\hat{\\beta}_0} + {\\color{#00CC66}\\hat{\\beta}_2} = {\\color{#0099FF}23.70} + {\\color{#00CC66}(- 7.13)} = 16.57\\)\n\n\n3\n1\n0\n0\n\\({\\color{#0099FF}\\hat{\\beta}_0} + {\\color{#fb5b89}\\hat{\\beta}_1} = {\\color{#0099FF}23.70} + {\\color{#fb5b89}4.06} = 27.76\\)\n\n\n4\n1\n1\n1\n\\({\\color{#0099FF}\\hat{\\beta}_0} + {\\color{#fb5b89}\\hat{\\beta}_1} + {\\color{#00CC66}\\hat{\\beta}_2} + {\\color{orange}\\hat{\\beta}_3} = {\\color{#0099FF}23.70} + {\\color{#fb5b89}4.06} + {\\color{#00CC66}(-7.13)} + {\\color{orange}(10.55)} = 31.18\\)\n\n\n\nFrom the table above, we can interpret the coefficients as follows:\n\n\\(\\color{#0099FF}{\\hat{\\beta}_0} = 23.70\\) is the expected value of medv for suburbs that are not next to the Charles River and are new (i.e., old = 0).\nComparing scenario 2 with 1: \\(\\color{#00CC66}{\\hat{\\beta}_2} = -7.13\\) is the difference in expected value of medv between new and old suburbs that are NOT next to the Charles River.\nIn other words, old suburbs that are not next to the Charles River have an expected medv that is 7.13 lower than new suburbs that are NOT next to the Charles River.\nComparing scenario 3 with 1: \\(\\color{#fb5b89}{\\hat{\\beta}_1} = 4.06\\) is the difference in expected value of medv between suburbs that are next to the Charles River and those that are not, among new suburbs (i.e., old = 0).\nIn other words, new suburbs that are next to the Charles River have an expected medv that is 4.06 higher than new suburbs that are not next to the Charles River.\n\\(\\color{orange}{\\hat{\\beta}_3} = 10.55\\) has two interpretations:\n\nIf we compare scenario 4 with scenario 2, the difference in expected value of medv is given by \\({\\color{#fb5b89}\\hat{\\beta}_1} + {\\color{orange}\\hat{\\beta}_3} = 4.06 + 10.55 = 14.61.\\)\nThis means that old suburbs that are next to the Charles River have an expected medv that is 14.61 higher than old suburbs that are not next to the Charles River.\n→ \\(\\color{orange}{\\hat{\\beta}_3}\\) can be interpreted as the difference in the effect of being next to the Charles River between old and new suburbs.\nIf we compare scenario 4 with scenario 3, the difference in expected value of medv is given by \\({\\color{#00CC66}\\hat{\\beta}_2} + {\\color{orange}\\hat{\\beta}_3} = -7.13 + 10.55 = 3.42.\\)\nThis means that old suburbs that are next to the Charles River have an expected medv that is 3.42 higher than new suburbs that are next to the Charles River.\n→ \\(\\color{orange}{\\hat{\\beta}_3}\\) can be interpreted as the difference in the effect of being old between suburbs that are next to the Charles River and those that are not.\n\n\n\n\n\n\n\n\nTip\n\n\n\nControlling for changes in the interacting variables is essential when interpreting the coefficient of interaction terms.\nA common approach is to substitute specific values for the interacting variables and compute the expected value of the dependent variable under different scenarios. This allows for a clearer interpretation of the coefficients by comparing how the expected value of the dependent variable varies across these scenarios.\n\n\n\n1.1 Add More Controls Variables\nWe can also add more control variables to Equation 1 to improve the model fit (Adjusted \\(R^2 = 12\\%\\) as of now). For example, we can add lstat and crim as additional control variables.\n\\[\nmedv_i = \\beta_0 + \\beta_1\\, chas_i + \\beta_2\\, old_i + \\beta_3\\, (chas_i \\times old_i) + \\beta_4\\, lstat_i + \\beta_5\\, crim_i + u_i\n\\tag{2}\\]\nWe estimate the above regression model as follows.\n\nmod_bb2 &lt;- lm(medv ~ chas * old + lstat + crim, data = Boston)\n# reorder the variables for stargazer output\nvars.order &lt;- c(\"chas\", \"old\", \"chas:old\", \"lstat\", \"crim\")\nstargazer(mod_bb, mod_bb2,\n    type = \"text\",\n    column.labels = c(\"Restricted\", \"Unrestricted\"),\n    order = paste0(\"^\", vars.order, \"$\")\n)\n\n\n====================================================================\n                                  Dependent variable:               \n                    ------------------------------------------------\n                                          medv                      \n                          Restricted              Unrestricted      \n                              (1)                     (2)           \n--------------------------------------------------------------------\nchas                        4.058**                 4.000***        \n                            (1.687)                 (1.183)         \n                                                                    \nold                        -7.132***                1.748**         \n                            (0.949)                 (0.776)         \n                                                                    \nchas:old                   10.546***                 4.008          \n                            (3.758)                 (2.651)         \n                                                                    \nlstat                                              -0.949***        \n                                                    (0.046)         \n                                                                    \ncrim                                                -0.079**        \n                                                    (0.036)         \n                                                                    \nConstant                   23.699***               34.106***        \n                            (0.450)                 (0.573)         \n                                                                    \n--------------------------------------------------------------------\nObservations                  506                     506           \nR2                           0.130                   0.574          \nAdjusted R2                  0.125                   0.570          \nResidual Std. Error    8.604 (df = 502)         6.033 (df = 500)    \nF Statistic         25.017*** (df = 3; 502) 134.705*** (df = 5; 500)\n====================================================================\nNote:                                    *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nComparing the two models:\n\nThe coefficient of old reversed signs, from negative (\\(-7.13\\)) to positive (\\(1.75\\)), suggesting that after accounting for crime and low socioeconomic status, older suburbs have slightly higher values.\nNegative coefficients for lstat (\\(-0.95\\)) and crim (\\(-0.079\\)) indicate that higher poverty and crime reduce house values.\n\nPlot lstat and crim group by old to see how the distribution of these two variables differ between new and old suburbs.\n\np1 &lt;- ggplot(Boston, aes(x = factor(old), y = lstat)) +\n    geom_boxplot(fill = \"lightblue\")\np2 &lt;- ggplot(Boston, aes(x = factor(old), y = crim)) +\n    geom_boxplot(fill = \"lightblue\")\nplot_grid(p1, p2, labels = c(\"A\", \"B\"), ncol = 2)\n\n\n\n\n\n\n\n\nWe see that old suburbs tend to have higher lstat and crim values compared to new suburbs. This suggests that controlling for these variables is important when examining the relationship between medv, chas, and old.\n\n\n1.2 Two-Way ANOVA Mathematical Formulation\nWe now conduct a two-way ANOVA to formally compare the two models mod_bb and mod_bb2.\n\nanova(mod_bb, mod_bb2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ chas * old\nModel 2: medv ~ chas * old + lstat + crim\n  Res.Df   RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    502 37161                                  \n2    500 18200  2     18961 260.45 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nState the null and alternative hypotheses. \\[\n\\begin{aligned}\nH_0 &: \\beta_4 = 0 \\text{ and } \\beta_5 = 0 \\\\\nH_1 &: \\beta_4 \\ne 0 \\text{ or } \\beta_5 \\ne 0\n\\end{aligned}\n\\]\nIn plain language:\n\n\\(H_0\\): The additional variables lstat and crim do not improve the model fit.\n\\(H_1\\): At least one of the additional variables lstat or crim improves the model fit.\n\nCalculate the F-statistic.\nThe F-statistic is based on the \\(R^2\\) of the two regressions: \\[\nF = \\left(\\frac{n - k_U -1}{p}\\right) \\left(\\frac{R_U^2-R_R^2}{1-R_U^2}\\right) \\sim F(p, n-k_U-1) .\n\\]\nwhere\n\n\\(n = 506\\) is the number of observations;\n\\(k_U = 5\\) is the number of predictors in the unrestricted model (where \\(H_1\\) is allowed to be true), excluding the intercept;\n\\(p = 2\\) is the number of restrictions (i.e., the number of additional variables in the unrestricted model);\n\\(R_U^2 = 0.574\\) is the \\(R^2\\) of the unrestricted model;\n\\(R_R^2 = 0.130\\) is the \\(R^2\\) of the restricted model.\n\nPlug in the numbers, we get \\[\nF = \\left(\\frac{506-5-1}{2}\\right) \\left(\\frac{0.574-0.130}{1-0.574}\\right) = 260.56\n\\]\nFind the critical value\nThe F-statistic follows an \\(F(2, 502)\\) distribution under the null hypothesis. At 5% significance level, we use the the critical value \\(F_{2,\\infty}=3.00.\\)\nDecision rule.\nSince the calculated F-statistic (\\(260.56\\)) is much larger than the critical value (\\(3.00\\)), we reject the null hypothesis.\nThis suggests that at least one of lstat and crim significantly improves the model fit.\nConclusion.\nWe conclude that model (2) with additional control variables lstat and crim provides a significantly better fit to the data compared to model (1) without these controls.",
    "crumbs": [
      "Home",
      "Basics",
      "Interactions of Binary Variables"
    ]
  },
  {
    "objectID": "08_Interactions.html#interactions-of-a-binary-variable-and-a-continuous-variable",
    "href": "08_Interactions.html#interactions-of-a-binary-variable-and-a-continuous-variable",
    "title": "Interactions of Binary Variables",
    "section": "2 Interactions of a Binary Variable and a Continuous Variable",
    "text": "2 Interactions of a Binary Variable and a Continuous Variable\nAn interaction between a binary variable and a continuous variable shows how the effect of the continuous variable changes depending on the group. This helps us see differences that a simple line wouldn’t capture. For example, the effect of business area on house prices might be different in new versus old suburbs.\nNow consider the regression model\n\\[\nmedv_i = \\beta_0 + \\beta_1 \\times indus_i + \\beta_2 \\times old_i + \\beta_3 \\times (indus_i\\times old_i) + u_i\n\\]\nwhere \\(indus_i\\) being the proportion of non-retail business acres in suburb \\(i\\); \\(old_i\\) is the same binary variable as defined above, indicating if the suburb is new.\nInstructions:\n\nEstimate the above regression model and assign the result to mod_bc.\nWhat is the relationship between medv and indus for new suburbs? What about old suburbs?\nIs the effect of indus on medv statistically different between new and old suburbs?\nPlot medv against indus and add the regression lines for both states of the binary variable old.\n\n\nmod_bc &lt;- lm(medv ~ indus * old, data = Boston)\nsummary(mod_bc)\n\n\nCall:\nlm(formula = medv ~ indus * old, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-12.379  -5.066  -1.588   3.015  33.046 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 30.06350    0.72882  41.250   &lt;2e-16 ***\nindus       -0.65844    0.06569 -10.024   &lt;2e-16 ***\nold         -7.48438    3.07918  -2.431   0.0154 *  \nindus:old    0.37115    0.17558   2.114   0.0350 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.024 on 502 degrees of freedom\nMultiple R-squared:  0.2434,    Adjusted R-squared:  0.2389 \nF-statistic: 53.83 on 3 and 502 DF,  p-value: &lt; 2.2e-16\n\n\nThe estimated regression model is given by\n\\[\n\\widehat{medv}_i = 30.06 - 0.66 \\times indus_i - 7.48 \\times old_i + 0.37 \\times (indus_i \\cdot old_i)\n\\]\nInterpretation of the coefficients\n\nFor new suburbs (i.e., old = 0), the relationship between medv and indus is given by\n\\[\nE(medv_i | old_i = 0) = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times indus_i = 30.06 - 0.66 \\times indus_i\n\\]\nThus, for new suburbs, a one-unit increase in indus is associated with a decrease in the expected value of medv by 0.66.\nFor old suburbs (i.e., old = 1), the relationship between medv and indus is given by\n\\[\n\\begin{split}\nE(medv_i | old_i = 1) &= (\\hat{\\beta}_0 + \\hat{\\beta}_2) + (\\hat{\\beta}_1 + \\hat{\\beta}_3) \\times indus_i \\\\\n&= (30.06 - 7.48) + (-0.66 + 0.37) \\times indus_i \\\\\n&= 22.58 - 0.29 \\times indus_i\n\\end{split}\n\\]\nThus, for old suburbs, a one-unit increase in indus is associated with a decrease in the expected value of medv by 0.29. The negative effect is smaller in old suburbs due to the positive interaction term (\\(\\hat{\\beta}_3 = 0.37\\)).\nThe effect of indus on medv is statistically different between new and old suburbs at 5% significance level given that the coefficient of the interaction term (\\(\\hat{\\beta}_3\\)) is statistically significant.\n\nVisualization\nNow we plot medv against indus with regression lines for both states of the binary variable old.\n\nggplot(Boston, aes(x = indus, y = medv, color = factor(old))) +\n    geom_point(alpha = 0.6) +\n    geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE) +\n    labs(\n        x = \"Proportion of Non-Retail Business Acres (indus)\",\n        y = \"Median Home Value (medv)\",\n        color = \"Old Suburb (old)\"\n    ) +\n    scale_color_manual(values = c(\"0\" = \"blue\", \"1\" = \"red\"), labels = c(\"New Suburb\", \"Old Suburb\")) +\n    theme_minimal(base_size = 14) +\n    theme(\n        legend.title = element_blank(),\n        legend.text = element_text(size = 14),\n        legend.position = c(0.8, 0.9)\n    )\n\n\n\n\n\n\n\n\nWe see that the slope of the regression line for new suburbs (blue) is steeper than that for old suburbs (red), indicating a stronger negative relationship between medv and indus in new suburbs compared to old suburbs. This visual representation aligns with our earlier interpretation of the regression coefficients.",
    "crumbs": [
      "Home",
      "Basics",
      "Interactions of Binary Variables"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "01_Lab-1.html",
    "href": "01_Lab-1.html",
    "title": "Lab 1: Probability & Descriptive Statistics",
    "section": "",
    "text": "Open in Google Colab: Link\nAlternatively, copy and paste the code below into RStuido Desktop or RStudio Cloud.\nFinish Quiz for Lab 1 on Canvas.\nFocus: explore core descriptive statistics and dependence concepts using simple synthetic data.\nYou will:\n\nExamine independence & correlation\nCompare skewed and roughly normal distributions\nCompute mean, variance, sd, skewness, kurtosis\nCompare biased vs unbiased variance estimators\nSimulate a high-correlation (ρ = 0.90) two-asset return setting\nStudy sampling distributions of asset and portfolio means",
    "crumbs": [
      "Home",
      "Basics",
      "Lab 1: Probability & Descriptive Statistics"
    ]
  },
  {
    "objectID": "01_Lab-1.html#instructions",
    "href": "01_Lab-1.html#instructions",
    "title": "Lab 1: Probability & Descriptive Statistics",
    "section": "",
    "text": "Open in Google Colab: Link\nAlternatively, copy and paste the code below into RStuido Desktop or RStudio Cloud.\nFinish Quiz for Lab 1 on Canvas.\nFocus: explore core descriptive statistics and dependence concepts using simple synthetic data.\nYou will:\n\nExamine independence & correlation\nCompare skewed and roughly normal distributions\nCompute mean, variance, sd, skewness, kurtosis\nCompare biased vs unbiased variance estimators\nSimulate a high-correlation (ρ = 0.90) two-asset return setting\nStudy sampling distributions of asset and portfolio means",
    "crumbs": [
      "Home",
      "Basics",
      "Lab 1: Probability & Descriptive Statistics"
    ]
  },
  {
    "objectID": "01_Lab-1.html#datasets",
    "href": "01_Lab-1.html#datasets",
    "title": "Lab 1: Probability & Descriptive Statistics",
    "section": "2 Datasets",
    "text": "2 Datasets\nSynthetic generators used:\n\nRight-skewed transaction values (log-normal)and approx. normal reference sample\nTwo correlated asset return series (ρ = 0.90) for sampling exercises\n\nRun each cell sequentially. Read the comments carefully.\n\n# -------- Setup: packages --------\n# Unified required package list\npkgs &lt;- c(\"tidyverse\", \"data.table\", \"ggsci\", \"moments\", \"knitr\", \"kableExtra\")\nmissing &lt;- setdiff(pkgs, rownames(installed.packages()))\nif (length(missing) &gt; 0) install.packages(missing)\n\n# Load all packages (silently)\ninvisible(lapply(pkgs, library, character.only = TRUE))\n\n# Set default options for figures in Jupyter Notebook\noptions(repr.plot.width = 12, repr.plot.height = 4)  # wider default figures\n\nmessage(\"\\n Setup complete (packages loaded: \", paste(pkgs, collapse = \", \"), \").\")\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n\n✔ dplyr     1.1.4          ✔ readr     2.1.5     \n\n✔ forcats   1.0.0          ✔ stringr   1.5.1     \n\n✔ ggplot2   3.5.2.9002     ✔ tibble    3.3.0     \n\n✔ lubridate 1.9.4          ✔ tidyr     1.3.1     \n\n✔ purrr     1.1.0          \n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n\n✖ dplyr::filter() masks stats::filter()\n\n✖ dplyr::lag()    masks stats::lag()\n\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nAttaching package: ‘data.table’\n\n\n\n\n\nThe following objects are masked from ‘package:lubridate’:\n\n\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n\n    yday, year\n\n\n\n\n\nThe following objects are masked from ‘package:dplyr’:\n\n\n\n    between, first, last\n\n\n\n\n\nThe following object is masked from ‘package:purrr’:\n\n\n\n    transpose\n\n\n\n\n\n\n\nAttaching package: ‘kableExtra’\n\n\n\n\n\nThe following object is masked from ‘package:dplyr’:\n\n\n\n    group_rows\n\n\n\n\n\n\n\n Setup complete (packages loaded: tidyverse, data.table, ggsci, moments, knitr, kableExtra).",
    "crumbs": [
      "Home",
      "Basics",
      "Lab 1: Probability & Descriptive Statistics"
    ]
  },
  {
    "objectID": "01_Lab-1.html#independence-vs.-zero-correlation",
    "href": "01_Lab-1.html#independence-vs.-zero-correlation",
    "title": "Lab 1: Probability & Descriptive Statistics",
    "section": "3 Independence vs. Zero Correlation",
    "text": "3 Independence vs. Zero Correlation\nIndependence means no influence between events, while correlation measures linear relationship strength.\nIndependence implies zero correlation, but zero correlation does not imply independence.\nE.g., \\(X\\) and \\(Y = X^2\\) have zero correlation but are not independent.\n\n# ---- Zero Correlation but NOT Independence: Y = X^2 ----\n# Idea: X ~ N(0,1); define Y = X^2.  Then cor(X,Y) ~ 0 (symmetry cancels linear relation),\n# yet Y is a deterministic function of X so they are NOT independent.\n\nset.seed(2025)\nn &lt;- 5000\nX &lt;- rnorm(n)\nY &lt;- X^2\nr_xy &lt;- cor(X, Y)\ncat(sprintf(\"Sample correlation cor(X, Y=X^2) = %.4f (near 0)\\n\", r_xy))\n\nSample correlation cor(X, Y=X^2) = 0.0198 (near 0)\n\n\nWe got \\(\\rho_{X,Y}=0.0198\\). It seems to be close to zero, but we need a statistical test to confirm this formally.\n→ Test for significance of the correlation coefficient\n\ncor.test(X, Y)\n\n\n    Pearson's product-moment correlation\n\ndata:  X and Y\nt = 1.4027, df = 4998, p-value = 0.1608\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.007887062  0.047529726\nsample estimates:\n       cor \n0.01983657 \n\n\n\n💡 Q: Based on the output of cor.test, what is the p-value for the correlation test? What can you conclude about the correlation between the two variables? Are \\(X\\) and \\(Y\\) independent? Write your answer in the cell below.\nA: [Type your answer here]\nDistinguish the two expressions:\n\ncorrelation coefficient equals zero ❌\ncorrelation coefficient (statistically) indifferent from zero ✅\n\n\nNow let’s visualize \\(Y=X^2\\) by plotting the scatter plot.\n\nlibrary(ggplot2)\noptions(repr.plot.width = 8, repr.plot.height = 7)\nscatter_df &lt;- data.frame(X = X, Y = Y)\np1 &lt;- ggplot(scatter_df, aes(X, Y)) +\n  geom_point(alpha = 0.25, color = '#2c7fb8') +\n  annotate('text', x = min(X)+0.2, y = max(Y)*0.95,\n           label = paste0('cor = ', sprintf('%.3f', r_xy)),\n           hjust = 0, size = 4) +\n  labs(title = 'Zero Linear Correlation but Nonlinear Dependence',\n       subtitle = 'Y = X^2 with X ~ N(0,1)',\n       x = 'X', y = 'Y = X^2') +\n  theme_minimal()\nprint(p1)\n\n\n\n\n\n\n\n\nThe following figure shows the scatter plot of two assets with various correlation coefficients.",
    "crumbs": [
      "Home",
      "Basics",
      "Lab 1: Probability & Descriptive Statistics"
    ]
  },
  {
    "objectID": "01_Lab-1.html#descriptive-statistics-on-asset-returns",
    "href": "01_Lab-1.html#descriptive-statistics-on-asset-returns",
    "title": "Lab 1: Probability & Descriptive Statistics",
    "section": "4 Descriptive Statistics on Asset Returns",
    "text": "4 Descriptive Statistics on Asset Returns\n\n# ---- Load Asset Returns Data ----\nasset_df &lt;- read_csv(\"https://raw.githubusercontent.com/my1396/FIN5005-Fall2025/refs/heads/main/data/asset_returns.csv\")\nprint(\"Preview: first 6 rows of asset returns data frame:\")\nhead(asset_df) %&gt;% round(2)\n\n[1] \"Preview: first 6 rows of asset returns data frame:\"\n\n\n\nA tibble: 6 × 3\n\n\nAsset_A\nAsset_B\nAsset_C\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n17.89\n4.31\n9.94\n\n\n10.30\n4.17\n10.55\n\n\n20.87\n14.16\n6.61\n\n\n18.21\n22.33\n9.58\n\n\n17.49\n2.23\n10.23\n\n\n29.87\n15.72\n8.97\n\n\n\n\n\nWe define a summary statistics function to compute the statistics of interest.\n\nquick_summary &lt;- function(x) {\n  # Function to compute basic descriptive statistics\n  data.frame(\n    n = length(x),\n    mean = mean(x),\n    sd = sd(x),\n    var = var(x),\n    skewness = moments::skewness(x),\n    kurtosis = moments::kurtosis(x),\n    row.names = NULL\n  )\n}\n\nThe summary statistic table is generated as follows:\n\nlapply(asset_df, quick_summary) %&gt;%\n    do.call(rbind, .) %&gt;%\n    kable(digits = 2, caption = \"Descriptive Statistics of Asset Returns\")\n\n\n\nTable: Descriptive Statistics of Asset Returns\n\n|        |     n|  mean|   sd|   var| skewness| kurtosis|\n|:-------|-----:|-----:|----:|-----:|--------:|--------:|\n|Asset_A | 10000| 20.14| 4.82| 23.24|    -0.10|     3.15|\n|Asset_B | 10000|  8.50| 4.44| 19.71|     1.71|     9.19|\n|Asset_C | 10000|  9.99| 1.01|  1.01|    -0.85|     3.50|\n\n\nPlot the histograms of the asset returns to visualize their distributions.\n\n# ---- Visualization: Histograms with Normal Density Overlay ----\n# Convert to long form\ncombined &lt;- asset_df %&gt;% pivot_longer(cols = everything(), names_to = \"type\", values_to = \"value\")\n\n# Compute per-type mean & sd and create normal curve points\nnorm_params &lt;- combined %&gt;% group_by(type) %&gt;% summarise(mu = mean(value), sigma = sd(value), .groups = 'drop')\n\nnorm_curve &lt;- norm_params %&gt;% group_by(type) %&gt;% do({\n  mu &lt;- .$mu; sigma &lt;- .$sigma\n  x &lt;- seq(mu - 4*sigma, mu + 4*sigma, length.out = 400)\n  tibble(value = x, density = dnorm(x, mu, sigma))\n})\n\noptions(repr.plot.width = 15, repr.plot.height = 6)\n\nggplot(combined, aes(value)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 40, fill = \"#3182bd\", alpha = 0.8, color = \"white\") +\n  geom_line(data = norm_curve, aes(value, density), color = \"red\", linewidth = 0.8) +\n  facet_wrap(~ type, scales = \"free\", nrow = 1) +\n  labs(title = \"Distribution Contrast with Normal Overlay\",\n       subtitle = \"Red curve = fitted normal using sample mean & sd per asset\",\n       x = \"Value\", y = \"Density\") +\n  theme(panel.spacing.x = unit(1.2, \"lines\"))\n\n\n\n\n\n\n\n\n\n💡 Q: Answer the following questions based on the summary statistics and the distribution plots:\n\nWhich asset has the highest mean return?\nWhich asset is the most risky? and which one has the most stable returns?\nWhich asset shows the positive skewness and what does it mean?\nIf an investor prefers assets with low risk and return distributions with tails close to normal, which asset is most appropriate?\nBased on mean and standard deviation, which asset has the best risk–return trade-off (highest mean per unit of risk)?\n\nA: [Type your answer here]",
    "crumbs": [
      "Home",
      "Basics",
      "Lab 1: Probability & Descriptive Statistics"
    ]
  },
  {
    "objectID": "01_Lab-1.html#large-sample-gives-more-accurate-estimates",
    "href": "01_Lab-1.html#large-sample-gives-more-accurate-estimates",
    "title": "Lab 1: Probability & Descriptive Statistics",
    "section": "5 Large Sample Gives More Accurate Estimates",
    "text": "5 Large Sample Gives More Accurate Estimates\nWe have a variable \\(X\\sim N(0,5^2).\\)\nNormal distribution notation: \\(X\\sim N(\\mu, \\sigma^2)\\) means that \\(X\\) is normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^2.\\)\nThe second parameter is the variance, NOT the standard deviation.\nNow we draw random samples of size 5, 10, 10, … until 5000 from this distribution and compute the sample mean and standard deviation for each sample.\nFor each sample, we compute the biased and unbiased sample variance\n\nThe biased sample variance is computed as \\[\\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^2.\\]\nThe unbiased sample variance is computed as \\[\\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2\\]\n\n\n# ---- Simulation: Sample vs Population variance ----\nset.seed(2025)  # for reproducibility\ntrue_var &lt;- 25  # sd^2 with sd=5, true population variance\nsample_sizes &lt;- c(5, 10, 20, 50, 100, 250, 500, 1000, 5000) # varying sample sizes from 5 to 500\nresults &lt;- lapply(sample_sizes, function(n){\n  x &lt;- rnorm(n, mean = 0, sd = 5)\n  data.frame(\"sample_size\" = n, \"var_biased\" = mean((x - mean(x))^2), \"var_unbiased\" = var(x))\n}) %&gt;% dplyr::bind_rows()\ncat('Variance estimators: var_n (biased, divide by n) vs var_n1 (unbiased, divide by n-1).\\n')\nresults\n\nVariance estimators: var_n (biased, divide by n) vs var_n1 (unbiased, divide by n-1).\n\n\n\nA data.frame: 9 × 3\n\n\nsample_size\nvar_biased\nvar_unbiased\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n5\n4.262707\n5.328384\n\n\n10\n14.832929\n16.481032\n\n\n20\n33.210505\n34.958426\n\n\n50\n25.332410\n25.849398\n\n\n100\n24.957059\n25.209150\n\n\n250\n22.936725\n23.028840\n\n\n500\n25.862610\n25.914439\n\n\n1000\n25.574182\n25.599782\n\n\n5000\n25.301658\n25.306719\n\n\n\n\n\n\n💡 Q: Based on the variance estimates for different sample sizes:\n\nAs the sample size increases, how do the sample variance estimates compare to the true variance \\(25\\)?\nDoes the sample variance converge to the true variance as the sample size increases?\nWhat is the difference between the biased and unbiased sample variance estimators?\n\nA: [Type your answer here]\n\n\n# ---- Plot: Convergence of variance estimators ----\n\nresults_long &lt;- results %&gt;%\n  tidyr::pivot_longer(var_biased:var_unbiased, names_to = \"estimator\", values_to = \"value\") %&gt;%\n  dplyr::mutate(estimator = dplyr::recode(estimator, var_n = \"Divide by n\", var_n1 = \"Divide by n-1\"))\n\noptions(repr.plot.width = 12, repr.plot.height = 8)\n\nggplot(results_long, aes(sample_size, value, color = estimator)) +\n  geom_line() +\n  geom_point() +\n  scale_color_npg() +\n  geom_hline(yintercept = true_var, linetype = \"dashed\") +\n  xlim(c(0,500)) +\n  labs(title = \"Convergence of Variance Estimators\", y = \"Estimated variance\", x = \"Sample size\")\n\n\nWarning message:\n\n“Removed 4 rows containing missing values or values outside the scale range\n\n(`geom_line()`).”\n\nWarning message:\n\n“Removed 4 rows containing missing values or values outside the scale range\n\n(`geom_point()`).”",
    "crumbs": [
      "Home",
      "Basics",
      "Lab 1: Probability & Descriptive Statistics"
    ]
  },
  {
    "objectID": "01_Lab-1.html#correlated-asset-returns",
    "href": "01_Lab-1.html#correlated-asset-returns",
    "title": "Lab 1: Probability & Descriptive Statistics",
    "section": "6 Correlated Asset Returns",
    "text": "6 Correlated Asset Returns\nWe will simulate repeated samples of two (correlated) asset return series and compare the sampling distributions of their sample means and the distribution of the portfolio mean (equal weights).\n\n# ---- Load Correlated Asset Returns Data ----\nX_demo &lt;- read_csv(\"https://raw.githubusercontent.com/my1396/FIN5005-Fall2025/refs/heads/main/data/correlated_asset_returns.csv\")\ncolnames(X_demo) &lt;- c(\"Asset_A\", \"Asset_B\")  # Rename columns for clarity\nX_demo %&gt;%\n    as.data.table() %&gt;%\n    .[, lapply(.SD, function(x) if (is.numeric(x)) round(x, 4) else x)] %&gt;%\n    print(digits = 4)\n\n      Asset_A Asset_B\n        &lt;num&gt;   &lt;num&gt;\n   1:  0.0129  0.0351\n   2:  0.0012  0.0094\n   3:  0.0160  0.0130\n   4:  0.0259  0.0550\n   5:  0.0079  0.0147\n  ---                \n1996:  0.0080  0.0177\n1997:  0.0061 -0.0001\n1998: -0.0299 -0.0425\n1999: -0.0202 -0.0357\n2000:  0.0603  0.0746\n\n\n\nemp_cor &lt;- cor(X_demo)\nemp_cor\n\n\nA matrix: 2 × 2 of type dbl\n\n\n\nAsset_A\nAsset_B\n\n\n\n\nAsset_A\n1.0000000\n0.9040318\n\n\nAsset_B\n0.9040318\n1.0000000\n\n\n\n\n\nPlot the scatter plot of the two asset returns to visualize their relationship.\n\noptions(repr.plot.width = 8, repr.plot.height = 7)\nggplot(X_demo, aes(Asset_A, Asset_B)) +\n  geom_point(alpha = 0.35, color = '#1b7837') \n\n\n\n\n\n\n\n\n\n💡Q: What is the correlation between the two asset returns? How do their prices move together?\nA: [Type your answer here]\n\n\n Exercise\nFollowing the steps below, you will compute the mean and standard deviation of the two asset returns, construct a portfolio, and analyze its properties.\nStep 1: Calculate the standard deviation of Asset A and B returns, respectively.\nStep 2: Construct a equally weighted portfolio of the two assets. Calculate the portfolio returns. \\[\nE[r_p] = \\frac{1}{2} (E[r_A] + E[r_B])\n\\]\nStep 3: Calculate the standard deviation of the portfolio returns, \\(\\text{sd}(r_p)\\).\nStep 4: Calculate the average of the volatility of Asset A and B (using results in Step 1). Compare it with the volatility of the portfolio returns.\n\n\n6.1 Reflection\nHow does correlation between assets influence the portfolio mean’s variability? Write 3–4 sentences interpreting for a diversified vs concentrated investment decision.",
    "crumbs": [
      "Home",
      "Basics",
      "Lab 1: Probability & Descriptive Statistics"
    ]
  }
]