[
  {
    "objectID": "04_Theoretical_Framework_LR.html",
    "href": "04_Theoretical_Framework_LR.html",
    "title": "Theoretical Framework of Linear Regression",
    "section": "",
    "text": "In this section, we will discuss the theoretical framework of linear regression, including the assumptions underlying the linear regression model and essential hypothesis tests.\nQ: Why the assumptions matter?\nA: In short, violation of the assumptions may lead to biased or inefficient estimates, incorrect inferences, and unreliable predictions.",
    "crumbs": [
      "Home",
      "Basics",
      "Theoretical Framework of Linear Regression"
    ]
  },
  {
    "objectID": "04_Theoretical_Framework_LR.html#assumptions-of-linear-regression",
    "href": "04_Theoretical_Framework_LR.html#assumptions-of-linear-regression",
    "title": "Theoretical Framework of Linear Regression",
    "section": "1 Assumptions of Linear Regression",
    "text": "1 Assumptions of Linear Regression\nQ: What are the assumptions of linear regression?\nA: The Gauss-Markov Assumptions ensure that the Ordinary Least Squares (OLS) estimator is the Best Linear Unbiased Estimator (BLUE).\n\nLinearity: The relationship between the independent variable(s) and the dependent variable is linear in parameters.\nNo perfect multicollinearity: The independent variables are NOT perfectly correlated.\nError has zero conditional mean\nError is homoskedastic\n\nHomoscedasticity: The variance of the error term is constant across all levels of the independent variable(s).\n\nIn time series settings (we won’t cover in this course), we also assume:\n\nNo autocorrelation: The error terms are not correlated with each other.\n\nResiduals are normally distributed\nThis assumption is useful for conducting hypothesis tests and constructing confidence intervals.",
    "crumbs": [
      "Home",
      "Basics",
      "Theoretical Framework of Linear Regression"
    ]
  },
  {
    "objectID": "04_Theoretical_Framework_LR.html#regression-framework",
    "href": "04_Theoretical_Framework_LR.html#regression-framework",
    "title": "Theoretical Framework of Linear Regression",
    "section": "2 Regression Framework",
    "text": "2 Regression Framework\nMultiple linear regression models is defined as\n\\[\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + ... + \\beta_k X_{ik} + \\varepsilon_i\n\\]\nwhere \\(i\\) indexes the observations, \\(Y_i\\) is the dependent variable, and \\(X_{ij}\\) represents the \\(j\\)-th independent variable for observation \\(i\\), with \\(j = 1, \\ldots, k\\) and \\(k\\) denoting the total number of independent variables. The coefficients \\(\\beta_j\\) are to be estimated, and \\(\\varepsilon_i\\) is the error term.\nOLS estimator \\(\\hat{\\beta}^{OLS}\\) minimizes the sum of squared residuals (SSR):\n\\[\n\\hat{\\beta}^{OLS} = \\arg\\min_{\\beta} \\sum_{i=1}^{n} e_i^2\n\\]\nwhere \\(e_i = Y_i - \\hat{Y}_i\\) is the residual for observation \\(i\\).\n\\(\\hat{Y}_i\\) is the predicted value of \\(Y_i\\) given the estimated coefficients.\n\\[\n\\hat{\\beta}^{OLS} = \\arg\\min_{\\beta} \\sum_{i=1}^{n} (Y_i - \\beta_0 - \\sum_{j=1}^{k} \\beta_j X_{ij})^2\n\\]\nProperties of OLS estimator:\n\nUnbiasedness: \\(E[\\hat{\\beta}^{OLS}] = \\beta\\) (if Gauss-Markov assumptions hold)\nEfficiency: Among all linear unbiased estimators, OLS has the smallest variance\nConsistency: As sample size \\(n \\to \\infty\\), \\(\\hat{\\beta}^{OLS} \\to \\beta\\) in probability",
    "crumbs": [
      "Home",
      "Basics",
      "Theoretical Framework of Linear Regression"
    ]
  },
  {
    "objectID": "04_Theoretical_Framework_LR.html#measure-of-fit",
    "href": "04_Theoretical_Framework_LR.html#measure-of-fit",
    "title": "Theoretical Framework of Linear Regression",
    "section": "3 Measure of Fit",
    "text": "3 Measure of Fit\nWe often use \\(R^2\\) to measure the goodness of fit of a regression model.\n\nIt represents the proportion of variance in the dependent variable that is predictable by the regression model.\n\n\\(R^2\\) is defined as:\n\\[\nR^2 = 1 - \\frac{SSR}{SST} = \\frac{SST - SSR}{SST} = \\frac{SSE}{SST}\n\\]\nwhere\n\n\\(SSR = \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2 = \\sum_{i=1}^{n} e_i^2\\) is the sum of squared residuals (unexplained variation)\n\\(SST = \\sum_{i=1}^{n} (Y_i - \\bar{Y})^2\\) is the total sum of squares (total variation)\n\\(SSE = \\sum_{i=1}^{n} (\\hat{Y}_i - \\bar{Y})^2\\) is the explained sum of squares (explained variation)\n\\(\\bar{Y}= \\frac{1}{n}\\sum_{i=1}^n Y_i\\) is the mean of the dependent variable\n\nNote that adding variables always increases \\(R^2\\), even if the new variables are not statistically significant. To address this, we use the adjusted \\(R^2\\).\nIn a regression model with multiple explanatory variables, we often use adjusted \\(R^2\\) that adjusts the number of explanatory variables.",
    "crumbs": [
      "Home",
      "Basics",
      "Theoretical Framework of Linear Regression"
    ]
  },
  {
    "objectID": "04_Theoretical_Framework_LR.html#hypothesis-testing-in-regression",
    "href": "04_Theoretical_Framework_LR.html#hypothesis-testing-in-regression",
    "title": "Theoretical Framework of Linear Regression",
    "section": "4 Hypothesis Testing in Regression",
    "text": "4 Hypothesis Testing in Regression\nWe have been using t-statistics and p-values to for testing hypotheses such as whether a correlation coefficient is significantly different from zero or if a regression coefficient is significantly different from zero.\nIn the following section, we will introduce the procedure of hypothesis testing formally.\n\n\n\n\n\n\nStudy objectives for hypothesis testing\n\n\n\nThe goal is be able to conduct hypothesis test manually given necessary information.\nFor example, in case of testing if a regression coefficient is significantly different from zero, you should be able to\n\n\nEstablish null and alternative hypotheses.\nCalculate the t-statistic given the coefficient estimate, standard error.\nFind the critical value of your test statistic given the significance level, 5% is commonly used.\nYou need to be familiar with the distribution table to find the critical value. The tables can be found in the formula sheet on Canvas under the “Past Exam” module.\nDecision rule.\nCompare the t-stat with the critical value to make decision on rejecting or not rejecting the null hypothesis.\nInterpret the result in context. What it means in reality?\n\n\n\n\n\n4.1 t-test for Single Coefficient\nUsing the simple linear regression model of California School Test Scores in the previous session as an example, test whether expenditure has a significant effect on test scores at the 5% significance level.\nCall:\nlm(formula = TestScore ~ expenditure, data = cas)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-50.146 -14.206   0.689  13.513  50.127 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 6.236e+02  7.720e+00  80.783  &lt; 2e-16 ***\nexpenditure 5.749e-03  1.443e-03   3.984 7.99e-05 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 18.72 on 418 degrees of freedom\nMultiple R-squared:  0.03659,   Adjusted R-squared:  0.03428 \nF-statistic: 15.87 on 1 and 418 DF,  p-value: 7.989e-05\nHere is the hypothesis test step by step for the expenditure coefficient in the simple linear regression of California school test scores.\n\n\nHypotheses\n\n\\(H_0:\\ \\beta_{\\text{exp}} = 0\\) (expenditure has no effect on test scores)\n\\(H_1:\\ \\beta_{\\text{exp}} \\neq 0\\) (two-sided)\n\nTest statistic\nEstimate \\(= 0.005749\\), standard error \\(= 0.001443\\).\n\\[\nt=\\frac{\\hat\\beta}{\\text{SE}}=\\frac{0.005749}{0.001443}=3.984\n\\]\nThe degrees of freedom (df) \\(= n - k - 1,\\) where \\(n=420\\) (number of observations) and \\(k=1\\) (number of predictors), so \\(df=420-1-1=418\\).\nCritical value at 5%\nTwo-sided \\(t_{0.975,418}\\approx 1.96\\).\n\n\n\n\n\n\nFinding critical value\n\n\n\nNotice that when df is large, the critical value approaches that of the standard normal distribution, \\(z_{0.975}=1.96\\).\nThe rule-of-thumb is that when \\(df &gt; 30\\), you can use the standard normal critical values.\n\n\nDecision rule\n\nReject \\(H_0\\) if \\(|t|&gt;1.96\\).\nFail to reject \\(H_0\\) if \\(|t|\\leq 1.96\\).\n\nSince \\(3.984&gt;1.97\\), reject \\(H_0\\).\nInterpretation in context\nExpenditure has a statistically significant positive association with test scores at the 5% level. The point estimate implies that a \\$1,000 increase in per-student expenditure is associated with about \\(0.005749\\times 1000=5.75\\) points higher test scores.\n\n\n\n\n4.2 Confidence Interval for Single Coefficient\nA 95 percent confidence interval for the slope is given by\n\\[\n\\left(\\hat{\\beta}-C_{\\alpha/2}\\times SE(\\hat{\\beta}),\\; \\hat{\\beta}+C_{\\alpha/2}\\times SE(\\hat{\\beta})\\right)\n\\]\nwhere \\(C_{\\alpha/2}\\) is the critical value for a two-sided test at significance level \\(\\alpha\\).\nFor the expenditure coefficient in the simple linear regression of California school test scores, \\(\\hat{\\beta}=0.005749\\), \\(SE(\\hat{\\beta})=0.001443\\), and \\(C_{0.025}=1.96\\).\nA 95% confidence interval is therefore\n\\[\n0.005749 \\pm 1.97\\times 0.001443 \\approx [0.0029,\\ 0.0086],\n\\]\nwhich corresponds to roughly 2.9 to 8.6 points per $1,000.\nThe effect is statistically significant but modest in size, and the low \\(R^2\\) indicates that many other factors also influence test scores.\n\n\n4.3 F-test for Significance of the Overall Regression\nF-test is also known as the one-way analysis of variance (ANOVA) test. It tests whether the regression model as a whole is statistically significant.\n[To be finished…]\n\n\n4.4 Homoskedasticity\nHomoskedasticity assumption requires constant variance of the error term across all levels of the independent variable(s).\n\\[\n\\var(\\varepsilon_i | X) = \\sigma^2\n\\]\nWhen the assumption is violated, we have heteroskedasticity:\n\\[\n\\var(\\varepsilon_i | X) = \\sigma^2_i\n\\]\nthat is, the variance of the error term varies with the level of the independent variable(s).\nIf the error has heteroskedasticity, the standard error under homoskedasticity assumption will be underestimated, leading to invalid t statistics, affecting hypothesis tests and confidence intervals.\nA quick fix is to use heteroskedasticity-robust standard errors, which are valid even when the homoskedasticity assumption is violated.\n\nlibrary(tidyverse)\nlibrary(sandwich) # for robust standard errors\nlibrary(lmtest)   # for coeftest()\nf_name &lt;- \"https://raw.githubusercontent.com/my1396/FIN5005-Fall2025/refs/heads/main/data/CASchools_test_score.csv\"\ncas &lt;- read_csv(f_name,\n    col_types = cols(\n        county = col_factor(), # read as factor\n        grades = col_factor()\n    )\n)\ncas &lt;- cas %&gt;%\n    mutate(TestScore = (read + math) / 2)\n\n\nmodel &lt;- lm(TestScore ~ expenditure, data = cas)\n# heteroskedasticity-robust standard errors\ncoeftest(model, vcov = vcovHC(model, type = \"HC1\"))\n\n\nt test of coefficients:\n\n              Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 6.2362e+02 8.4664e+00  73.657 &lt; 2.2e-16 ***\nexpenditure 5.7488e-03 1.6203e-03   3.548 0.0004322 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nUnder the heteroskedasticity-robust standard errors:\n\nno effects on the coefficient estimates\nthe Std. Error is slightly larger (0.001620 vs. 0.001443). The coefficient remains statistically significant at the 5% level.\n\nInterpretation\nThe heteroskedasticity-robust results suggest that the OLS standard errors may have been too optimistic (underestimated variability). While the statistical significance of the expenditure effect is slightly reduced, it remains highly significant, indicating a robust positive relationship between expenditure and test scores.",
    "crumbs": [
      "Home",
      "Basics",
      "Theoretical Framework of Linear Regression"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Business Analytics (Fall 2025)",
    "section": "",
    "text": "What Business Analytics (BA) is about?\nThe aim of this course is to provide students advanced knowledge, skills and competencies they can use to make data driven decisions for organizations.\nData-driven decision making\n\nDescribe the data: what happened\nUsing introductory statistics to identify patterns, trends, and insights embedded in historical data. E.g., summary statistics.\nPredictive: what will happen\nUsing statistical models on historical data and forecast future outcomes. E.g., regression, time series.\nPrescriptive: what should we do\nUsing optimization, simulation, and decision analysis techniques to suggest the best course of action based on data, predictions, and constraints.\nAutonomous: automated decision-making using Machine Learning (ML) and Artificial Intelligence (AI) techniques.\n\nOutline of models and applications we will cover in this course:1\n\n\n\n\n\n\n\n\nCategory\nTopics\nApplications in Business\n\n\n\n\nDescriptive\nData Visualization, Descriptive Analysis\nAsset return → Finance\n\n\nPredictive\nLinear regression\nAsset return prediction → FinanceDemand prediction → Economics\n\n\n\nHypothesis testing\nEffectiveness of advertisements → Marketing\n\n\n\nClassification, logistic regression\nAsset return increasing/decreasing prediction → FinanceEmployee satisfaction → Operation\n\n\nPrescriptive\nOptimization models\nPricing optimization → OperationSensitivity analysis, Scenario analysis → Accounting\n\n\n\n[1] This is a preliminary plan. Topics and applications are subject to change as the course moves forward.\n\nSoftware: R programming\nR is an open-source programming language widely used for statistical computing and data analysis. It provides a rich ecosystem of packages and libraries for data manipulation, visualization, and modeling.\nWhy open-source stands out in the competition?\nAI is the game changer here. AI significantly improves coding efficiency and productivity. You don’t need to memorize every function or syntax anymore. The current role for humans is to communicate your needs to AI, and AI will generate the code for you.\n\nOpen-source software integrates cutting-edge AI tools, faster and more efficiently than paid software.\nBy contrast, paid software is often slower to implement new features due to development cycles and licensing restrictions.\n\n Demonstration in VS Code. \nBy typing # create a function taking the n-th power of a number, AI will generate the code for you.\nHere is how to use GitHub Copilot in RStudio: RStudio User Guide: Tools, GitHub Copilot."
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "Business Analytics (Fall 2025)",
    "section": "",
    "text": "What Business Analytics (BA) is about?\nThe aim of this course is to provide students advanced knowledge, skills and competencies they can use to make data driven decisions for organizations.\nData-driven decision making\n\nDescribe the data: what happened\nUsing introductory statistics to identify patterns, trends, and insights embedded in historical data. E.g., summary statistics.\nPredictive: what will happen\nUsing statistical models on historical data and forecast future outcomes. E.g., regression, time series.\nPrescriptive: what should we do\nUsing optimization, simulation, and decision analysis techniques to suggest the best course of action based on data, predictions, and constraints.\nAutonomous: automated decision-making using Machine Learning (ML) and Artificial Intelligence (AI) techniques.\n\nOutline of models and applications we will cover in this course:1\n\n\n\n\n\n\n\n\nCategory\nTopics\nApplications in Business\n\n\n\n\nDescriptive\nData Visualization, Descriptive Analysis\nAsset return → Finance\n\n\nPredictive\nLinear regression\nAsset return prediction → FinanceDemand prediction → Economics\n\n\n\nHypothesis testing\nEffectiveness of advertisements → Marketing\n\n\n\nClassification, logistic regression\nAsset return increasing/decreasing prediction → FinanceEmployee satisfaction → Operation\n\n\nPrescriptive\nOptimization models\nPricing optimization → OperationSensitivity analysis, Scenario analysis → Accounting\n\n\n\n[1] This is a preliminary plan. Topics and applications are subject to change as the course moves forward.\n\nSoftware: R programming\nR is an open-source programming language widely used for statistical computing and data analysis. It provides a rich ecosystem of packages and libraries for data manipulation, visualization, and modeling.\nWhy open-source stands out in the competition?\nAI is the game changer here. AI significantly improves coding efficiency and productivity. You don’t need to memorize every function or syntax anymore. The current role for humans is to communicate your needs to AI, and AI will generate the code for you.\n\nOpen-source software integrates cutting-edge AI tools, faster and more efficiently than paid software.\nBy contrast, paid software is often slower to implement new features due to development cycles and licensing restrictions.\n\n Demonstration in VS Code. \nBy typing # create a function taking the n-th power of a number, AI will generate the code for you.\nHere is how to use GitHub Copilot in RStudio: RStudio User Guide: Tools, GitHub Copilot."
  },
  {
    "objectID": "index.html#course-materials",
    "href": "index.html#course-materials",
    "title": "Business Analytics (Fall 2025)",
    "section": "2 Course Materials",
    "text": "2 Course Materials\nThe course materials will be self-contained.\n\nLecture notes: lecture notes will be provided in the course website.\nR code: R code will be provided in form of Lab Jupyter Notebooks.\nYou may copy and paste the code into RStudio to run it.\n\nIf you want to learn in-depth, you can refer to the following textbooks and resources.\nTextbooks\n\nEvans, J.R. (2021) Business analytics: methods, models, and decisions. Third edition. Harlow: Pearson.\nStatistics primer. Basic introduction to business analytics at the undergraduate level.\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An Introduction to Statistical Learning with Applications in R. 2nd edition. Springer. Online version\nMain textbook for the course. It covers the fundamental concepts and methods of statistical learning, with practical applications in R.\nHuntsinger, R. (2025). Business Analytics: Methods and Cases for Data-Driven Decisions. Cambridge: Cambridge University Press. eTextbook available through Cambridge University Press.\nAdvanced methods and case studies for data-driven decision-making. We might use case studies from this book in the course."
  },
  {
    "objectID": "index.html#course-evaluation",
    "href": "index.html#course-evaluation",
    "title": "Business Analytics (Fall 2025)",
    "section": "3 Course Evaluation",
    "text": "3 Course Evaluation\nCompud Assessment\n\nOppgave (home assignment): 40%\nGroup work (1-3 students) is possible; including a case study with data analysis and visualization; a report will be submitted;\nDate: week 42 (preliminary date: 14.10.2025)\nThe assignment will be released in Inspera, and you will have one week to complete it.\nMore details will be provided as the course progresses.\nEksamen (school exam): 60%\nDigitally in Inspera;\nDate: 17.11.2025"
  },
  {
    "objectID": "index.html#study-objectives",
    "href": "index.html#study-objectives",
    "title": "Business Analytics (Fall 2025)",
    "section": "4 Study Objectives",
    "text": "4 Study Objectives\n\nKnowledge:\n\nFamiliarity with statistical methods and models used in business analytics.\nFocus on descriptive analytics and predictive analytics. Prescriptive analytics will be introduced if time allows.\nKnowledge of data visualization techniques and tools.\n\nSkills:\n\nAbility to analyze and interpret data using statistical methods.\nProficiency in using R for data analysis and visualization.\nCompetence in applying business analytics techniques to real-world problems.\n\nCompetencies:\n\nDevelop critical thinking skills to evaluate data-driven decisions.\nAbility to communicate findings effectively through reports and presentations."
  },
  {
    "objectID": "index.html#how-to-reach-me",
    "href": "index.html#how-to-reach-me",
    "title": "Business Analytics (Fall 2025)",
    "section": "5 How to Reach Me",
    "text": "5 How to Reach Me\n\n\n\nInstructor:\n\n\nMenghan Yuan\n\n\n\n\nEmail:\n\n\nmenghan.yuan@nord.no\n\n\n\n\nOffice Hours:\n\n\nBy appointment\n\n\n\n\nOffice:\n\n\nHovedbygning A257"
  },
  {
    "objectID": "03_Linear_Regression.html",
    "href": "03_Linear_Regression.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "In this session, we will use the same CASchools dataset from the previous session and implement a simple linear regression model to explore a question of interest:",
    "crumbs": [
      "Home",
      "Basics",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "03_Linear_Regression.html#data-preparation",
    "href": "03_Linear_Regression.html#data-preparation",
    "title": "Simple Linear Regression",
    "section": "1 Data Preparation",
    "text": "1 Data Preparation\n\n# load packages and dataset\npkgs &lt;- c(\"tidyverse\", \"moments\", \"data.table\", \"ggsci\", \"stargazer\")\nmissing &lt;- setdiff(pkgs, rownames(installed.packages()))\nif (length(missing) &gt; 0) install.packages(missing)\ninvisible(lapply(pkgs, function(pkg) suppressPackageStartupMessages(library(pkg, character.only = TRUE))))\nf_name &lt;- \"https://raw.githubusercontent.com/my1396/FIN5005-Fall2025/refs/heads/main/data/CASchools_test_score.csv\"\ncas &lt;- read_csv(f_name,\n    col_types = cols(\n        county = col_factor(), # read as factor\n        grades = col_factor()\n    )\n)\ncas %&gt;% as.data.table()\n\n     district                          school      county grades students\n        &lt;num&gt;                          &lt;char&gt;      &lt;fctr&gt; &lt;fctr&gt;    &lt;num&gt;\n  1:    75119              Sunol Glen Unified     Alameda  KK-08      195\n  2:    61499            Manzanita Elementary       Butte  KK-08      240\n  3:    61549     Thermalito Union Elementary       Butte  KK-08     1550\n  4:    61457 Golden Feather Union Elementary       Butte  KK-08      243\n  5:    61523        Palermo Union Elementary       Butte  KK-08     1335\n ---                                                                     \n416:    68957          Las Lomitas Elementary   San Mateo  KK-08      984\n417:    69518            Los Altos Elementary Santa Clara  KK-08     3724\n418:    72611          Somis Union Elementary     Ventura  KK-08      441\n419:    72744               Plumas Elementary        Yuba  KK-08      101\n420:    72751            Wheatland Elementary        Yuba  KK-08     1778\n     teachers calworks   lunch computer expenditure    income   english  read\n        &lt;num&gt;    &lt;num&gt;   &lt;num&gt;    &lt;num&gt;       &lt;num&gt;     &lt;num&gt;     &lt;num&gt; &lt;num&gt;\n  1:    10.90   0.5102  2.0408       67    6384.911 22.690001  0.000000 691.6\n  2:    11.15  15.4167 47.9167      101    5099.381  9.824000  4.583333 660.5\n  3:    82.90  55.0323 76.3226      169    5501.955  8.978000 30.000002 636.3\n  4:    14.00  36.4754 77.0492       85    7101.831  8.978000  0.000000 651.9\n  5:    71.50  33.1086 78.4270      171    5235.988  9.080333 13.857677 641.8\n ---                                                                         \n416:    59.73   0.1016  3.5569      195    7290.339 28.716999  5.995935 700.9\n417:   208.48   1.0741  1.5038      721    5741.463 41.734108  4.726101 704.0\n418:    20.15   3.5635 37.1938       45    4402.832 23.733000 24.263039 648.3\n419:     5.00  11.8812 59.4059       14    4776.336  9.952000  2.970297 667.9\n420:    93.40   6.9235 47.5712      313    5993.393 12.502000  5.005624 660.5\n      math\n     &lt;num&gt;\n  1: 690.0\n  2: 661.9\n  3: 650.9\n  4: 643.5\n  5: 639.9\n ---      \n416: 707.7\n417: 709.5\n418: 641.7\n419: 676.5\n420: 651.0\n\n\nVariable of interest: Test scores\n\nread and math are average reading and math test scores for each district\nIn the last session, we show that these two variables are highly correlated (r=0.92)\nWe construct an average test score as the average of the test score for reading and the score of the math test.\n\n\n# compute TestScore and append it to CASchools\ncas &lt;- cas %&gt;%\n  mutate(TestScore = (read + math) / 2)",
    "crumbs": [
      "Home",
      "Basics",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "03_Linear_Regression.html#descriptive-statistics",
    "href": "03_Linear_Regression.html#descriptive-statistics",
    "title": "Simple Linear Regression",
    "section": "2 Descriptive Statistics",
    "text": "2 Descriptive Statistics\nRefer to the previous session for detailed summary statistics and visualizations.\nA quick preview\nv &lt;- setdiff(names(cas), c(\"district\", \"school\", \"county\", \"grades\"))\nstargazer(as.data.frame(cas[v]), type=\"html\", digits=2)\n\n\n\n\n\n\n\nStatistic\n\n\nN\n\n\nMean\n\n\nSt. Dev.\n\n\nMin\n\n\nMax\n\n\n\n\n\n\n\n\nstudents\n\n\n420\n\n\n2,628.79\n\n\n3,913.10\n\n\n81\n\n\n27,176\n\n\n\n\nteachers\n\n\n420\n\n\n129.07\n\n\n187.91\n\n\n4.85\n\n\n1,429.00\n\n\n\n\ncalworks\n\n\n420\n\n\n13.25\n\n\n11.45\n\n\n0.00\n\n\n78.99\n\n\n\n\nlunch\n\n\n420\n\n\n44.71\n\n\n27.12\n\n\n0.00\n\n\n100.00\n\n\n\n\ncomputer\n\n\n420\n\n\n303.38\n\n\n441.34\n\n\n0\n\n\n3,324\n\n\n\n\nexpenditure\n\n\n420\n\n\n5,312.41\n\n\n633.94\n\n\n3,926.07\n\n\n7,711.51\n\n\n\n\nincome\n\n\n420\n\n\n15.32\n\n\n7.23\n\n\n5.34\n\n\n55.33\n\n\n\n\nenglish\n\n\n420\n\n\n15.77\n\n\n18.29\n\n\n0.00\n\n\n85.54\n\n\n\n\nread\n\n\n420\n\n\n654.97\n\n\n20.11\n\n\n604.50\n\n\n704.00\n\n\n\n\nmath\n\n\n420\n\n\n653.34\n\n\n18.75\n\n\n605.40\n\n\n709.50\n\n\n\n\nTestScore\n\n\n420\n\n\n654.16\n\n\n19.05\n\n\n605.55\n\n\n706.75",
    "crumbs": [
      "Home",
      "Basics",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "03_Linear_Regression.html#scatter-plot",
    "href": "03_Linear_Regression.html#scatter-plot",
    "title": "Simple Linear Regression",
    "section": "3 Scatter Plot",
    "text": "3 Scatter Plot\nHypothesis: Higher spending per student leads to better test scores.\n\nggplot(cas, aes(x = expenditure, y = TestScore)) +\n    geom_point(alpha = 0.6, color = \"#1976d2\") +\n    geom_smooth(method = \"lm\", color = \"#d32f2f\", se = FALSE) +\n    labs(\n        title = \"Scatter Plot of Test Scores vs. Expenditure per Student\",\n        x = \"Expenditure per Student (USD)\",\n        y = \"Average Test Score\"\n    ) +\n    theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\nThere seems to be a positive relationship between expenditure per student and average test scores. Also note that the points are dispersed, indicating that other factors may also influence test scores. We will further explore this using multiple regression in future sessions.\nFor now, we will focus on a simple linear regression model with only one independent variable: expenditure per student.\nLet’s calculate the correlation coefficient between expenditure and test scores.\n\nwith(cas, cor.test(expenditure, TestScore))\n\n\n    Pearson's product-moment correlation\n\ndata:  expenditure and TestScore\nt = 3.9841, df = 418, p-value = 7.989e-05\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.09736861 0.28180139\nsample estimates:\n      cor \n0.1912728 \n\n\n\nWhat does the correlation coefficient tell us? Is it statistically significant? Justify your answer.\n\n\nSolution\n\n\n\nThe reported correlation coefficient is 0.19 between expenditure and TestScore.\nThis is a positive but weak correlation: higher expenditures are associated with higher test scores, but the relationship is not very strong.\nThe p-value = 7.989e-05, which is far below common significance levels (0.05, 0.01, or even 0.001).\nThis means we reject the null hypothesis that the true correlation is zero. The result is statistically significant.",
    "crumbs": [
      "Home",
      "Basics",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "03_Linear_Regression.html#simple-linear-regression",
    "href": "03_Linear_Regression.html#simple-linear-regression",
    "title": "Simple Linear Regression",
    "section": "4 Simple Linear Regression",
    "text": "4 Simple Linear Regression\n\n4.1 Model Specification\nWe will fit a simple linear regression model to examine the relationship between expenditure per student and average test scores.\n\\[\n\\text{TestScore}_i = \\beta_0 + \\beta_1 \\times \\text{expenditure}_i + \\varepsilon_i\n\\tag{1}\\]\nEquation 1 is the linear regression model with a single independent variable.\nWhere:\n\n\n\n\n\n\nKey Components of the Linear Regression Model\n\n\n\n\n\\(\\text{TestScore}_i\\) in the Left Hand Side (LHS) is the dependent variable (response variable). \\(i\\) indexes different school districts.\n\\(\\text{expenditure}_i\\) in the Right Hand Side (RHS) is the independent variable (explanatory variable or regressor).\n\\(\\beta_0\\) and \\(\\beta_1\\) are known as the parameters (coefficients) of the model.\n\n\\(\\beta_0\\) is the intercept, representing the expected value of TestScore when expenditure is zero.\n\\(\\beta_1\\) is the slope coefficient, representing the change in TestScore for a one dollar increase in expenditure.\n\n\\(\\varepsilon_i\\) is the error term, capturing all other factors affecting not included in the model, e.g, teacher quality, school facilities, parental involvement, etc.\n\n\n\nEquation 1 can be written more generally as:\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i\n\\]\n\n\\(\\beta_1\\) is the slope coefficient, which measures the change in the dependent variable \\(Y\\) associated with a one-unit increase in the independent variable \\(X.\\)\n\\(\\beta_0\\) is the intercept, which is the expected value of \\(Y\\) when \\(X=0;\\) it is the point at which the population regression line intersects the Y axis.\n\n📌 In some econometric applications, the intercept has a meaningful economic interpretation. In other applications, the intercept has no real-world meaning; for example, when \\(X\\) is the expenditure per student, strictly speaking the intercept is the expected value of test scores when there is no expenditure, which might be unrealistic.\nWhen the real-world meaning of the intercept is nonsensical, it is best to think of it simply as the coefficient that determines the level of the regression line.\n\n\n4.2 Estimating the Coefficients of the Linear Regression Model\nBased on the scatter plot and the correlation analysis, we expect a positive relationship between expenditure and test scores. We will use the Ordinary Least Squares (OLS) method to estimate the coefficients of the linear regression model.\nTo run this regression, we use the lm() function in R.\n\n# Fit the linear regression model\nmodel &lt;- lm(TestScore ~ expenditure, data = cas)\nsummary(model)\n\n\nCall:\nlm(formula = TestScore ~ expenditure, data = cas)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-50.146 -14.206   0.689  13.513  50.127 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 6.236e+02  7.720e+00  80.783  &lt; 2e-16 ***\nexpenditure 5.749e-03  1.443e-03   3.984 7.99e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.72 on 418 degrees of freedom\nMultiple R-squared:  0.03659,   Adjusted R-squared:  0.03428 \nF-statistic: 15.87 on 1 and 418 DF,  p-value: 7.989e-05\n\n\n\n\n4.3 Interpretation of the output\n\nA. Model Specification and Fitted Values\nThe regression estimates the effect of expenditure (per student, in USD) on test scores (combined reading and math).\nFrom Theory to Practice:\nWe started with the theoretical model: \\[\n\\text{TestScore}_i = \\beta_0 + \\beta_1 \\times \\text{expenditure}_i + \\varepsilon_i\n\\]\nUsing OLS estimation, we obtain the fitted model:\n\\[\n\\widehat{\\text{TestScore}} = 623.6 + 0.00575 \\times \\text{expenditure}\n\\]\nThe hat symbol (\\(\\widehat{\\text{TestScore}}\\)) indicates these are predicted values based on our sample data. Notice that we no longer have the error terms (\\(\\varepsilon_i\\)) because this equation gives us the predicted values from the regression line.\nUnderstanding Residuals:\nSince our model can’t perfectly predict every observation, we need to measure how far off our predictions are from reality:\n\\[\n\\text{Residual}_i = \\text{TestScore}_i - \\widehat{\\text{TestScore}}_i\n\\]\nResiduals represent the difference between actual and predicted values - essentially, they capture what our model couldn’t explain. In the original theoretical model, these correspond to the error terms (\\(\\varepsilon_i\\)) that we estimated.\n\n\n\nB. Coefficients\n\nIntercept (623.6): When expenditure = 0, the predicted test score is about 623.6. While not meaningful in practice (since expenditure cannot realistically be zero), it serves as the baseline of the regression line.\nExpenditure (0.00575): For every one-dollar increase in expenditure per student, the average test score is predicted to increase by 0.0057 points. Put differently: a $1,000 increase in expenditure is associated with a 5.75-point increase in test scores.\n\n\n\n\nC. Statistical Significance\nFor the slope coefficient (\\(\\beta_1\\)):\n\nThe t-value = 3.984 and p-value = 7.99e-05, which is well below 0.001.\nThis indicates that expenditure has a statistically significant positive effect on test scores.\n\n\n\n\nD. Model Fit\n\nR-squared = 0.0366 (about 3.7%).\n\nThis means expenditure explains only a small portion of the variation in test scores.\nMany other factors (teacher quality, socio-economic background, school resources, etc.) likely play a larger role.\n\nF-statistic = 15.87 with a p-value of 7.99e-05.\n\nThis tests the null hypothesis that all regression coefficients are equal to zero (no effect).\nThe low p-value indicates we reject the null hypothesis, confirming that the model has some explanatory power.\n\n\n\n\n\nE. Residuals\nThe investigation of residuals requires diagnostic test which checks the assumptions of linear regression (e.g., homoscedasticity, normality of errors).\nWe will cover them in future sessions.\nHere we provide a brief overview.\n\n\n\n\n\n\n\n\n\n\nThe first plot is “Residuals vs Fitted” plot.\nThe residuals appear reasonably balanced around zero, though the spread suggests substantial unexplained variation.\nThe spread of residuals seems to increase slightly with fitted values, indicating potential heteroscedasticity (non-constant variance of errors). This suggests that the assumption of homoscedasticity may be violated, which could affect the reliability of our coefficient estimates and standard errors.\nThe second plot is the normal Q-Q plot, which assesses whether the residuals are normally distributed.\n\nThe points generally follow the reference line, suggesting that the residuals are approximately normally distributed, although there shows thin tails.\nThis indicates that there are few extreme residuals. The model predictions don’t have large outliers.\n\n\nFrom the summary output:\n\nThe residual standard error = 18.72, which is the average size of prediction errors (in test score points).\n\n\n✅ Summary Interpretation:\nThe regression analysis shows a statistically significant positive relationship between school expenditure and test scores. On average, an additional \\$1,000 in per-student expenditure is associated with an increase of about 5.75 points in test scores. However, the explanatory power of the model is low (R² ≈ 3.7%), indicating that expenditure alone explains only a small fraction of the variation in test scores. This suggests that while spending matters, many other factors also influence student performance.\nTo gain a more comprehensive understanding of the factors affecting student achievement, future analyses should employ multiple regression models that incorporate additional explanatory variables such as student demographics, teacher qualifications, and school characteristics. We will cover multiple regression in subsequent sessions.",
    "crumbs": [
      "Home",
      "Basics",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "03_Linear_Regression.html#report-by-stargazer",
    "href": "03_Linear_Regression.html#report-by-stargazer",
    "title": "Simple Linear Regression",
    "section": "5 Report by Stargazer",
    "text": "5 Report by Stargazer\nstargazer produces well-formatted regression tables that can be easily included in reports or publications.\n\nstargazer(model, type = \"text\", digits = 2)\n\n\n===============================================\n                        Dependent variable:    \n                    ---------------------------\n                             TestScore         \n-----------------------------------------------\nexpenditure                   0.01***          \n                              (0.001)          \n                                               \nConstant                     623.62***         \n                              (7.72)           \n                                               \n-----------------------------------------------\nObservations                    420            \nR2                             0.04            \nAdjusted R2                    0.03            \nResidual Std. Error      18.72 (df = 418)      \nF Statistic           15.87*** (df = 1; 418)   \n===============================================\nNote:               *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\n\n\nReferences\n\n\nIntroduction to Econometrics with R",
    "crumbs": [
      "Home",
      "Basics",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "02_Data_Visualization.html",
    "href": "02_Data_Visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Study Objectives\n\nLoads and prepares the California school dataset (CASchools) for analysis.\n\nDemonstrates how to visualize data using scatter plots, line charts, histograms, box plots.\nExplores expenditure per student across different grade spans.\nInvestigates correlations between expenditure and other variables.\n\nQQ plots to assess normality and interpret skewness and kurtosis in the data.\n\nGiven a QQ plot, you should be able to identify how does the distribution compare to normal: positively skewed, negatively skewed, has fat tails, or thin tails.",
    "crumbs": [
      "Home",
      "Basics",
      "Data Visualization"
    ]
  },
  {
    "objectID": "02_Data_Visualization.html#descriptive-analysis-and-data-visualization-with-caschools",
    "href": "02_Data_Visualization.html#descriptive-analysis-and-data-visualization-with-caschools",
    "title": "Data Visualization",
    "section": "1 Descriptive Analysis and Data Visualization with CASchools",
    "text": "1 Descriptive Analysis and Data Visualization with CASchools\nData visualization is a powerful tool for understanding patterns, relationships, and distributions in datasets. In this session, we will explore the California Schools dataset using various visualization techniques including histograms, box plots, scatter plots, and Q-Q plots. These visualizations help us answer important questions about educational spending, student performance, and the relationships between various school characteristics.\n\n1.1 Dataset Overview\nThe following prints the first and last 5 rows of the dataset, together with data types of each column:\n\n# load packages and dataset\npkgs &lt;- c(\"tidyverse\", \"moments\", \"data.table\", \"ggsci\")\nmissing &lt;- setdiff(pkgs, rownames(installed.packages()))\nif (length(missing) &gt; 0) install.packages(missing)\ninvisible(lapply(pkgs, function(pkg) suppressPackageStartupMessages(library(pkg, character.only = TRUE))))\nf_name &lt;- \"https://raw.githubusercontent.com/my1396/FIN5005-Fall2025/refs/heads/main/data/CASchools_test_score.csv\"\ncas &lt;- read_csv(f_name, \n  col_types = cols(\n    county = col_factor(), # read as factor\n    grades = col_factor()\n  ))\n\nThe following prints the first and last 5 rows of the dataset, together with data types of each column:\n\ncas %&gt;% as.data.table()\n\n     district                          school      county grades students\n        &lt;num&gt;                          &lt;char&gt;      &lt;fctr&gt; &lt;fctr&gt;    &lt;num&gt;\n  1:    75119              Sunol Glen Unified     Alameda  KK-08      195\n  2:    61499            Manzanita Elementary       Butte  KK-08      240\n  3:    61549     Thermalito Union Elementary       Butte  KK-08     1550\n  4:    61457 Golden Feather Union Elementary       Butte  KK-08      243\n  5:    61523        Palermo Union Elementary       Butte  KK-08     1335\n ---                                                                     \n416:    68957          Las Lomitas Elementary   San Mateo  KK-08      984\n417:    69518            Los Altos Elementary Santa Clara  KK-08     3724\n418:    72611          Somis Union Elementary     Ventura  KK-08      441\n419:    72744               Plumas Elementary        Yuba  KK-08      101\n420:    72751            Wheatland Elementary        Yuba  KK-08     1778\n     teachers calworks   lunch computer expenditure    income   english  read\n        &lt;num&gt;    &lt;num&gt;   &lt;num&gt;    &lt;num&gt;       &lt;num&gt;     &lt;num&gt;     &lt;num&gt; &lt;num&gt;\n  1:    10.90   0.5102  2.0408       67    6384.911 22.690001  0.000000 691.6\n  2:    11.15  15.4167 47.9167      101    5099.381  9.824000  4.583333 660.5\n  3:    82.90  55.0323 76.3226      169    5501.955  8.978000 30.000002 636.3\n  4:    14.00  36.4754 77.0492       85    7101.831  8.978000  0.000000 651.9\n  5:    71.50  33.1086 78.4270      171    5235.988  9.080333 13.857677 641.8\n ---                                                                         \n416:    59.73   0.1016  3.5569      195    7290.339 28.716999  5.995935 700.9\n417:   208.48   1.0741  1.5038      721    5741.463 41.734108  4.726101 704.0\n418:    20.15   3.5635 37.1938       45    4402.832 23.733000 24.263039 648.3\n419:     5.00  11.8812 59.4059       14    4776.336  9.952000  2.970297 667.9\n420:    93.40   6.9235 47.5712      313    5993.393 12.502000  5.005624 660.5\n      math\n     &lt;num&gt;\n  1: 690.0\n  2: 661.9\n  3: 650.9\n  4: 643.5\n  5: 639.9\n ---      \n416: 707.7\n417: 709.5\n418: 641.7\n419: 676.5\n420: 651.0\n\n\nThe cas dataset contains information on 420 public school districts in California, specifically those that operate either kindergarten through 6th grade (K–6) or kindergarten through 8th grade (K–8) schools, with data collected for the years 1998 and 1999.\nThe dataset includes:\n\nStudent performance measures (average reading and math scores).\nSchool characteristics (such as enrollment, number of teachers, student–teacher ratio, number of computers, expenditure per student).\nStudent demographics (including income, percentage qualifying for reduced-price lunch, percentage of English learners, etc.).\n\ncas is a data frame containing 420 observations on 14 variables. Refer to the following table for the full definition.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ndistrict\nCharacter. District code. Unique ID.\n\n\nschool\nCharacter. School name.\n\n\ncounty\nFactor indicating county.\n\n\ngrades\nFactor indicating grade span covered by the district’s schools. It takes on two values: - KK-06: means the district’s schools serve kindergarten through 6th grade.- KK-08: means the district’s schools serve kindergarten through 6th grade.\n\n\nstudents\nTotal enrollment.\n\n\nteachers\nNumber of teachers.\n\n\ncalworks\nPercent qualifying for CalWorks (income assistance program).\n\n\nlunch\nPercent qualifying for reduced-price lunch.\n\n\ncomputer\nNumber of computers.\n\n\nexpenditure\nExpenditure per student.\n\n\nincome\nDistrict average income (in USD 1,000).\n\n\nenglish\nPercent of English learners (i.e., students for whom English is a second language).\n\n\nread\nAverage reading score.\n\n\nmath\nAverage math score.\n\n\n\nData source: Stock, J. H. and Watson, M. W. (2020). Introduction to Econometrics, 4th ed. Pearson.\n\n\n\n1.2 Summary Statistics\nThe following displays the summary for each variable in the dataset:\n\nsummary(cas)\n\n    district        school                  county      grades   \n Min.   :61382   Length:420         Sonoma     : 29   KK-08:359  \n 1st Qu.:64308   Class :character   Kern       : 27   KK-06: 61  \n Median :67760   Mode  :character   Los Angeles: 27              \n Mean   :67473                      Tulare     : 24              \n 3rd Qu.:70419                      San Diego  : 21              \n Max.   :75440                      Santa Clara: 20              \n                                    (Other)    :272              \n    students          teachers          calworks          lunch       \n Min.   :   81.0   Min.   :   4.85   Min.   : 0.000   Min.   :  0.00  \n 1st Qu.:  379.0   1st Qu.:  19.66   1st Qu.: 4.395   1st Qu.: 23.28  \n Median :  950.5   Median :  48.56   Median :10.520   Median : 41.75  \n Mean   : 2628.8   Mean   : 129.07   Mean   :13.246   Mean   : 44.71  \n 3rd Qu.: 3008.0   3rd Qu.: 146.35   3rd Qu.:18.981   3rd Qu.: 66.86  \n Max.   :27176.0   Max.   :1429.00   Max.   :78.994   Max.   :100.00  \n                                                                      \n    computer       expenditure       income          english      \n Min.   :   0.0   Min.   :3926   Min.   : 5.335   Min.   : 0.000  \n 1st Qu.:  46.0   1st Qu.:4906   1st Qu.:10.639   1st Qu.: 1.941  \n Median : 117.5   Median :5215   Median :13.728   Median : 8.778  \n Mean   : 303.4   Mean   :5312   Mean   :15.317   Mean   :15.768  \n 3rd Qu.: 375.2   3rd Qu.:5601   3rd Qu.:17.629   3rd Qu.:22.970  \n Max.   :3324.0   Max.   :7712   Max.   :55.328   Max.   :85.540  \n                                                                  \n      read            math      \n Min.   :604.5   Min.   :605.4  \n 1st Qu.:640.4   1st Qu.:639.4  \n Median :655.8   Median :652.4  \n Mean   :655.0   Mean   :653.3  \n 3rd Qu.:668.7   3rd Qu.:665.8  \n Max.   :704.0   Max.   :709.5  \n                                \n\n\nInterpreting the Summary Statistics\nThis summary provides key insights into our dataset:\n\nContinuous variables (students, teachers, expenditure, income, etc.) show their distribution through quartiles, mean, and range\nCategorical variables (county, grades) show frequency counts\nNotice the range of expenditure per student: from around \\$3,900 to over \\$7,700 - this substantial variation will be a key focus of our analysis\nTest scores (reading and math) also show considerable variation, suggesting potential relationships with school resources\n\n\n\n1.3 Expenditure Variation Analysis\nQ: To what extent does expenditure per student vary?\nUnderstanding the distribution of expenditure per student is crucial for education policy analysis. Let’s examine detailed descriptive statistics:\n\nquick_summary &lt;- function(x, full=FALSE) {\n    # Function to compute basic descriptive statistics\n    if (!full){\n        # Basic summary\n        data.frame(\n          n = length(x),\n          min = min(x),\n          mean = mean(x),\n          median = median(x),\n          max = max(x),\n          sd = sd(x),\n          skewness = moments::skewness(x),\n          kurtosis = moments::kurtosis(x),\n          row.names = NULL\n        )\n    } else {\n        # Full summary with quartiles and IQR\n        data.frame(\n          n = length(x),\n          min = min(x),\n          Q1 = quantile(x, 0.25),\n          mean = mean(x),\n          median = median(x),\n          Q3 = quantile(x, 0.75),\n          max = max(x),\n          IQR = IQR(x),\n          sd = sd(x),\n          skewness = moments::skewness(x),\n          kurtosis = moments::kurtosis(x),\n          row.names = NULL\n        )\n    }\n}\nquick_summary(cas$expenditure) %&gt;% round(2)\n\n    n     min    mean  median     max     sd skewness kurtosis\n1 420 3926.07 5312.41 5214.52 7711.51 633.94     1.07     4.88\n\n\nInterpreting the Results:\n\nRange: Expenditure varies from about \\$3,926 to \\$7,712 per student - a difference of nearly \\$4,000\nCentral Tendency: The mean (\\$5,312) is slightly higher than the median (\\$5,196), suggesting a slight right skew\nVariability: Standard deviation of \\$633 indicates moderate spread around the mean\nSkewness (1.07): Positive skewness confirms some districts spend considerably more than average\nKurtosis (4.88): Larger than 3 indicates a distribution with fat tails relative to the normal distribution.\n\n\n# Basic histogram\nggplot(cas, aes(x = expenditure)) +\n  geom_histogram(binwidth = 500, boundary = 0, fill = \"lightblue\", color = \"black\") +\n  labs(title = \"Histogram of Expenditure per Student\",\n       x = \"Expenditure per Student (USD)\",\n       y = \"Frequency\") +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\nWhat the Histogram Reveals:\n\nMost districts cluster around the \\$5,000-\\$5,500 range\nVariability is moderate\nThe distribution has a long right tail, indicating positive skewness. That suggests that expenditure per student is concentrated at lower values for most districts, while a smaller number of districts spend substantially more, pulling the mean above the median\nThere are few extreme positive outliers, indicating some districts do spend significantly more than average\n\n\n\n\n1.4 Group Comparison: K-6 vs K-8 Schools\nQ: Are there any differences in expenditure per student between K-6 and K-8 schools?\nThis comparison helps us understand whether grade span affects resource allocation. Let’s examine the statistics for each group:\n\ncas %&gt;%\n    group_by(grades) %&gt;%\n    group_map(~ {\n        s &lt;- quick_summary(.x$expenditure, full=TRUE)\n        s &lt;- cbind(\n            grades = .y$grades,    # add group info\n            s)\n        return(s)\n    }) %&gt;%\n    bind_rows()\n\n  grades   n      min       Q1     mean   median       Q3      max      IQR\n1  KK-08 359 3926.070 4881.139 5267.365 5195.919 5539.380 7711.507 658.2412\n2  KK-06  61 4715.446 5092.917 5577.493 5399.383 5990.794 7614.379 897.8770\n        sd skewness kurtosis\n1 618.6415 1.094514 5.284716\n2 662.8033 1.019435 3.309185\n\n\nKey Findings:\n\nK–6 schools (61 districts): Mean expenditure of \\$5,577, slightly higher than K–8 schools\nK–8 schools (359 districts): Mean expenditure of \\$5,267, with a larger sample size\nK–6 schools indicating higher variability, with a standard deviation of \\$662 compared to \\$618 for K-8 schools\nSkewness is positive for both groups, with K–8 schools showing slightly more skewness\nThe difference suggests that smaller grade span districts may have slightly higher per-pupil spending\n\n\nBox plot by grades span\n\nggplot(cas, aes(x = grades, y = expenditure, fill = grades)) +\n  geom_boxplot() +\n  scale_fill_npg() +\n  labs(x = \"Grade Span\",\n       y = \"Expenditure per Student (USD)\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nUnderstanding the Box Plot:\n1. Box and middle bar (median)\n\nThe box itself spans from the first quartile (Q1, 25th percentile) to the third quartile (Q3, 75th percentile).\nThis range is called the interquartile range (IQR = Q3 - Q1).\nThe middle bar inside the box represents the median (50th percentile).\n\nIf the median is closer to the bottom of the box, the lower half of the data is more concentrated.\nIf closer to the top, the upper half is more concentrated.\n\n\n2. Whiskers\n\nThe whiskers extend from the box to show the range of the data excluding outliers.\nA whisker extends to the largest/smallest value within 1.5 × IQR from the box:\n\nUpper whisker = largest value ≤ Q3 + 1.5 × IQR\nLower whisker = smallest value ≥ Q1 - 1.5 × IQR\n\n\n3. Outliers\n\nPoints beyond the whiskers are plotted individually as outliers.\nThese are values that are unusually high or low relative to the bulk of the data.\n\n\n\n\n\n\n\nSummary of boxplot\n\nBox: middle 50% of the data\nMiddle line: median\nWhiskers: typical range, excluding extreme values\nOutliers: extreme observations\n\n\n\n\n\nInterpreting the Box Plot:\n\nMedian lines: K–6 districts show a higher median expenditure than K-8 districts\nBox sizes: K–6 district show a larger interquartile ranges (IQR), indicating greater middle 50% spread\nOutliers: K–8 groups show more positive outliers (points beyond the whiskers), representing districts with unusually high expenditure\n\n\n\n\n\n1.5 Correlation Analysis\nQ: What predicts expenditure per student?\nUnderstanding which factors correlate with expenditure helps identify patterns in educational resource allocation. Let’s examine correlations:\nCorrelation coefficients in ascending order:\n\n# Get the set of numeric variables\nv &lt;- setdiff(\n    names(cas),\n    c(\"district\", \"school\", \"county\", \"grades\")\n)\ncorExp &lt;- cor(cas[\"expenditure\"], cas[setdiff(v, \"expenditure\")])\nt(corExp) %&gt;%\n    as.data.frame() %&gt;%\n    rownames_to_column(var = \"variable\") %&gt;%\n    rename(correlation = expenditure) %&gt;% \n    arrange(correlation)\n\n  variable correlation\n1 students -0.11228455\n2 teachers -0.09519483\n3  english -0.07139604\n4 computer -0.07131050\n5    lunch -0.06103871\n6 calworks  0.06788857\n7     math  0.15498949\n8     read  0.21792682\n9   income  0.31448448\n\n\nKey Correlation Insights:\n\nNo strong correlations (e.g., \\(&gt;|0.7|\\)) with expenditure\nPositive correlations: Income (0.31), math scores (0.22), and reading scores (0.15)\nModerate Negative correlations: Students(\\(-0.11\\)), indicating districts with a larger amount of students tend to spend less per student\nInterpretation:\n\nWealthier districts tend to spend more per student\nHigher spending correlates with better test scores\nDistricts with more students may face budget constraints leading to lower per-student expenditure\n\n\nPlot scatter plots for the top three correlated variables\n\ntop_vars &lt;- c(\"income\", \"math\", \"read\")\ncas_long &lt;- cas %&gt;%\n    select(expenditure, all_of(top_vars)) %&gt;%\n    pivot_longer(cols = all_of(top_vars), names_to = \"variable\", values_to = \"value\")\nggplot(cas_long, aes(x = value, y = expenditure)) +\n    geom_point(alpha = 0.6, color = \"#1976d2\") +\n    facet_wrap(~variable, scales = \"free_x\") +\n    labs(\n        title = \"Scatter Plots of Expenditure per Student vs. Top Correlated Variables\",\n        x = \"Value\",\n        y = \"Expenditure per Student (USD)\"\n    ) +\n    theme_minimal(base_size = 14)   \n\n\n\n\n\n\n\n\nWhat These Scatter Plots Reveal:\n\nIncome vs. Expenditure: Positive relationship - wealthier districts consistently spend more per student; the trend is relatively clear with some variability;\nMath Scores vs. Expenditure: Positive trend suggests higher spending correlates with better math performance; the points are more scattered, indicating other factors also play a role;\nReading Scores vs. Expenditure: Similar pattern to math scores, showing the relationship between resources and achievement\nData Distribution: Points are fairly scattered, indicating that while correlations exist, other factors also influence these relationships\n\n\n\n\n1.6 Academic Performance Relationship\nQ: what is the relationship between district level maths and reading scores?\nAcademic performance across different subjects often correlates, as they reflect overall educational quality and student preparation. Let’s examine this relationship:\n\nwith(cas, cor.test(math, read))\n\n\n    Pearson's product-moment correlation\n\ndata:  math and read\nt = 49.005, df = 418, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9073417 0.9359362\nsample estimates:\n      cor \n0.9229015 \n\n\n\nggplot(cas, aes(x = read, y = math)) +\n    geom_point(alpha = 0.6, color = \"#1976d2\") +\n    geom_smooth(method = \"lm\", color = \"#d32f2f\") +\n    labs(\n        title = \"Scatter Plot of Math vs. Reading Scores\",\n        x = \"Average Reading Score\",\n        y = \"Average Math Score\"\n    ) +\n    theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\nAnalysis of the Relationship:\n\nStrong positive correlation: Districts with higher reading scores typically have higher math scores\nLinear trend: The smooth line shows a clear linear relationship between the two subjects\nTight clustering: Most points cluster closely around the trend line, indicating a strong relationship\nEducational insights:\n\nThis suggests that factors affecting academic performance impact multiple subjects similarly\nDistricts that excel in one area tend to excel in others",
    "crumbs": [
      "Home",
      "Basics",
      "Data Visualization"
    ]
  },
  {
    "objectID": "02_Data_Visualization.html#q-q-plot",
    "href": "02_Data_Visualization.html#q-q-plot",
    "title": "Data Visualization",
    "section": "2 Q-Q Plot",
    "text": "2 Q-Q Plot\nThe Q-Q plot, or quantile-quantile plot, is a graphical tool to help us assess if a set of data plausibly came from some theoretical distribution such as a normal distribution.\nUnderstanding Q-Q Plots:\nA QQ plot is a scatter plot created by plotting two sets of quantiles against one another.\n\nIf both sets of quantiles came from the same distribution, we should see the points forming a line that’s roughly straight.\nFor example, if we run a statistical analysis that assumes our residuals are normally distributed, we can use a normal QQ plot to check that assumption.\nWhen the theoretical quantiles are from a normal distribution, the plot is called a normal QQ plot.\n\nWhy Q-Q Plots Matter:\n\nMany statistical tests assume normality; deviations from normality can affect the validity of statistical inferences\nQ-Q plots provide a visual method to assess these assumptions\nThey help identify the type of non-normality (skewness, heavy tails, etc.)\n\n\n2.1 QQ plot visualization tool\nhttps://xiongge.shinyapps.io/QQplots/\n\nAdjust Skewness and Kurtosis to see how the QQ plot changes.\nNote the website is not able to host many users at the same time, so if it does not load, please try again later.\n\n\n\n\n2.2 Skewness\n\nPositive skew\nPositively skewed (mean &gt; median) data have a J-shaped pattern in the Q-Q plot.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary statistics:\n\n\n      n   min mean median  max sd skewness kurtosis\n1 10000 -2.01 0.01  -0.16 5.48  1     0.82     3.44\n\n\n💡 Note that for a positive skewed distribution, the mean is larger than the median because the long right tail pulls the mean above the median.\nUnderstanding Positive Skewness:\n\nShape: Long tail extending to the right\nMean vs. Median: Mean &gt; Median (tail pulls the mean upward)\nQ-Q Plot Pattern: J-shaped curve, points above the line at both ends\nReal-world examples: Income distribution, housing prices\nStatistical implications: May indicate floor effects or bounded distributions\n\n\n\nNegative skew\nNegatively skewed (mean &lt; median) data have Q-Q plots that display an inverted J-shape.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary statistics:\n\n\n      n  min  mean median  max   sd skewness kurtosis\n1 10000 -5.9 -0.02   0.16 1.98 1.01     -0.9     3.69\n\n\n💡 Note that for a negatively skewed distribution, the mean is smaller than the median because the long left tail pulls the mean above the median.\nUnderstanding Negative Skewness:\n\nShape: Long tail extending to the left\nMean vs. Median: Mean &lt; Median (tail pulls the mean downward)\nQ-Q Plot Pattern: Inverted J-shaped curve, points below the line at both ends\nReal-world examples:\n\nTest scores (when most students perform well): Most students score high (80-100), with fewer low scores creating a left tail\nAge at retirement: Most people retire around 65-67, but some retire much earlier due to health or financial reasons\n\nStatistical implications: May indicate ceiling effects or bounded distributions\n\n\n\n\n2.3 Kurtosis\nKurtosis measures the “tailedness” of a distribution compared to a normal distribution. Understanding kurtosis helps identify distributions with unusually many or few extreme values.\n\nFat tails\nThis plot shows a t-distribution with \\(3\\) degrees of freedom, which has heavier tails than a normal distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary statistics:\n\n\n     n    min  mean median  max   sd skewness kurtosis\n1 1000 -14.61 -0.05  -0.03 9.14 1.61    -0.67    13.74\n\n\nUnderstanding Fat Tails (High Kurtosis):\n\nQ-Q Plot Pattern: Reverse S-shaped curve - points below the line at low quantiles, above at high quantiles.\nKurtosis &gt; 3: Indicates heavier tails than normal distribution\nPractical meaning: More extreme values (both high and low) occur than expected under normality\nReal-world examples:\n\nFinancial returns: Stock market crashes and booms occur more frequently than normal distribution predicts\nMeasurement errors: Occasionally very large errors occur due to equipment malfunction or human mistakes\nResponse times: Most responses are quick, but some take much longer due to system overload or user distraction\n\nStatistical implications: Higher risk of extreme events; standard confidence intervals may be too narrow\n\n\n\nThin tails\nThis example shows a distribution with lighter tails than normal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary statistics:\n\n\n     n   min mean median  max   sd skewness kurtosis\n1 1000 -1.18 0.01   0.01 1.17 0.58    -0.07     1.91\n\n\nUnderstanding Thin Tails (Low Kurtosis):\n\nQ-Q Plot Pattern: S-shaped curve - points above the line at low quantiles, below at high quantilees.\nKurtosis &lt; 3: Indicates lighter tails than normal distribution\nPractical meaning: Fewer extreme values occur than expected under normality\nReal-world examples: Bounded measurements, standardized test scores, some quality control data\nStatistical implications: Lower risk of extreme events; data more predictable than normal assumption suggests\n\n\nSummary of Distribution Shapes\nUnderstanding these patterns helps in:\n\nChoosing appropriate statistical methods\nIdentifying data quality issues\nMaking informed assumptions about uncertainty\nSelecting proper transformations when needed\nInterpreting results correctly in context",
    "crumbs": [
      "Home",
      "Basics",
      "Data Visualization"
    ]
  },
  {
    "objectID": "01_introduction.html",
    "href": "01_introduction.html",
    "title": "Probability and Data in Business Analytics",
    "section": "",
    "text": "Study Objectives\n\nApply probability rules to real business events.\nDistinguish clearly between population parameters and sample estimators.\nCompute and explain expectation, variance, and standard deviation in business contexts.\nInterpret higher moments: skewness and kurtosis for tail and asymmetry assessment.\nCombine random variables (e.g., portfolio or multi‑channel forecast) and decompose variance with covariance terms.\nCalculate and interpret covariance and correlation; relate independence vs. zero correlation.\nProbability is the foundation for making data-driven decisions in business. This section covers the essential concepts, formulas, and business examples you’ll use in analytics.",
    "crumbs": [
      "Home",
      "Basics",
      "Probability and Data in Business Analytics"
    ]
  },
  {
    "objectID": "01_introduction.html#probability-axioms",
    "href": "01_introduction.html#probability-axioms",
    "title": "Probability and Data in Business Analytics",
    "section": "1 Probability Axioms",
    "text": "1 Probability Axioms\nProbability measures how likely an event is.\n\\[P(A) = \\frac{\\text{Number of ways A can occur}}{\\text{Total possible outcomes}}\\]\n\n\\(P(A) \\geq 0\\) for any event \\(A\\)\n\\(P(\\text{all possible outcomes}) = 1\\)\n\\(P(A \\text{ or } B) = P(A) + P(B) - P(A\\text{ and }B)\\)\nIf \\(A\\) and \\(B\\) are mutually exclusive, then \\[\nP(A \\text{ and } B) = 0\n\\] and \\[\nP(A \\text{ or } B) = P(A) + P(B)\n\\] Mutually exclusive means “cannot happen together.”\n\n\nExample 1 A retailer estimates a 30% chance of a supply chain disruption and a 50% chance of a demand spike. If these events are mutually exclusive, what is the probability of either event occurring?\n\n\nSolution\n\n\nSolution 1. Since the events are mutually exclusive, \\(P(A \\text{ and } B)=0,\\) then \\[\n\\begin{split}\nP(A \\text{ and } B)=P(A)+P(B)-P(A \\text{ and } B)=0.30+0.50=0.80.\n\\end{split}\n\\]\n\n\nExample 2 In a multi-channel marketing campaign, 35% of customers click an email offer (Event A) and 25% engage with a social media ad (Event B). Data shows 10% of customers do both. What is the probability a randomly selected customer engages with at least one of the two channels (email OR social)?\n\n\nSolution\n\n\nSolution 2. \n\\(P(A) + P(B) - P(A \\text{ and } B) = 0.35 + 0.25 - 0.10 = 0.50\\)",
    "crumbs": [
      "Home",
      "Basics",
      "Probability and Data in Business Analytics"
    ]
  },
  {
    "objectID": "01_introduction.html#sample-vs.-population-estimating-the-big-picture",
    "href": "01_introduction.html#sample-vs.-population-estimating-the-big-picture",
    "title": "Probability and Data in Business Analytics",
    "section": "2 Sample vs. Population: Estimating the Big Picture",
    "text": "2 Sample vs. Population: Estimating the Big Picture\nIn business, we use samples to estimate population characteristics.\nSample mean:\n\\[\n\\overline{X} = \\frac{1}{n} \\sum_{i=1}^n X_i\n\\]\n\nExample 3 A telecom company surveys a randomly selected sample of 500 customers about service quality. The sample mean satisfaction score is 4.2. How can this estimate help guide company-wide improvements?\n\n\nSolution\n\n\nSolution 3. \n\nThe sample mean (4.2) is based on 500 surveyed customers (the sample).\nThe population is all customers of the telecom company.\nTo guide company-wide improvements, we need to know if the sample is representative of the population.\nThere is uncertainty in the estimate; larger samples make the sample mean closer to the true population mean.\nThe sample mean is useful, but its reliability depends on sample representativeness and size.\n\n\n\nExample 4 Why is it important to distinguish between sample and population in business analytics?\n\n\nSolution\n\n\nSolution 4. \nBecause we use samples to estimate population parameters, and understanding the difference helps us make better decisions and avoid bias.",
    "crumbs": [
      "Home",
      "Basics",
      "Probability and Data in Business Analytics"
    ]
  },
  {
    "objectID": "01_introduction.html#moments",
    "href": "01_introduction.html#moments",
    "title": "Probability and Data in Business Analytics",
    "section": "3 Moments",
    "text": "3 Moments\n\n3.1 Expectation, Variance, and Standard Deviation\n\nExpectation (population): Average level of the outcome (true but unknown in practice). \\[\n\\mathbb{E}[X] =\n\\begin{cases}\n\\displaystyle \\sum_{i} x_i \\, P(X = x_i) & \\text{(discrete)} \\\\\n\\displaystyle \\int_{-\\infty}^{\\infty} x \\, f(x) \\, dx & \\text{(continuous)} \\\\\n\\end{cases}\n\\] Sample estimator: Replace the distribution by observed data: \\[\\bar{X} = \\dfrac{1}{n}\\sum_{i=1}^n X_i.\\] The sample mean \\(\\bar{X}\\) estimates the population mean \\(\\mathbb{E}[X]\\).\nVariance (population): How much values fluctuate around the true mean.\n\\[\\text{Var}(X) = E[(X - E[X])^2]\\]\nThe variance can also be expressed as:\n\\[\n\\begin{aligned}\n\\color{#00CC66}{\\text{Var}(X)} &= \\mathbb{E}\\left[\\left(X - \\mathbb{E}[X]\\right)^2\\right] \\\\\n&= \\color{#00CC66}{\\mathbb{E}[X^2] - (\\mathbb{E}[X])^2}.\n\\end{aligned}\n\\]\nThis second form is often faster to compute when you already have (or can easily get) \\(\\mathbb{E}[X^2]\\) (e.g., from grouped data or a probability model). In finance, we frequently estimate variance from simulated or historical returns by computing the average of squared returns (giving \\(\\mathbb{E}[X^2]\\)) and subtracting the square of the average return.\nSample estimators:\n\nBiased (population style) estimator: \\[ \\hat{\\sigma}^2 = \\dfrac{1}{n}\\sum_{i=1}^n (X_i - \\bar{X})^2 \\]\nUnbiased estimator: \\[s^2 = \\dfrac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X})^2 \\] The latter (\\(s^2\\)) subtracts 1 from \\(n\\) in the denominator, which is known as a degrees of freedom correction. This version has some desirable properties but we will not discuss these for now. Suffice to say that both versions are usually fine.\n\n\n\nExample 5 An operations planner models next-hour demand arrivals for a micro-warehouse (number of urgent orders) as \\(X \\in \\{0,1,2\\}\\) with probabilities 0.2, 0.5, 0.3. Compute the variability (variance) of order arrivals using the shortcut formula \\(\\mathbb{E}[X^2] - (\\mathbb{E}[X])^2\\) to inform staffing.\n\n\nSolution\n\n\nSolution 5. \\[\n\\begin{split}\n\\mathbb{E}[X] &= 0(0.2)+1(0.5)+2(0.3)=1.1 \\\\\n\\mathbb{E}[X^2] &= 0^2(0.2)+1^2(0.5)+2^2(0.3)=0+0.5+1.2=1.7 \\\\\n\\text{Var}(X) &= \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 = 1.7 - 1.1^2 = 1.7 - 1.21 = 0.49\n\\end{split}\n\\]\n\n\nExample 6 A SaaS company tracks daily change in Daily Active Users (DAU) (in %) over 5 days: \\(0.4\\), \\(0.6\\), \\(-0.2\\), \\(0.5\\), \\(0.7\\). Verify that the direct variance definition and the shortcut formula match; interpret what the variance implies for short‑term engagement volatility.\n\n\nSolution\n\n\nSolution 6. Daily % changes: \\(0.4,\\ 0.6,\\ -0.2,\\ 0.5,\\ 0.7\\). Let units be percentage points (pp). \\[\n\\mathbb{E}[X] = \\frac{0.4+0.6-0.2+0.5+0.7}{5} = 0.4 \\text{ pp}\n\\] \\[\n\\mathbb{E}[X^2] = \\frac{0.16+0.36+0.04+0.25+0.49}{5} = \\frac{1.30}{5} = 0.26 \\text{ (pp)}^2\n\\] Variance (percentage-point squared): \\[\n\\text{Var}(X) = 0.26 - (0.4)^2 = 0.26 - 0.16 = 0.10 \\text{ (pp)}^2\n\\] Standard deviation \\(= \\sqrt{0.10} \\approx 0.316\\) pp (about \\(0.32\\) percentage points). Direct (long) method matches (0.10).\nInterpretation: Moderate short-term volatility compared to the mean change of 0.4 pp. This suggests some fluctuations in user engagement, but not extreme.\n\n\nStandard Deviation (population): Square root of variance, showing average deviation from the true mean.\n\\[\\text{sd}(X) = \\sqrt{\\text{Var}(X)}\\]\nSample estimator: \\(s = \\sqrt{s^2}\\) (using the unbiased \\(s^2\\) above).\n\n\nExample 7 A hedge fund analyzes daily returns of two portfolios. Portfolio A has higher mean but also higher variance. How should risk-adjusted performance be compared?\n\n\nSolution\n\n\nSolution 7. \nCompare risk-adjusted metrics (e.g., Sharpe ratio = (mean - rf)/sd). Higher mean with disproportionate variance may yield lower Sharpe.\n\n\nExample 8 A business has daily profits of $200, $250, $180, $220, and $210. Calculate the mean and variance.\n\n\nSolution\n\n\nSolution 8. Mean: \\(\\bar{x}=\\dfrac{200+250+180+220+210}{5}=\\dfrac{1060}{5}=212\\).\nVariance calculations:\n\nSum of squared deviations\n\\[\n\\begin{aligned}\n(200-212)^2 &+ (250-212)^2 + (180-212)^2 + (220-212)^2 + (210-212)^2 \\\\\n&= 144 + 1444 + 1024 + 64 + 4 = 2680\n\\end{aligned}\n\\]\nPopulation variance (divide by \\(n=5\\))\n\\[ \\sigma^2 = \\frac{2680}{5} = 536 \\]\nSample variance (unbiased, divide by \\(n-1=4\\))\n\\[ s^2 = \\frac{2680}{4} = 670 \\]\n\nPopulation variance uses \\(n\\) when treating these 5 observations as the entire population; sample variance uses \\(n-1\\) to unbiasedly estimate \\(\\text{Var}(X)\\) of a larger process.\n\n\n\n3.2 Sums of Random Variables: Expectation & Variance\nPopulation linearity of expectation: \\[E[X + Y] = E[X] + E[Y]\\]\nSample counterpart: \\(\\overline{X+Y} = \\bar{X} + \\bar{Y}\\) (sample means add componentwise).\nIf \\(X\\) and \\(Y\\) are independent, the population variance of their sum is the sum of their variances: \\[\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\]\nIf not independent, add twice the covariance: \\[\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y) + 2\\text{Cov}(X, Y)\\]\nSample counterpart (unbiased style): \\[s_{X+Y}^2 = s_X^2 + s_Y^2 + 2 s_{XY},\\] where \\(s_{XY} = \\dfrac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar{X})(Y_i-\\bar{Y}).\\)\nIntuitive Example: In portfolio management, the total risk (variance) of a portfolio depends on the variances of individual assets and how they move together (covariance).\n\nExample 9 Two independent projects have profit variances of 100 and 150. What is the variance of total profit?\n\n\nSolution\n\n\nSolution 9. \n\\(100 + 150 = 250\\).\n\n\nVariance of Linear Transformations & Portfolios\nThe variance operator is the expectation of a quadratic function, so it is not linear. Instead, for constants \\(a, b\\):\n\\[\n\\text{Var}(a + bX) = b^2\\,\\text{Var}(X).\n\\]\nMore generally, for constants \\(a_i \\in \\mathbb{R}, i=1,\\ldots,n\\):\n\\[\n\\text{Var}\\left(\\sum_{i=1}^n a_i X_i \\right) = \\sum_{i=1}^n a_i^2 \\, \\text{Var}(X_i) + \\sum_{i=1}^n \\sum_{j=1}^n a_i a_j \\, \\text{Cov}(X_i, X_j).\n\\]\nIf the \\(X_i\\) are pairwise independent (or uncorrelated), the double sum of covariance terms drops out for \\(i\\neq j\\), leaving only the first sum.\n\nExample 10 A portfolio allocates 40% to Asset A and 60% to Asset B. \\(\\text{Var}(A)=0.04\\), \\(\\text{Var}(B)=0.09\\), and \\(\\text{Cov}(A,B)=0.01\\). Compute portfolio variance using the general formula.\n\n\nSolution\n\n\nSolution 10. \n\\(\\text{Var}(P)=0.4^2(0.04)+0.6^2(0.09)+2(0.4)(0.6)(0.01)=0.0064+0.0324+0.0048=0.0436\\).\n\n\nExample 11 A production planner combines forecasts: Final forecast \\(F = 2 + 0.7X_1 + 0.3X_2\\). If \\(\\text{Var}(X_1)=25\\), \\(\\text{Var}(X_2)=9\\), and \\(\\text{Cov}(X_1,X_2)=6\\), find \\(\\text{Var}(F)\\).\n\n\nSolution\n\n\nSolution 11. \nConstant 2 adds no variance. \\(\\text{Var}(F)=0.7^2(25)+0.3^2(9)+2(0.7)(0.3)(6)=12.25+0.81+2.52=15.58\\).\n\n\n\n\n3.3 Skewness\nSkewness measures asymmetry in a distribution.\n\nSkewness = 0: Symmetric (e.g., normal)\nSkewness &lt; 0: Longer left tail (large losses more likely than large gains)\nSkewness &gt; 0: Longer right tail (occasional big gains)\n\n\n\n\n\n\n\n\n\nFigure 1: Diagram of Skewness.\n\n\n\n\n\nBusiness reading: Positive skew in quarterly sales means you usually hit average numbers but sometimes land a very big contract. Negative skew in operational losses could mean rare but severe downside events that need contingency planning.\nLet \\(\\mu_3 = E[(X-\\mathbb{E}[X])^3]\\) denote the third central moment. Standardized (population) skewness: \\[\\gamma_1 = \\frac{\\mu_3}{\\text{sd}(X)^3}.\\]\nSample (bias‑adjusted) skewness estimator: \\[\\hat{\\gamma}_1 = \\frac{n}{(n-1)(n-2)} \\sum_{i=1}^n \\left(\\frac{X_i-\\bar{X}}{s}\\right)^3.\\]\n\n\n\n\n\n\nTip\n\n\n\nNote for skewness and kurtosis, it is NOT expected to hand calculate the values, the focus is on\n\nthe interpretation of given results.\nbe able to recognize skewness and kurtosis in data visualizations (e.g., histograms, box plots).\n\n\n\n\n\n3.4 Kurtosis\nKurtosis measures tail weight (propensity for outliers) relative to a normal distribution.\n\nKurtosis ≈ 3: Normal tail thickness\nKurtosis &gt; 3 (leptokurtic): Heavy / fat tails, more extreme outcomes\nKurtosis &lt; 3 (platykurtic): Light / thin tails, fewer extremes\n\n\n\n\n\n\n\n\n\nFigure 2: Examples of heavy-tailed distributions.\n\n\n\n\n\n\\(t\\)-distribution has higher kurtosis than normal distributions.\n\nMeaning that \\(t\\)-distribution has a higher probability of obtaining values that are far from the mean than a normal distribution.\nIt is less peaked in the center and higher in the tails than normal distribution.\nAs the degree of freedom increases, \\(t\\)-distribution approximates to normal distribution, kurtosis decreases and approximates to 3.\n\n\n\n\n\n\n\n\n\nFigure 3: The comparison between the t-distribution and the normal distribution at degrees of freedom ranging from 1 to 50. Figure source: from T.J. Kyner.\n\n\n\n\n\n\n\n\n\n\n\nFor the \\(t\\)-distribution:\n\n\n\n\nAs the degrees of freedom (df) increase, the \\(t\\)-distribution approaches the normal distribution.\nA common rule of thumb is that for \\(df &gt; 30\\), one can pretty safely use the normal distribution in place of a t-distribution unless you are interested in the extreme tails.\n\n\n\nLet \\(\\mu_4 = E[(X-\\mathbb{E}[X])^4]\\) be the fourth central moment. Standardized (population) kurtosis: \\[\\kappa = \\frac{\\mu_4}{\\text{sd}(X)^4}, \\qquad \\text{Excess kurtosis} = \\kappa - 3.\\] Sample (Fisher) excess kurtosis estimator: \\[\\hat{g}_2 = \\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\sum_{i=1}^n \\left(\\frac{X_i-\\bar{X}}{s}\\right)^4 - \\frac{3(n-1)^2}{(n-2)(n-3)}.\\] High excess kurtosis signals greater tail risk; low or negative excess indicates fewer extreme outcomes.\nInterpretation link: High positive skewness without high kurtosis means upside spikes with limited extra tail risk; high kurtosis (regardless of skew sign) means fatter tails and a need to focus on outlier management.\n\nExample 12 A company’s quarterly profits have excess kurtosis of 4. What does this mean for risk management?\n\n\nSolution\n\n\nSolution 12. Excess kurtosis is defined relative to a normal distribution (which has kurtosis 3).\n\nExcess kurtosis = Kurtosis − 3\nSo an excess kurtosis of 4 means the total kurtosis is 7, much higher than a normal distribution.\n\nThis implies the distribution of quarterly profits has fat tails — extreme profit or loss outcomes are more likely than under a normal distribution. Recommend to focus risk management on outliers.",
    "crumbs": [
      "Home",
      "Basics",
      "Probability and Data in Business Analytics"
    ]
  },
  {
    "objectID": "01_introduction.html#covariance-correlation-independence",
    "href": "01_introduction.html#covariance-correlation-independence",
    "title": "Probability and Data in Business Analytics",
    "section": "4 Covariance, Correlation & Independence",
    "text": "4 Covariance, Correlation & Independence\nPopulation covariance measures the direction of comovement between two variables:\n\nPositive covariance: Both variables increase or decrease together\nNegative covariance: One increases while the other decreases\nMagnitude: Does not indicate strength of relationship\n\nPopulation formulas: \\[\\text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])]\\] \\[\\text{Cov}(X, Y) = E[XY] - E[X]E[Y]\\]\nSample covariance (unbiased): \\[s_{XY} = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y}).\\]\nPopulation correlation standardizes covariance to a scale from -1 to 1: \\[\\text{Corr}(X, Y) = \\frac{\\text{Cov}(X, Y)}{\\text{sd}(X) \\cdot \\text{sd}(Y)}\\]\nSample correlation: \\[r_{XY} = \\frac{s_{XY}}{s_X s_Y}.\\]\n\nCorrelation = 1: Perfect positive relationship\nCorrelation = \\(-1\\): Perfect negative relationship\nCorrelation = 0: No linear relationship\n\nIndependence means knowing one event doesn’t affect the other: \\[P(A \\text{ and } B) = P(A) \\cdot P(B)\\]\n\n\n\n\n\n\nIndependence vs. Correlation\n\n\n\nIndependence means no influence between events, while correlation measures linear relationship strength.\nIndependence implies zero correlation, but zero correlation does not imply independence.\nE.g., \\(X\\) and \\(Y = X^2\\) have zero correlation but are not independent.\n\n\nBusiness Example: Suppose a retailer finds that sales and weather are correlated. This information can be used to adjust inventory planning for seasonal effects. If two marketing campaigns are independent, the outcome of one does not affect the other.\n\nExample 13 Suppose a customer visits a supermarket. Let:\n\n\\(A\\) = “customer buys a loaf of bread today” and \\(P(A)=30\\%\\).\n\\(B\\) = “customer buys laundry detergent today” and \\(P(B)=10\\%\\).\n\nWhat is the probability the customer buys both bread and detergent today? What does observing a bread purchase tell you about \\(P(B)\\)?\n\n\nSolution\n\n\nSolution 13. Bread (frequent staple) and detergent (infrequent refill) decisions are unrelated, so it is reasonable to assume independence. Hence \\[\nP(A \\text{ and } B) = 30\\% \\times 10\\% = 3\\%\n\\] Observing bread does not change \\(P(B)=10\\%\\).\n\n\nExample 14 If the correlation between two stocks is \\(-0.8,\\) what does this mean for portfolio diversification?\n\n\nSolution\n\n\nSolution 14. \nThey tend to move in opposite directions, aiding diversification and reducing portfolio variance.\n\n\n\n\n\n\n\nMutually Exclusive vs Independence\n\n\n\nMutually exclusive: Cannot happen together \\[P(A\\cap B)=0\\]\nIndependent: Knowledge of one does not change probability of the other \\[P(A\\cap B)=P(A)P(B)\\]\nThink: Disjoint = “never together”; Independent = “no influence”.",
    "crumbs": [
      "Home",
      "Basics",
      "Probability and Data in Business Analytics"
    ]
  },
  {
    "objectID": "01_introduction.html#population-vs.-sample-summary-estimators",
    "href": "01_introduction.html#population-vs.-sample-summary-estimators",
    "title": "Probability and Data in Business Analytics",
    "section": "5 Population vs. Sample: Summary & Estimators",
    "text": "5 Population vs. Sample: Summary & Estimators\nWhen analysing data we distinguish between:\n\nPopulation parameters (target): Fixed (but usually unknown) numerical characteristics of the underlying process (e.g., \\(\\mathbb{E}[X], \\text{Var}(X), \\gamma_1, \\kappa, \\text{Cov}(X,Y)\\)).\nSample statistics (estimators): Functions of observed data used to approximate population parameters (e.g., \\(\\bar{X}, s^2, s, \\hat{\\gamma}_1, \\hat{g}_2, s_{XY}, r_{XY}\\)). They vary from sample to sample.\n\nKey principles:\n\nReplace integrals / probability-weighted sums with empirical averages.\nCenter around the sample mean when the population mean is unknown.\nUse \\(n-1\\) (unbiased) denominators for second moments (variance, covariance) when estimating from an i.i.d. sample.\nAlways interpret sample statistics as estimates with sampling variability; risk management and forecasting should account for estimation error (e.g., standard deviation and confidence intervals).\n\n\n5.1 Quick Reference Table\n\n\n\n\n\n\n\n\n\nConcept\nPopulation Symbol / Definition\nSample Estimator\nNotes\n\n\n\n\nMean\n\\(\\mathbb{E}[X]\\)\n\\(\\bar{X}=\\tfrac{1}{n}\\sum X_i\\)\nUnbiased for mean (i.i.d.)\n\n\nVariance\n\\(\\text{Var}(X)=E[(X-\\mathbb{E}[X])^2]\\)\n\\(s^2=\\tfrac{1}{n-1}\\sum (X_i-\\bar{X})^2\\)\n\\(s_n^2\\) (divide by \\(n\\)) is biased low\n\n\nStd. Dev.\n\\(\\text{sd}(X)=\\sqrt{\\text{Var}(X)}\\)\n\\(s=\\sqrt{s^2}\\)\nPlug-in\n\n\nSkewness\n\\(\\gamma_1=\\mu_3/\\text{sd}^3\\), \\(\\mu_3=E[(X-\\mathbb{E}[X])^3]\\)\n\\(\\hat{\\gamma}_1=\\frac{n}{(n-1)(n-2)}\\sum (\\frac{X_i-\\bar{X}}{s})^3\\)\nMeasures asymmetry\n\n\nKurtosis (excess)\n\\(\\kappa-3\\), \\(\\kappa=\\mu_4/\\text{sd}^4\\)\n\\(\\hat{g}_2\\) (Fisher)\nTail heaviness vs. normal\n\n\nCovariance\n\\(\\text{Cov}(X,Y)=E[(X-\\mathbb{E}X)(Y-\\mathbb{E}Y)]\\)\n\\(s_{XY}=\\tfrac{1}{n-1}\\sum (X_i-\\bar{X})(Y_i-\\bar{Y})\\)\nSign = direction\n\n\nCorrelation\n\\(\\rho=\\dfrac{\\text{Cov}(X,Y)}{\\text{sd}(X)\\text{sd}(Y)}\\)\n\\(r_{XY}=\\dfrac{s_{XY}}{s_X s_Y}\\)\nScale-free ( -1 to 1 )\n\n\n\n\n\n\n\n\n\nInterpretation tip\n\n\n\nIf a sample statistic looks extreme (e.g., very high skewness or kurtosis), examine sample size and outliers; small samples amplify noise in higher-moment estimates.",
    "crumbs": [
      "Home",
      "Basics",
      "Probability and Data in Business Analytics"
    ]
  },
  {
    "objectID": "01_Lab-1.html",
    "href": "01_Lab-1.html",
    "title": "Lab 1: Probability & Descriptive Statistics",
    "section": "",
    "text": "Open in Google Colab: Link\nAlternatively, copy and paste the code below into RStuido Desktop or RStudio Cloud.\nFinish Quiz for Lab 1 on Canvas.\nFocus: explore core descriptive statistics and dependence concepts using simple synthetic data.\nYou will:\n\nExamine independence & correlation\nCompare skewed and roughly normal distributions\nCompute mean, variance, sd, skewness, kurtosis\nCompare biased vs unbiased variance estimators\nSimulate a high-correlation (ρ = 0.90) two-asset return setting\nStudy sampling distributions of asset and portfolio means",
    "crumbs": [
      "Home",
      "Basics",
      "Lab 1: Probability & Descriptive Statistics"
    ]
  },
  {
    "objectID": "01_Lab-1.html#instructions",
    "href": "01_Lab-1.html#instructions",
    "title": "Lab 1: Probability & Descriptive Statistics",
    "section": "",
    "text": "Open in Google Colab: Link\nAlternatively, copy and paste the code below into RStuido Desktop or RStudio Cloud.\nFinish Quiz for Lab 1 on Canvas.\nFocus: explore core descriptive statistics and dependence concepts using simple synthetic data.\nYou will:\n\nExamine independence & correlation\nCompare skewed and roughly normal distributions\nCompute mean, variance, sd, skewness, kurtosis\nCompare biased vs unbiased variance estimators\nSimulate a high-correlation (ρ = 0.90) two-asset return setting\nStudy sampling distributions of asset and portfolio means",
    "crumbs": [
      "Home",
      "Basics",
      "Lab 1: Probability & Descriptive Statistics"
    ]
  },
  {
    "objectID": "01_Lab-1.html#datasets",
    "href": "01_Lab-1.html#datasets",
    "title": "Lab 1: Probability & Descriptive Statistics",
    "section": "2 Datasets",
    "text": "2 Datasets\nSynthetic generators used:\n\nRight-skewed transaction values (log-normal)and approx. normal reference sample\nTwo correlated asset return series (ρ = 0.90) for sampling exercises\n\nRun each cell sequentially. Read the comments carefully.\n\n# -------- Setup: packages --------\n# Unified required package list\npkgs &lt;- c(\"tidyverse\", \"ggsci\", \"moments\", \"knitr\", \"kableExtra\")\nmissing &lt;- setdiff(pkgs, rownames(installed.packages()))\nif (length(missing) &gt; 0) install.packages(missing)\n\n# Load all packages (silently)\ninvisible(lapply(pkgs, library, character.only = TRUE))\n\n# Set default options for figures in Jupyter Notebook\noptions(repr.plot.width = 12, repr.plot.height = 4)  # wider default figures\n\nmessage(\"\\n Setup complete (packages loaded: \", paste(pkgs, collapse = \", \"), \").\")\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n\n✔ dplyr     1.1.4          ✔ readr     2.1.5     \n\n✔ forcats   1.0.0          ✔ stringr   1.5.1     \n\n✔ ggplot2   3.5.2.9002     ✔ tibble    3.3.0     \n\n✔ lubridate 1.9.4          ✔ tidyr     1.3.1     \n\n✔ purrr     1.1.0          \n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n\n✖ dplyr::filter() masks stats::filter()\n\n✖ dplyr::lag()    masks stats::lag()\n\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n\n✖ dplyr::filter() masks stats::filter()\n\n✖ dplyr::lag()    masks stats::lag()\n\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nAttaching package: ‘kableExtra’\n\n\n\n\n\nThe following object is masked from ‘package:dplyr’:\n\n\n\n    group_rows\n\n\n\n\n\n\n\n Setup complete (packages loaded: tidyverse, ggsci, moments, knitr, kableExtra).\n\n\n\n\n\nAttaching package: ‘kableExtra’\n\n\n\n\n\nThe following object is masked from ‘package:dplyr’:\n\n\n\n    group_rows\n\n\n\n\n\n\n\n Setup complete (packages loaded: tidyverse, ggsci, moments, knitr, kableExtra).",
    "crumbs": [
      "Home",
      "Basics",
      "Lab 1: Probability & Descriptive Statistics"
    ]
  },
  {
    "objectID": "01_Lab-1.html#independence-vs.-zero-correlation",
    "href": "01_Lab-1.html#independence-vs.-zero-correlation",
    "title": "Lab 1: Probability & Descriptive Statistics",
    "section": "3 Independence vs. Zero Correlation",
    "text": "3 Independence vs. Zero Correlation\nIndependence means no influence between events, while correlation measures linear relationship strength.\nIndependence implies zero correlation, but zero correlation does not imply independence.\nE.g., \\(X\\) and \\(Y = X^2\\) have zero correlation but are not independent.\n\n# ---- Zero Correlation but NOT Independence: Y = X^2 ----\n# Idea: X ~ N(0,1); define Y = X^2.  Then cor(X,Y) ~ 0 (symmetry cancels linear relation),\n# yet Y is a deterministic function of X so they are NOT independent.\n\nset.seed(2025)\nn &lt;- 5000\nX &lt;- rnorm(n)\nY &lt;- X^2\nr_xy &lt;- cor(X, Y)\ncat(sprintf(\"Sample correlation cor(X, Y=X^2) = %.4f (near 0)\\n\", r_xy))\n\nSample correlation cor(X, Y=X^2) = 0.0198 (near 0)\n\n\nWe got \\(\\rho_{X,Y}=0.0198\\). It seems to be close to zero, but we need a statistical test to confirm this formally.\n→ Test for significance of the correlation coefficient\n\ncor.test(X, Y)\n\n\n    Pearson's product-moment correlation\n\ndata:  X and Y\nt = 1.4027, df = 4998, p-value = 0.1608\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.007887062  0.047529726\nsample estimates:\n       cor \n0.01983657 \n\n\n\n💡 Q: Based on the output of cor.test, what is the p-value for the correlation test? What can you conclude about the correlation between the two variables? Are \\(X\\) and \\(Y\\) independent? Write your answer in the cell below.\nA: [Type your answer here]\nDistinguish the two expressions:\n\ncorrelation coefficient equals zero ❌\ncorrelation coefficient (statistically) indifferent from zero ✅\n\n\nNow let’s visualize \\(Y=X^2\\) by plotting the scatter plot.\n\nlibrary(ggplot2)\noptions(repr.plot.width = 8, repr.plot.height = 7)\nscatter_df &lt;- data.frame(X = X, Y = Y)\np1 &lt;- ggplot(scatter_df, aes(X, Y)) +\n  geom_point(alpha = 0.25, color = '#2c7fb8') +\n  annotate('text', x = min(X)+0.2, y = max(Y)*0.95,\n           label = paste0('cor = ', sprintf('%.3f', r_xy)),\n           hjust = 0, size = 4) +\n  labs(title = 'Zero Linear Correlation but Nonlinear Dependence',\n       subtitle = 'Y = X^2 with X ~ N(0,1)',\n       x = 'X', y = 'Y = X^2') +\n  theme_minimal()\nprint(p1)\n\n\n\n\n\n\n\n\nThe following figure shows the scatter plot of two assets with various correlation coefficients.",
    "crumbs": [
      "Home",
      "Basics",
      "Lab 1: Probability & Descriptive Statistics"
    ]
  },
  {
    "objectID": "01_Lab-1.html#descriptive-statistics-on-asset-returns",
    "href": "01_Lab-1.html#descriptive-statistics-on-asset-returns",
    "title": "Lab 1: Probability & Descriptive Statistics",
    "section": "4 Descriptive Statistics on Asset Returns",
    "text": "4 Descriptive Statistics on Asset Returns\n\n# ---- Load Asset Returns Data ----\nasset_df &lt;- read_csv(\"https://raw.githubusercontent.com/my1396/FIN5005-Fall2025/refs/heads/main/data/asset_returns.csv\")\nprint(\"Preview: first 6 rows of asset returns data frame:\")\nhead(asset_df) %&gt;% round(2)\n\n[1] \"Preview: first 6 rows of asset returns data frame:\"\n\n\n\nA tibble: 6 × 3\n\n\nAsset_A\nAsset_B\nAsset_C\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n17.89\n4.31\n9.94\n\n\n10.30\n4.17\n10.55\n\n\n20.87\n14.16\n6.61\n\n\n18.21\n22.33\n9.58\n\n\n17.49\n2.23\n10.23\n\n\n29.87\n15.72\n8.97\n\n\n\n\n\nWe define a summary statistics function to compute the statistics of interest.\n\nquick_summary &lt;- function(x) {\n  # Function to compute basic descriptive statistics\n  data.frame(\n    n = length(x),\n    mean = mean(x),\n    sd = sd(x),\n    var = var(x),\n    skewness = moments::skewness(x),\n    kurtosis = moments::kurtosis(x),\n    row.names = NULL\n  )\n}\n\nThe summary statistic table is generated as follows:\n\nlapply(asset_df, quick_summary) %&gt;%\n    do.call(rbind, .) %&gt;%\n    kable(digits = 2, caption = \"Descriptive Statistics of Asset Returns\")\n\n\n\nTable: Descriptive Statistics of Asset Returns\n\n|        |     n|  mean|   sd|   var| skewness| kurtosis|\n|:-------|-----:|-----:|----:|-----:|--------:|--------:|\n|Asset_A | 10000| 20.14| 4.82| 23.24|    -0.10|     3.15|\n|Asset_B | 10000|  8.50| 4.44| 19.71|     1.71|     9.19|\n|Asset_C | 10000|  9.99| 1.01|  1.01|    -0.85|     3.50|\n\n\nPlot the histograms of the asset returns to visualize their distributions.\n\n# ---- Visualization: Histograms with Normal Density Overlay ----\n# Convert to long form\ncombined &lt;- asset_df %&gt;% pivot_longer(cols = everything(), names_to = \"type\", values_to = \"value\")\n\n# Compute per-type mean & sd and create normal curve points\nnorm_params &lt;- combined %&gt;% group_by(type) %&gt;% summarise(mu = mean(value), sigma = sd(value), .groups = 'drop')\n\nnorm_curve &lt;- norm_params %&gt;% group_by(type) %&gt;% do({\n  mu &lt;- .$mu; sigma &lt;- .$sigma\n  x &lt;- seq(mu - 4*sigma, mu + 4*sigma, length.out = 400)\n  tibble(value = x, density = dnorm(x, mu, sigma))\n})\n\noptions(repr.plot.width = 15, repr.plot.height = 6)\n\nggplot(combined, aes(value)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 40, fill = \"#3182bd\", alpha = 0.8, color = \"white\") +\n  geom_line(data = norm_curve, aes(value, density), color = \"red\", linewidth = 0.8) +\n  facet_wrap(~ type, scales = \"free\", nrow = 1) +\n  labs(title = \"Distribution Contrast with Normal Overlay\",\n       subtitle = \"Red curve = fitted normal using sample mean & sd per asset\",\n       x = \"Value\", y = \"Density\") +\n  theme(panel.spacing.x = unit(1.2, \"lines\"))\n\n\n\n\n\n\n\n\n\n💡 Q: Answer the following questions based on the summary statistics and the distribution plots:\n\nWhich asset has the highest mean return?\nWhich asset is the most risky? and which one has the most stable returns?\nWhich asset shows the positive skewness and what does it mean?\nIf an investor prefers assets with low risk and return distributions with tails close to normal, which asset is most appropriate?\nBased on mean and standard deviation, which asset has the best risk–return trade-off (highest mean per unit of risk)?\n\nA: [Type your answer here]",
    "crumbs": [
      "Home",
      "Basics",
      "Lab 1: Probability & Descriptive Statistics"
    ]
  },
  {
    "objectID": "01_Lab-1.html#large-sample-gives-more-accurate-estimates",
    "href": "01_Lab-1.html#large-sample-gives-more-accurate-estimates",
    "title": "Lab 1: Probability & Descriptive Statistics",
    "section": "5 Large Sample Gives More Accurate Estimates",
    "text": "5 Large Sample Gives More Accurate Estimates\nWe have a variable \\(X\\sim N(0,5^2).\\)\nNormal distribution notation: \\(X\\sim N(\\mu, \\sigma^2)\\) means that \\(X\\) is normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^2.\\)\nThe second parameter is the variance, NOT the standard deviation.\nNow we draw random samples of size 5, 10, 10, … until 5000 from this distribution and compute the sample mean and standard deviation for each sample.\nFor each sample, we compute the biased and unbiased sample variance\n\nThe biased sample variance is computed as \\[\\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^2.\\]\nThe unbiased sample variance is computed as \\[\\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2\\]\n\n\n# ---- Simulation: Sample vs Population variance ----\nset.seed(2025)  # for reproducibility\ntrue_var &lt;- 25  # sd^2 with sd=5, true population variance\nsample_sizes &lt;- c(5, 10, 20, 50, 100, 250, 500, 1000, 5000) # varying sample sizes from 5 to 500\nresults &lt;- lapply(sample_sizes, function(n){\n  x &lt;- rnorm(n, mean = 0, sd = 5)\n  data.frame(\"sample_size\" = n, \"var_biased\" = mean((x - mean(x))^2), \"var_unbiased\" = var(x))\n}) %&gt;% dplyr::bind_rows()\ncat('Variance estimators: var_n (biased, divide by n) vs var_n1 (unbiased, divide by n-1).\\n')\nresults\n\nVariance estimators: var_n (biased, divide by n) vs var_n1 (unbiased, divide by n-1).\n\n\n\nA data.frame: 9 × 3\n\n\nsample_size\nvar_biased\nvar_unbiased\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n5\n4.262707\n5.328384\n\n\n10\n14.832929\n16.481032\n\n\n20\n33.210505\n34.958426\n\n\n50\n25.332410\n25.849398\n\n\n100\n24.957059\n25.209150\n\n\n250\n22.936725\n23.028840\n\n\n500\n25.862610\n25.914439\n\n\n1000\n25.574182\n25.599782\n\n\n5000\n25.301658\n25.306719\n\n\n\n\n\n\n💡 Q: Based on the variance estimates for different sample sizes:\n\nAs the sample size increases, how do the sample variance estimates compare to the true variance \\(25\\)?\nDoes the sample variance converge to the true variance as the sample size increases?\nWhat is the difference between the biased and unbiased sample variance estimators?\n\nA: [Type your answer here]\n\n\n# ---- Plot: Convergence of variance estimators ----\n\nresults_long &lt;- results %&gt;%\n  tidyr::pivot_longer(var_biased:var_unbiased, names_to = \"estimator\", values_to = \"value\") %&gt;%\n  dplyr::mutate(estimator = dplyr::recode(estimator, var_n = \"Divide by n\", var_n1 = \"Divide by n-1\"))\n\noptions(repr.plot.width = 12, repr.plot.height = 8)\n\nggplot(results_long, aes(sample_size, value, color = estimator)) +\n  geom_line() +\n  geom_point() +\n  scale_color_npg() +\n  geom_hline(yintercept = true_var, linetype = \"dashed\") +\n  xlim(c(0,500)) +\n  labs(title = \"Convergence of Variance Estimators\", y = \"Estimated variance\", x = \"Sample size\")",
    "crumbs": [
      "Home",
      "Basics",
      "Lab 1: Probability & Descriptive Statistics"
    ]
  },
  {
    "objectID": "01_Lab-1.html#correlated-asset-returns",
    "href": "01_Lab-1.html#correlated-asset-returns",
    "title": "Lab 1: Probability & Descriptive Statistics",
    "section": "6 Correlated Asset Returns",
    "text": "6 Correlated Asset Returns\nWe will simulate repeated samples of two (correlated) asset return series and compare the sampling distributions of their sample means and the distribution of the portfolio mean (equal weights).\n\n# ---- Load Correlated Asset Returns Data ----\nX_demo &lt;- read_csv(\"https://raw.githubusercontent.com/my1396/FIN5005-Fall2025/refs/heads/main/data/correlated_asset_returns.csv\")\ncolnames(X_demo) &lt;- c(\"Asset_A\", \"Asset_B\")  # Rename columns for clarity\nhead(X_demo) %&gt;% round(4)\n\n\nA tibble: 6 × 2\n\n\nAsset_A\nAsset_B\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n0.0129\n0.0351\n\n\n0.0012\n0.0094\n\n\n0.0160\n0.0130\n\n\n0.0259\n0.0550\n\n\n0.0079\n0.0147\n\n\n-0.0028\n-0.0234\n\n\n\n\n\n\nemp_cor &lt;- cor(X_demo)\nemp_cor\n\nPlot the scatter plot of the two asset returns to visualize their relationship.\n\noptions(repr.plot.width = 8, repr.plot.height = 7)\nggplot(X_demo, aes(Asset_A, Asset_B)) +\n  geom_point(alpha = 0.35, color = '#1b7837') \n\n\n💡Q: What is the correlation between the two asset returns? How do their prices move together?\nA: [Type your answer here]\n\n\n Exercise\nFollowing the steps below, you will compute the mean and standard deviation of the two asset returns, construct a portfolio, and analyze its properties.\nStep 1: Calculate the standard deviation of Asset A and B returns, respectively.\nStep 2: Construct a equally weighted portfolio of the two assets. Calculate the portfolio returns. \\[\nE[r_p] = \\frac{1}{2} (E[r_A] + E[r_B])\n\\]\nStep 3: Calculate the standard deviation of the portfolio returns, \\(\\text{sd}(r_p)\\).\nStep 4: Calculate the average of the volatility of Asset A and B (using results in Step 1). Compare it with the volatility of the portfolio returns.\n\n\n6.1 Reflection\nHow does correlation between assets influence the portfolio mean’s variability? Write 3–4 sentences interpreting for a diversified vs concentrated investment decision.",
    "crumbs": [
      "Home",
      "Basics",
      "Lab 1: Probability & Descriptive Statistics"
    ]
  },
  {
    "objectID": "00_environment_setup.html",
    "href": "00_environment_setup.html",
    "title": "1 Seting Up Your Environment",
    "section": "",
    "text": "This guide will help you set up your coding environment for FIN5005, with both cloud-based and local options available (you may choose either).\n\nYou will learn how to use RStudio Cloud for quick, browser-based access and RStudio Desktop for the full set of features on your computer.\nIt is strongly recommended to install the local version (RStudio Desktop) on your computer.\n\nFollow the instructions below to get started.\nEstimated reading time: 6 minutes\n\n\n\n\n\nCloud solution means that you don’t need to install anything on your computer. You can run R code directly in your web browser. This is the easiest way to get started without worrying about installation issues.\nBut soon enough, you will realize that the cloud-based solution is limited in:\n\ncompute power: 1 CPU core, 1 GB RAM\nexecution hours: 25 hours per month;\nslow performance\nAI integration is better in local environment\nGitHub Copilot is a powerful AI tool that offers autocomplete-style suggestions as you code. The tool can give you suggestions based on the code you want to use or by simply inquiring about what you want the code to do.\nIt is developed by GitHub in partnership with OpenAI, and it is designated to best assist developers in writing code more efficiently.\nSome of its highlight features include:\n\nCode autocompletion: generating suggestions while typing the code.\nCode generation: Copilot will use the context of the active document to generate suggestions for code that might be useful.\nAnswering questions: it can also be used to ask simple questions while you are coding (e.g., “What is the definition of mean?”).\nLanguage support: supports multiple programming languages, including R, Python, SQL, HTML, and JavaScript.\n\nTo enable Copilot in RStudio: click on Tools → Global Options → Copilot → tick the box saying “Enable GitHub Copilot” → sign in to your GitHub account, and there you go; you are ready to start!\nGiHub Copilot free plan has limited usage: 2000 code autocompletions per month, 30 code generations per month, and 30 question answers per month.\nSoon enough, you will realize that you need more than that. Because every character you type counts as usage, regless of whether you accept the suggestion or not.\n🎈 The good news is: You can use Copilot Pro for free if you are a student!\nSee Apply to GitHub Education as a student.\nIf you wonder anything, look up in GitHub Copilot Documentation.\n\n\nNonetheless, here are two popular cloud solutions:\n\nGoogle Colab: good support for Jupyter Notebooks .ipynb.\nWe will use Jupyter Notebooks in this course.\nRStudio Cloud: basic emulator for RStudio IDE in the cloud.\n\n\n\nOnce in the dashboard, you can click on the New Project - New RStudio Project to get started:\n\nOnce the new project is created, you will see the RStudio IDE interface, which is similar to the one you would find in RStudio Desktop. You can write and execute R code, create scripts, and manage your projects directly in the cloud.\n\n\n\n\n\n\n\nRStudio Desktop is a free and open-source integrated development environment (IDE) for R. It provides a user-friendly interface for writing and executing R code, making it easier to work with R scripts, data analysis, visualization.\nTo install RStudio Desktop, follow these steps:\n\nInstall R\nInstall RStudio Desktop\n\nGo to RStudio Desktop Download and download the installer for your operating system (Windows, macOS, or Linux).\n\nref:\n\nRStudio Cloud - How to Get Started For Free",
    "crumbs": [
      "Home",
      "Environment Setup"
    ]
  },
  {
    "objectID": "00_environment_setup.html#cloud-solution",
    "href": "00_environment_setup.html#cloud-solution",
    "title": "1 Seting Up Your Environment",
    "section": "",
    "text": "Cloud solution means that you don’t need to install anything on your computer. You can run R code directly in your web browser. This is the easiest way to get started without worrying about installation issues.\nBut soon enough, you will realize that the cloud-based solution is limited in:\n\ncompute power: 1 CPU core, 1 GB RAM\nexecution hours: 25 hours per month;\nslow performance\nAI integration is better in local environment\nGitHub Copilot is a powerful AI tool that offers autocomplete-style suggestions as you code. The tool can give you suggestions based on the code you want to use or by simply inquiring about what you want the code to do.\nIt is developed by GitHub in partnership with OpenAI, and it is designated to best assist developers in writing code more efficiently.\nSome of its highlight features include:\n\nCode autocompletion: generating suggestions while typing the code.\nCode generation: Copilot will use the context of the active document to generate suggestions for code that might be useful.\nAnswering questions: it can also be used to ask simple questions while you are coding (e.g., “What is the definition of mean?”).\nLanguage support: supports multiple programming languages, including R, Python, SQL, HTML, and JavaScript.\n\nTo enable Copilot in RStudio: click on Tools → Global Options → Copilot → tick the box saying “Enable GitHub Copilot” → sign in to your GitHub account, and there you go; you are ready to start!\nGiHub Copilot free plan has limited usage: 2000 code autocompletions per month, 30 code generations per month, and 30 question answers per month.\nSoon enough, you will realize that you need more than that. Because every character you type counts as usage, regless of whether you accept the suggestion or not.\n🎈 The good news is: You can use Copilot Pro for free if you are a student!\nSee Apply to GitHub Education as a student.\nIf you wonder anything, look up in GitHub Copilot Documentation.\n\n\nNonetheless, here are two popular cloud solutions:\n\nGoogle Colab: good support for Jupyter Notebooks .ipynb.\nWe will use Jupyter Notebooks in this course.\nRStudio Cloud: basic emulator for RStudio IDE in the cloud.\n\n\n\nOnce in the dashboard, you can click on the New Project - New RStudio Project to get started:\n\nOnce the new project is created, you will see the RStudio IDE interface, which is similar to the one you would find in RStudio Desktop. You can write and execute R code, create scripts, and manage your projects directly in the cloud.",
    "crumbs": [
      "Home",
      "Environment Setup"
    ]
  },
  {
    "objectID": "00_environment_setup.html#local-solution",
    "href": "00_environment_setup.html#local-solution",
    "title": "1 Seting Up Your Environment",
    "section": "",
    "text": "RStudio Desktop is a free and open-source integrated development environment (IDE) for R. It provides a user-friendly interface for writing and executing R code, making it easier to work with R scripts, data analysis, visualization.\nTo install RStudio Desktop, follow these steps:\n\nInstall R\nInstall RStudio Desktop\n\nGo to RStudio Desktop Download and download the installer for your operating system (Windows, macOS, or Linux).\n\nref:\n\nRStudio Cloud - How to Get Started For Free",
    "crumbs": [
      "Home",
      "Environment Setup"
    ]
  }
]