{
  "hash": "b9fb9cebc3cf08b6fb1066739a17bfef",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Multiple Linear Regression\"\n---\n\n\n::: {.callout-note appearance=\"simple\" icon=false}\n🎯 **Study Objectives**\n\n- Understand the concept of multiple linear regression (MLR) and how it extends simple linear regression (SLR).\n- Perform multiple regression analysis using R and interpret the results.\n- Learn how to interpret the coefficients in a multiple regression model.\n- Understand the importance of controlling for confounding variables in regression analysis.\n- Conduct and interpret F-tests for the overall significance of a regression model.\n- Explore alternative model specifications and their implications on regression results.\n:::\n\n\n<!-- Chunk option settings --> \n\n\n\n<!-- Define chunk option `max.lines` --> \n<!-- controls max lines printed --> \n\n\n\nThe multiple regression model extends the basic concept of the simple regression model to include multiple explanatory variables. This allows us to analyze the relationship between a dependent variable and several independent variables simultaneously.\n\nA multiple regression model enables us to estimate the effect on $Y_i$ of changing a regressor $X_{1i}$ if the remaining regressors $X_{2i}, X_{3i}, \\ldots, X_{ki}$ are held constant.\n\nJust like in the simple regression model, we assume the true relationship between $Y_i$ and $X_{1i}, X_{2i}, X_{3i}, \\ldots, X_{ki}$ to be linear. The relation is given by the population regression function:\n\n$$\nY_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{3i} + \\ldots + \\beta_k X_{ki} + \\varepsilon_i .\n$$\n\n\nNow we go back to the example of test scores of students in California schools. Recall in the simple regression model, we regression test score on expenditure per student and found statistically significant positive effect. \nHowever, the R-squared (3.7%) was rather low and residuals plot shows substantial noise, indicating that there are other factors affecting test scores.\n\n💡 We can improve the model by including more explanatory variables.\n\nWe expect that smaller class sizes lead to better test scores, as teachers can give more individual attention to each student.\nTherefore, we introduce class size (number of students per teacher) as an additional regressor. \n\n\nThe multiple regression model is then given by:\n\n$$\n\\text{TestScore}_i = \\beta_0 + \\beta_1 \\times \\text{expenditure}_i + \\beta_2 \\times \\text{STR} + \\varepsilon_i\n$$ \n\nwhere $\\text{STR}$ is the <span class=\"env-green\">student-teacher ratio</span> (representing class size).\n\n# Descriptive Analysis\n\n## Summary Statistics\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Data Preparation\nlibrary(tidyverse)\n\nf_name <- \"https://raw.githubusercontent.com/my1396/FIN5005-Fall2025/refs/heads/main/data/CASchools_test_score.csv\"\ncas <- read_csv(f_name,\n    col_types = cols(\n        county = col_factor(), # read as factor\n        grades = col_factor()\n    )\n)\ncas <- cas %>%\n    mutate(\n        TestScore = (read + math) / 2,\n        STR = students / teachers\n    )\n# summary statistics\ncas %>%\n    select(TestScore, expenditure, STR) %>%\n    summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   TestScore      expenditure        STR       \n Min.   :605.6   Min.   :3926   Min.   :14.00  \n 1st Qu.:640.0   1st Qu.:4906   1st Qu.:18.58  \n Median :654.5   Median :5215   Median :19.72  \n Mean   :654.2   Mean   :5312   Mean   :19.64  \n 3rd Qu.:666.7   3rd Qu.:5601   3rd Qu.:20.87  \n Max.   :706.8   Max.   :7712   Max.   :25.80  \n```\n\n\n:::\n:::\n\n\n\n## Scatter Plot\n\nScatter plot of TestScore against expenditure and STR:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Scatter plots of TestScore vs expenditure and STR. STR stands for students-teach ratio, proxying class size.](06_Multiple_Linear_Regression_files/figure-html/fig-scatter-1-1.png){#fig-scatter-1 fig-align='center' fig-pos='H' width=1152}\n:::\n:::\n\n\nThere seems to be a negative correlation between class size (STR) and test scores, as expected. \n\nUse `cor()` to compute the correlation matrix.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncor(cas %>% select(TestScore, expenditure, STR)) %>% round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            TestScore expenditure   STR\nTestScore        1.00        0.19 -0.23\nexpenditure      0.19        1.00 -0.62\nSTR             -0.23       -0.62  1.00\n```\n\n\n:::\n:::\n\n\n\n# Run Multiple Regression\n\nWe estimate the model using OLS (`lm()` function) and obtain the following results:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_multiple <- lm(TestScore ~ expenditure + STR, data = cas)\nsummary(model_multiple)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = TestScore ~ expenditure + STR, data = cas)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-47.507 -14.403   0.407  13.195  48.392 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 675.577176  19.562221  34.535   <2e-16 ***\nexpenditure   0.002487   0.001823   1.364   0.1733    \nSTR          -1.763216   0.610914  -2.886   0.0041 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.56 on 417 degrees of freedom\nMultiple R-squared:  0.05545,\tAdjusted R-squared:  0.05092 \nF-statistic: 12.24 on 2 and 417 DF,  p-value: 6.824e-06\n```\n\n\n:::\n:::\n\n\n## Interpretation of the Coefficients\n\nThe estimated model is\n\n$$\n\\widehat{\\text{TestScore}} = 675.58 + 0.0025 \\cdot \\text{expenditure} - 1.76 \\cdot \\text{STR}.\n$$\n\n* **STR (–1.76):** **Holding expenditure constant**, an additional student per teacher lowers the test score on average by 1.76 points. This effect is statistically significant at the 1% level.\n* **Expenditure (0.0025):** **Holding STR constant**, a one-unit increase in expenditure is associated with an increase of 0.0025 points in the test score. This effect is very small and statistically insignificant (p-value = 0.17).\n\n\n## Compare with Simple Regression\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(stargazer)\nmodel_simple <- lm(TestScore ~ expenditure, data = cas)\nstargazer(model_simple, model_multiple,\n    digits = 4,\n    type = \"html\",\n    dep.var.labels = \"Test Score\",\n    column.labels = c(\"SLR\", \"MLR\"),\n    covariate.labels = c(\"Expenditure\", \"STR\"),\n    notes = \"<span>&#42;</span>: p<0.1; <span>&#42;&#42;</span>: <strong>p<0.05</strong>; <span>&#42;&#42;&#42;</span>: p<0.01 <br> Standard errors in parentheses.\",\n    notes.append = F\n)\n```\n\n\n<table style=\"text-align:center\"><tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"></td><td colspan=\"2\"><em>Dependent variable:</em></td></tr>\n<tr><td></td><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr>\n<tr><td style=\"text-align:left\"></td><td colspan=\"2\">Test Score</td></tr>\n<tr><td style=\"text-align:left\"></td><td>SLR</td><td>MLR</td></tr>\n<tr><td style=\"text-align:left\"></td><td>(1)</td><td>(2)</td></tr>\n<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">Expenditure</td><td>0.0057<sup>***</sup></td><td>0.0025</td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.0014)</td><td>(0.0018)</td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td></td></tr>\n<tr><td style=\"text-align:left\">STR</td><td></td><td>-1.7632<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td>(0.6109)</td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td></td></tr>\n<tr><td style=\"text-align:left\">Constant</td><td>623.6165<sup>***</sup></td><td>675.5772<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(7.7197)</td><td>(19.5622)</td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td></td></tr>\n<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">Observations</td><td>420</td><td>420</td></tr>\n<tr><td style=\"text-align:left\">R<sup>2</sup></td><td>0.0366</td><td>0.0555</td></tr>\n<tr><td style=\"text-align:left\">Adjusted R<sup>2</sup></td><td>0.0343</td><td>0.0509</td></tr>\n<tr><td style=\"text-align:left\">Residual Std. Error</td><td>18.7239 (df = 418)</td><td>18.5619 (df = 417)</td></tr>\n<tr><td style=\"text-align:left\">F Statistic</td><td>15.8734<sup>***</sup> (df = 1; 418)</td><td>12.2409<sup>***</sup> (df = 2; 417)</td></tr>\n<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"><em>Note:</em></td><td colspan=\"2\" style=\"text-align:right\"><span>&#42;</span>: p<0.1; <span>&#42;&#42;</span>: <strong>p<0.05</strong>; <span>&#42;&#42;&#42;</span>: p<0.01 <br> Standard errors in parentheses.</td></tr>\n</table>\n:::\n\n\nIn the simple regression of TestScore on expenditure:\n\n$$\n\\widehat{\\text{TestScore}} = 623.6 + 0.0057 \\times \\text{expenditure},\n$$\n\n* Expenditure had a positive and statistically significant effect (p < 0.001).\n* The effect size (0.0057 vs. 0.0025) was larger than in the multiple regression.\n* The overall explanatory power was improved (Adjusted $R^2$ ≈ 0.0343 in the SLR vs. $R^2$ ≈ 0.0509 in the MLR).\n\nThus, once STR is included, the apparent effect of expenditure becomes much smaller and loses statistical significance.\n\n___\n\nQ: Why does the effect of expenditure change when STR is included?\n\nA: Expenditure is no longer significant in the multiple regression because expenditure and STR are **correlated**. Districts that spend more on education often also have smaller class sizes ($\\rho = -0.62$).\n\nIn the simple regression, the positive effect of expenditure partly reflected the fact that higher spending was associated with lower STR, which itself improves test scores. Once STR is explicitly controlled for, the “true” partial effect of expenditure (holding class size constant) is close to zero.\n\nThis illustrates the importance of multiple regression: it separates the effect of each regressor while holding others fixed, and it reveals that the real driver of test scores here is class size, not expenditure.\n\n\n## Regression diagnostics\n\n* **Residual Standard Error (RSE):**\n\n  * SLR: 18.72 (df = 418) → MLR: 18.56 (df = 417)\n  * The MLR has a slightly smaller RSE, meaning it predicts test scores a bit more accurately.\n\n* **Multiple $R^2$:**\n\n  * SLR: 0.037 → MLR: 0.055\n  * Adding STR increases the proportion of variation explained, though the overall fit remains modest.\n\n* **Adjusted $R^2$:**\n\n  * SLR: 0.034 → MLR: 0.051\n  * The adjusted $R^2$ also improves, showing STR provides genuine explanatory power beyond expenditure.\n\n* **F-statistic (overall significance):**\n\n  * SLR: 15.87, p < 0.001 →  MLR: 12.24, p < 0.001\n  * Both models are statistically significant overall. The MLR has a smaller F-statistic because it includes more regressors, but still clearly improves explanatory power.\n\n\n# F-test for Significance of the Overall Regression\n\nF-test is also known as the one-way analysis of variance (ANOVA) test. It tests whether the regression model as a whole is statistically significant.\n\nF-test for that all $k$ of the slope coefficients in a linear model are equal to zero, i.e., to test the exclusion of all explanatory variables <span class=\"env-orange\">except the intercept</span>, $\\beta_0$. \nFormally speaking.\n$$\n\\begin{aligned}\n&\\text{H}_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_K=0 \\\\\n&\\text{H}_1: \\text{At least one of the } \\beta_1, \\beta_2,\\ldots, \\beta_K,\\text{is not zero.}\n\\end{aligned}\n$$\nThis is referred to as the <span class=\"env-green\">F-test</span> for <span class=\"env-green\">one-way ANOVA</span>.\n\nThe test statistic is given by:\n$$\nF = \\left(\\frac{n-k-1}{k}\\right) \\left(\\frac{R^2}{1-R^2}\\right) \\sim F(k, n-k-1) .\n$$\n\nwhere $F$ is $F$-distributed with $k$ and $n-k-1$ degrees of freedom.\n\nThe $F$-statistic can be rewritten as:\n\n$$\n\\begin{split}\nF &= \\frac{R^2/k}{(1-R^2)/(n-k-1)} = \\frac{SSE/k}{SSR/(n-k-1)} \\\\\n&= \\frac{\\text{MSE}}{\\text{MSR}} \\\\\n& = \\frac{\\text{\\green{Explained} Variation per degree of freedom}}{\\text{\\green{Residual} Variation per degree of freedom}} ,\n\\end{split}\n$$\n\nwhere\n\n- $SSE$ is the explained sum of squares (variation **explained** by the regression model).\n  - $MSE = SSE/k$ is the mean square due to explained variation.\n- $SSR$ is the residual sum of squares (variation **not explained** by the regression model).\n  - $MSR = SSR/(n-k-1)$ is the mean square due to residual variation.\n\nWe reject $\\text{H}_0$ if $F>F_{\\alpha}(k, n-k-1)$. \n\n- $F_{\\alpha}(k,n-k-1)$ is the $(1-\\alpha)$ percentile in the $F(k, n-k-1)$ distribution, corresponding to the level of significance, $\\alpha$.\n\n\n<img src=\"https://drive.google.com/thumbnail?id=1LSF9a8en1SvwZDL76SHeZ0BpDYhHbLz-&sz=w1000\" alt=\"F distribution\" style=\"display: block; margin-right: auto; margin-left: auto; zoom:80%;\" />\n\n\nThe $p$-value is found by:\n$$\n\\text{P-value} = \\mathbb{P}(F>F_{\\text{obs}}) ,\n$$\nwhere $F_{\\text{obs}}$ is your calculated/observed test statistic based on your data sample.\n\n- Large values of $F$ give evidence against the validity of the null hypothesis. Note that a large $F$ is induced by a large value of $R^2$.\n\n- The logic of the test is that the $F$ statistic is a measure of the loss of fit (namely, all of $R^2$) that results when we impose the restriction that all the slopes are zero. If $F$ is large, then the hypothesis is rejected.\n\n\n## Example: F-test for the Multiple Regression Model\n\n::: {.step-list}\n1. **Null and alternative hypotheses**\n    $$\n    \\begin{aligned}\n    &\\text{H}_0: \\beta_1 = \\beta_2 = 0 \\\\\n    &\\text{H}_1: \\text{At least one of the } \\beta_1, \\beta_2,\\text{is not zero.}\n    \\end{aligned}\n    $$\n    In plain language,\n    - $\\text{H}_0$: Expenditure and STR have no effect on test scores.\n    - $\\text{H}_1$: At least one of expenditure and STR has an effect on test scores.\n\n2. **Calculate the test statistic**\n   $$\n   F=\\frac{(R^{2}/k)}{(1-R^{2})/(n-k-1)}\n   $$\n\n   where $k$ is the number of regressors (not counting the intercept) and $n$ is the sample size.\n\n   From the regression output, we have\n\n   - $R^{2}/k = 0.05545/2 = 0.027725.$\n   - $(1-R^{2})/(n-k-1) = (1-0.05545)/417 \\approx 0.00226511$\n   \n   Hence, the $F$ statistic is\n   $$\n   F = \\frac{0.027725}{0.00226511} \\approx 12.24 .\n   $$\n\n3. **Critical value at $\\alpha=5\\%$**\n   \n   Degrees of freedom for the $F$ distribution are $(df_{1}, df_{2})=(k, n-k-1)=(2, 417)$. \n\n   Referring to the [$F$-distribution table][distribution-table] to find the critical value.\n   \n   According to the $F$ table, there is no exact value for $df_2=417$, we use the value for $df_2=\\infty$ as an approximation.\n\n   We use the The critical value $F_{0.95}(2, \\infty) = 3.00$. The exact value is $F_{0.95}(2, 417)$ is expected to be slightly larger than $3.00$. But since our calculated $F$ statistic is much larger than $3.00$, we can safely reject the null hypothesis. It does not matter if the exact critical value is $3.01$ or $3.06$.\n\n4. **Decision rule and conclusion**\n   \n   Decision rule at 5 percent significance level: reject $H_{0}$ if $F>F_{\\text{crit}}.$ \n\n   Since $F_{\\text{obs}}=12.24>F_{\\text{crit}}=3.00$, we reject $H_{0}$ and conclude that at least one of expenditure and STR has a statistically significant effect on test scores.\n\n5. **Interpretation in context**\n   \n   Statistically the regression is significant overall. That means at least one of the regressors, expenditure or STR, is linearly related to test scores. \n\n   From the coefficient t tests in your summary we know STR is the significant predictor while expenditure is not significant once STR is included. In plain language this suggests that class size is driving the overall significance of the model rather than expenditure.\n:::\n\n\n[distribution-table]: https://drive.google.com/file/d/1Iwec44x4N5AItbpFugbUOKHBIE10cMOw/view?usp=drive_link \"distribution tables all-in-one\"\n\n\n# More Alternative Model Specifications\n\nWe now consider student demographics as additional regressors. We include the following variables:\n\n- `eng`: percentage of students who are English learners.\n- `lunch`: percentage of students who are eligible for free lunch.\n- `calworks`: percentage of students whose families receive aid from the California Work Opportunity and Responsibility to Kids (CalWORKs) program.\n\nNote\n\n- Students eligible for CalWorks live in families with a total income below the threshold for the subsidized lunch program so both variables are indicators for the share of **economically disadvantaged children**. \n\n\nWe consider the following models:\n\n$$\n\\begin{align*}\n  (I) \\quad TestScore=& \\, \\beta_0 + \\beta_1 \\times STR + u, \\\\\n  (II) \\quad TestScore=& \\, \\beta_0 + \\beta_1 \\times STR + \\beta_2 \\times english + u, \\\\\n  (III) \\quad TestScore=& \\, \\beta_0 + \\beta_1 \\times STR + \\beta_2 \\times english + \\beta_3 \\times lunch + u, \\\\\n  (IV) \\quad TestScore=& \\, \\beta_0 + \\beta_1 \\times STR + \\beta_2 \\times english + \\beta_4 \\times calworks + u, \\\\\n  (V) \\quad TestScore=& \\, \\beta_0 + \\beta_1 \\times STR + \\beta_2 \\times english + \\beta_3 \\times lunch + \\beta_4 \\times calworks + u.\n\\end{align*}\n$$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(sandwich) # for robust standard errors\n\n# estimate different model specifications\nspec1 <- lm(TestScore ~ STR, data = cas)\nspec2 <- lm(TestScore ~ STR + english, data = cas)\nspec3 <- lm(TestScore ~ STR + english + lunch, data = cas)\nspec4 <- lm(TestScore ~ STR + english + calworks, data = cas)\nspec5 <- lm(TestScore ~ STR + english + lunch + calworks, data = cas)\n\n# gather robust standard errors in a list\nrob_se <- list(\n    sqrt(diag(vcovHC(spec1, type = \"HC1\"))),\n    sqrt(diag(vcovHC(spec2, type = \"HC1\"))),\n    sqrt(diag(vcovHC(spec3, type = \"HC1\"))),\n    sqrt(diag(vcovHC(spec4, type = \"HC1\"))),\n    sqrt(diag(vcovHC(spec5, type = \"HC1\")))\n)\n\n# generate a summarizing table using stargazer\nstargazer(spec1, spec2, spec3, spec4, spec5,\n    type = \"html\",\n    se = rob_se,\n    digits = 3,\n    header = F,\n    column.labels = c(\"(I)\", \"(II)\", \"(III)\", \"(IV)\", \"(V)\"),\n    model.numbers = FALSE,\n    notes = \"<span>&#42;</span>: p<0.1; <span>&#42;&#42;</span>: <strong>p<0.05</strong>; <span>&#42;&#42;&#42;</span>: p<0.01 <br> Standard errors in parentheses.\",\n    notes.append = F\n)\n```\n\n\n<table style=\"text-align:center\"><tr><td colspan=\"6\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"></td><td colspan=\"5\"><em>Dependent variable:</em></td></tr>\n<tr><td></td><td colspan=\"5\" style=\"border-bottom: 1px solid black\"></td></tr>\n<tr><td style=\"text-align:left\"></td><td colspan=\"5\">TestScore</td></tr>\n<tr><td style=\"text-align:left\"></td><td>(I)</td><td>(II)</td><td>(III)</td><td>(IV)</td><td>(V)</td></tr>\n<tr><td colspan=\"6\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">STR</td><td>-2.280<sup>***</sup></td><td>-1.101<sup>**</sup></td><td>-0.998<sup>***</sup></td><td>-1.308<sup>***</sup></td><td>-1.014<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.519)</td><td>(0.433)</td><td>(0.270)</td><td>(0.339)</td><td>(0.269)</td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td></td><td></td><td></td><td></td></tr>\n<tr><td style=\"text-align:left\">english</td><td></td><td>-0.650<sup>***</sup></td><td>-0.122<sup>***</sup></td><td>-0.488<sup>***</sup></td><td>-0.130<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td>(0.031)</td><td>(0.033)</td><td>(0.030)</td><td>(0.036)</td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td></td><td></td><td></td><td></td></tr>\n<tr><td style=\"text-align:left\">lunch</td><td></td><td></td><td>-0.547<sup>***</sup></td><td></td><td>-0.529<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td></td><td>(0.024)</td><td></td><td>(0.038)</td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td></td><td></td><td></td><td></td></tr>\n<tr><td style=\"text-align:left\">calworks</td><td></td><td></td><td></td><td>-0.790<sup>***</sup></td><td>-0.048</td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td></td><td></td><td>(0.068)</td><td>(0.059)</td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td></td><td></td><td></td><td></td></tr>\n<tr><td style=\"text-align:left\">Constant</td><td>698.933<sup>***</sup></td><td>686.032<sup>***</sup></td><td>700.150<sup>***</sup></td><td>697.999<sup>***</sup></td><td>700.392<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(10.364)</td><td>(8.728)</td><td>(5.568)</td><td>(6.920)</td><td>(5.537)</td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td></td><td></td><td></td><td></td></tr>\n<tr><td colspan=\"6\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">Observations</td><td>420</td><td>420</td><td>420</td><td>420</td><td>420</td></tr>\n<tr><td style=\"text-align:left\">R<sup>2</sup></td><td>0.051</td><td>0.426</td><td>0.775</td><td>0.629</td><td>0.775</td></tr>\n<tr><td style=\"text-align:left\">Adjusted R<sup>2</sup></td><td>0.049</td><td>0.424</td><td>0.773</td><td>0.626</td><td>0.773</td></tr>\n<tr><td style=\"text-align:left\">Residual Std. Error</td><td>18.581 (df = 418)</td><td>14.464 (df = 417)</td><td>9.080 (df = 416)</td><td>11.654 (df = 416)</td><td>9.084 (df = 415)</td></tr>\n<tr><td style=\"text-align:left\">F Statistic</td><td>22.575<sup>***</sup> (df = 1; 418)</td><td>155.014<sup>***</sup> (df = 2; 417)</td><td>476.306<sup>***</sup> (df = 3; 416)</td><td>234.638<sup>***</sup> (df = 3; 416)</td><td>357.054<sup>***</sup> (df = 4; 415)</td></tr>\n<tr><td colspan=\"6\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"><em>Note:</em></td><td colspan=\"5\" style=\"text-align:right\"><span>&#42;</span>: p<0.1; <span>&#42;&#42;</span>: <strong>p<0.05</strong>; <span>&#42;&#42;&#42;</span>: p<0.01 <br> Standard errors in parentheses.</td></tr>\n</table>\n:::\n\n\n\n## <span class=\"env-green\">**Interpretations**</span>\n\n- Adding control variables roughly halves the coefficient on **STR**.  \n- The estimate is sensitive to the set of control variables used.  \n- Conclusion: decreasing the student–teacher ratio <span class=\"env-green\">*ceteris paribus*</span> by one unit leads to an estimated average increase in test scores of about **1 point**.  \n  \n  \"Ceteris Paribus\" means \"all other things being equal\", i.e., holding all other regressors constant.\n\n---\n\n- Adding student characteristics as controls increases  \n  - $R^{2}$ and $\\bar{R}^{2}$ from **0.049 (spec (1))** up to **0.773 (spec (3) and spec (5))**.  \n  - These variables can therefore be considered suitable predictors of test scores.  \n\n---\n\n- `calworks` is not statistically significant in all models.  \n  - Example: in **spec (5)**, the coefficient on `calworks` is not significantly different from zero at the 5% level since\n    $$\n    \\left|\\frac{-0.048}{0.059}\\right| = 0.81 < 1.96.\n    $$\n    For $df = 415$, we can safely use the normal distribution table to find the critical value for a two-sided t-test at the 5% significance level as the sample size is large ($df>30$).\n\n    Commonly used critical values under normal distribution: \n      - <span class=\"env-green\">1.96 for 5% significance level;</span>\n      - 1.64 for 10% significance level; and\n      - 2.58 for 1% significance level.\n- The effect of adding `calworks` to the base specification (spec (3)) on the coefficient of **size** and its standard error is negligible. \n- We can therefore consider `calworks` as a **superfluous control variable**, given the inclusion of `lunch` in this model. \n  \n  The two variables are highly correlated ($\\rho = 0.74$). See @fig-scatter-2 for a scatter plot.\n\n  ::: {.cell layout-align=\"center\"}\n  \n  ```{.r .cell-code}\n  cat(\"Correlation between lunch and calworks: \\n\\n\")\n  cor(cas %>% select(lunch, calworks)) %>% round(2)\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  \n  ```\n  Correlation between lunch and calworks: \n  \n           lunch calworks\n  lunch     1.00     0.74\n  calworks  0.74     1.00\n  ```\n  \n  \n  :::\n  :::\n\n\n\n\n  ::: {.cell layout-align=\"center\"}\n  ::: {.cell-output-display}\n  ![Scatter plots of `lunch` vs. `calworks`. Positive linear relationship between the two variables.](06_Multiple_Linear_Regression_files/figure-html/fig-scatter-2-1.png){#fig-scatter-2 fig-align='center' fig-pos='H' width=672}\n  :::\n  :::\n\n\n___\n\n**Ref:**\n\n- Section 7.6 Analysis of the Test Score Data Set, *Introduction to Econometrics with R*, <https://www.econometrics-with-r.org/7-6-analysis-of-the-test-score-data-set.html>",
    "supporting": [
      "06_Multiple_Linear_Regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}