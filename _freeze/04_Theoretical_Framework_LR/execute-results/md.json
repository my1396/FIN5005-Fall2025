{
  "hash": "55d2e51e8d34a9ff52fe4de8c65865d0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Theoretical Framework of Linear Regression\"\n---\n\nIn this section, we will discuss the theoretical framework of linear regression, including the assumptions underlying the linear regression model and essential hypothesis tests.\n\ntest \\( X_2 \\)\n\nQ: Why the assumptions matter?\n\nA: In short, violation of the assumptions may lead to biased or inefficient estimates, incorrect inferences, and unreliable predictions.\n\n--------------------------------------------------------------------------------\n\n## Assumptions of Linear Regression\n\nQ: What are the assumptions of linear regression?\n\nA: The **Gauss-Markov Assumptions** ensure that the Ordinary Least Squares (OLS) estimator is the Best Linear Unbiased Estimator (BLUE). \n\n1. **Linearity**: The relationship between the independent variable(s) and the dependent variable is linear in parameters.\n2. **No perfect multicollinearity**: The independent variables are NOT <span class=\"env-orange\">perfectly correlated.</span>\n3. **Error has zero conditional mean**\n4. **Error is homoskedastic**\n   - Homoscedasticity: The variance of the error term is constant across all levels of the independent variable(s).\n   \n   <span style=\"color: #888888;\">In time series settings (we won't cover in this course), we also assume:</span>\n   \n   - <span style=\"color: #888888;\">No autocorrelation: The error terms are not correlated with each other.</span>\n   \n5. **Residuals are normally distributed**\n   \n   This assumption is useful for conducting hypothesis tests and constructing confidence intervals. \n\n## Regression Framework and OLS Estimator\n\nMultiple linear regression models is defined as \n\n$$\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + ... + \\beta_k X_{ik} + \\varepsilon_i\n$$\n\nwhere \n\n- $i=1,\\ldots, n$ indexes the observations, \n- $Y_i$ is the dependent variable, and \n- $X_{ij}$ represents the $j$-th independent variable for observation $i$, with $j = 1, \\ldots, k$ and $k$ denoting the total number of independent variables. \n- The coefficients $\\beta_j$ are to be estimated, and \n- $\\varepsilon_i$ is the error term.\n\n**The OLS estimator $\\hat{\\beta}^{OLS}$ minimizes the sum of squared residuals (SSR)**:\n\n$$\n\\hat{\\beta}^{OLS} = \\arg\\min_{\\beta} \\sum_{i=1}^{n} e_i^2 \n$$\n\nwhere $e_i = Y_i - \\hat{Y}_i$ is the residual for observation $i$.\n\n$\\hat{Y}_i$ is the predicted value of $Y_i$ given the estimated coefficients.\n\n$$\n\\hat{\\beta}^{OLS} = \\arg\\min_{\\beta} \\sum_{i=1}^{n} (Y_i - \\beta_0 - \\sum_{j=1}^{k} \\beta_j X_{ij})^2\n$$\n\n$\\hat{\\beta}^{OLS}$ can be obtained by the first order condition (setting the derivative of SSR with respect to each $\\beta_j$ to zero and solving the resulting system of equations).\n\nRead [OLS in Matrix Form](https://github.com/my1396/FIN5005-Fall2024/blob/a4ec0c87c6bcbd463b4da1ad78878b1fe329cda4/images/matrix_OLS_NYU_notes.pdf) for more details. We won't cover the OLS derivation in this course.\n\nHere we give the direct formula for the OLS estimator:\n\n$$\n\\hat\\beta = \\left(\\sum_{i=1}^n X_iX_i'\\right)^{-1} \\left(\\sum_{i=1}^n X_iY_i\\right)\n$$\n\nwhere \n\n$$\nX_i = \\begin{pmatrix}1 \\\\ X_{i1} \\\\ X_{i2} \\\\ \\vdots \\\\ X_{ik}\\end{pmatrix}\n$$\n\nis a $(k+1) \\times 1$ vector of independent variables including the intercept for observation $i$.\n\n\n\n**Properties of OLS estimator:**\n\n- **Unbiasedness**: $E[\\hat{\\beta}^{OLS}] = \\beta$ (if Gauss-Markov assumptions hold)\n- **Efficiency**: Among all linear unbiased estimators, OLS has the smallest variance\n- **Consistency**: As sample size $n \\to \\infty$, $\\hat{\\beta}^{OLS} \\to \\beta$ in probability\n\n\n### Distribution of OLS Estimator\n\nUnder the assumption of normally distributed errors, $\\boldsymbol{\\varepsilon} \\vert \\boldsymbol{X} \\sim N(\\boldsymbol{0}, \\sigma^2\\boldsymbol{I})$, we have in matrix form\n\n$$\n\\hat{\\boldsymbol{\\beta}} \\sim N(\\boldsymbol{\\beta}, \\sigma^2 (\\boldsymbol{X}'\\boldsymbol{X})^{-1}).\n$$ {#eq-beta-distribution}\n\nwhere \n\n- $\\sigma^2 = \\var (\\varepsilon_i \\mid \\bX)$ is the variance of the error term.\n- $\\bX$ is the matrix of independent variables.\n  $$\n  \\bX = \\begin{pmatrix}\n     X_1' \\\\ X_2' \\\\ \\vdots \\\\ X_n'\n  \\end{pmatrix}_{n \\times (k+1)}\n  $$\n\n@eq-beta-distribution is called the **exact (finite-sample) distribution** of the OLS estimator.\n\n$\\var(\\hat\\bbeta) = \\sigma^2 (\\boldsymbol{X}'\\boldsymbol{X})^{-1}$ is called the **variance-covariance matrix** of the OLS estimator, often denoted as $V_{\\hat\\beta}.$\n\n\nWe estimate $\\sigma^2$ with $\\hat{\\sigma}^2:$\n$$\n\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^ne_i^2}{n-k-1}.\n$$\n\n\nThe standard errors of $\\hat{\\beta}_j$, $j=1,\\ldots,k,$ are given by the square root of the $(j,j)$ element of the variance-covariance matrix, $V_{\\hat\\beta}$.\n\nThe individual coefficient estimates $\\hat{\\beta}_j$ are normally distributed:\n\n$$\n\\hat{\\beta}_j \\sim N(\\beta_j, \\sigma^2 v_{jj})\n$$\n\n### Sampling Distribution of OLS Estimator\n\nBecausethe OLS estimator $\\hat\\beta_0, \\hat\\beta_1, \\ldots$ are computed from a randomly drawn sample of data, the estimators themselves are random variables with a probability distribution.\n\nThe sampling distribution describes the values they could take over different possible random samples. In small samples, theses sampling distributions are complicated, but in large samples, they are normal because of the Central Limit Theorem (CLT).\n\n::: {.callout-tip title=\"Sampling Distribution vs. Exact Distribution\"}\n- The distribution depends on the sample data $(X_i, Y_i).$\n- Based on Central Limit Theorem (CLT), we can establish the sampling distribution of $\\hat\\bbeta$ when sample size is large, i.e., $n\\to\\infty$.\n:::\n\n\n### Central Limit Theorem (CLT)\n\n::: {#thm-clt title=\"Central Limit Theorem (CLT)\"}\nSuppose that $X_1, \\ldots, X_n$ is an independent and identically distributed (iid) sequence with a finite mean $\\mathbb{E}(X_i)=\\mu$ and variance  $\\text{Var}(X_i)=\\sigma^2$, where $0<\\sigma^2<\\infty$. \n\nDefine a sample mean: $\\overline{X}=\\frac{1}{n}\\sum_{i=1}^n X_i.$\n\nThe <span class=\"env-green\">**Lindeberg-LÃ©vy CLT**</span> states that\n\n$$\n\\frac{\\overline{X}-\\mathbb{E}[\\overline{X}]}{\\sqrt{\\text{Var}[\\overline{X}]}}\n= \\frac{\\overline{X}-\\mu}{\\sqrt{\\sigma^2/n}}\n= {\\color{#00CC66} \\sqrt{n} \\cdot \\frac{\\overline{X}-\\mu}{\\sigma} }\n\\xrightarrow{d} N(0,1)\n$$\n\nAlternative expressions of CLT:\n$$\n\\begin{split}\n\\sqrt{n} (\\overline{X}-\\mu)  & \\xrightarrow{d} N(0,\\sigma^2) \\\\\n{\\color{red}\\overline{X}} & {\\color{red}\\xrightarrow{d} N(\\mu,\\sigma^2/n)}\n\\end{split}\n$$\n:::\n\n\nQ: What does CLT mean? \n\nA:\n\n- The central limit theorem implies that if $N$ is large **enough**, we can **approximate** the distribution of $\\overline{X}$ by the standard normal distribution with mean $\\mu$ and variance $\\sigma^2 / N$ **regardless of the underlying distribution of $X$.**  \n\n- This property is called **asymptotic normality**.\n\n\n### CLT Example\n\nThe amount of time customers spend in a grocery store is a random variable with mean $\\mu = 40$ minutes and standard deviation $\\sigma = 15$ minutes. \n\n$$\n\\mathbb{E}[X] = \\mu = 40, \\quad \\text{and} \\quad \\text{SD}(X) = \\sigma = 15\n$$\n\nConsider the following probabilities:\n\n::: {.example #exm-clt1}\nAssuming $X$ is normally distributed, what is the probability that a randomly selected customer spends more than 42 minutes in the store, i.e., compute $P(X > 42)$?\n:::\n\n<button class=\"solution-btn\" onclick=\"myFunction('clt1')\">Solution</button>\n::: {#clt1 .solution-answer .env-green}\n**Individual Customer**\n\nWe define the standard normal variable $Z$ as:\n\n$$\nZ = \\frac{X - \\mu}{\\sigma} \\sim N(0,1)\n$$\n\nwhere $\\mu = 40$ and $\\sigma = 15$.\n\nTo compute $P(X > 42)$, we convert to the standard normal:\n\n$$\nP(X > 42) = P(\\frac{X-40}{15} > \\frac{42-40}{15}) =  P(Z > 0.1333)\n$$\n\nBy looking up the standard normal distribution table, we find:\n\n$$\n\\Phi(-0.1333) = 0.4483\n$$\n\nBy symmetry of the normal distribution:\n\n$$P(Z > 0.1333) = P(Z < -0.1333) = 0.4483$$\n\n\nThere is approximately a **44.83%** probability that a randomly selected customer will spend more than 42 minutes in the store.\n\nNote that the **specific distribution** for \\( X \\), i.e., normality here, is **required** to compute the probability for a single customer.\n:::\n\n\n::: {.example #exm-clt2}\nGiven a random sample of $n=64$ customers, what is the probability of the average time spent by the 64 customers exceeds 42 minutes, i.e., compute $P(\\overline{X}_{64} > 42)$? \n\nHint: apply CLT and justify why CLT can be applied here.\n:::\n\n<button class=\"solution-btn\" onclick=\"myFunction('clt2')\">Solution</button>\n::: {#clt2 .solution-answer .env-green}\n**Sample Mean for \\(n = 64\\)**\n\nBy the Central Limit Theorem, for **large \\( n \\) with finite mean and variance**, the sampling distribution of the sample mean \\( \\overline{X}_{64} \\) is approximately normal (regardless the distribution of $X$):\n\n$$\n\\overline{X}_{64} \\sim N (\\mu, \\frac{\\sigma^2}{n})\n$$\n\n\nPlug in $\\mu = 40$ and $\\sigma = 15$:\n\n$$\n\\sigma_{\\overline{X}_{64}} = \\frac{\\sigma}{\\sqrt{n}} = \\frac{15}{\\sqrt{64}} = 1.875\n$$\n\nAccording to CLT $\\overline{X}_{64}$ is normally distributed:\n\n$$\n\\overline{X}_{64} \\sim N\\left(40, 1.875^2\\right)\n$$\n\nWe standardize using the standard normal variable \\( Z \\), defined as:\n\n$$\nZ = \\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} = \\frac{42 - 40}{1.875} \\approx 1.067\n$$\n\nUsing the standard normal distribution table:\n\n$$\nP(\\overline{X} > 42) = P(Z > 1.067) \\approx 0.1436\n$$\n\nThere is a **14.35%** probability that the average time spent by a random sample of 64 customers exceeds 42 minutes.\n:::\n\n\n::: {.example #exm-clt3}\n- Given a random sample of $n=81$ customers, what is the probability of the average time spent by the 81 customers exceeds 42 minutes, i.e., compute $P(\\overline{X}_{81} > 42)$?\n:::\n\n<button class=\"solution-btn\" onclick=\"myFunction('clt3')\">Solution</button>\n::: {#clt3 .solution-answer .env-green}\n**Sample Mean for \\(n = 81\\)**\n\n$$\n\\sigma_{\\overline{X}_{81}} = \\frac{\\sigma}{\\sqrt{n}} = \\frac{15}{\\sqrt{81}} = \\frac{15}{9} = 1.667\n$$\n\nAccording to CLIT $\\overline{X}_{81}$ is normally distributed:\n\n$$\n\\overline{X}_{81} \\sim N(\\mu= 40, \\sigma_{\\overline{X}_{81}} = 1.667^2)\n$$\n\n$$\nZ = \\frac{42 - 40}{3.75} = 0.533\n$$\n\n$$\nP(\\overline{X} > 42) = P(Z > 0.533) \\approx 0.2970\n$$\nA random sample of 16 customers has a **29.70%** chance of yielding an average time above 42 minutes.\n:::\n\n\n\n\n\nDiscussion: Compare your results in the three probabilities. Provide a brief interpretation of how the probabilities differ and why.\n\n\n## Measure of Fit\n\nWe often use $R^2$ to measure the goodness of fit of a regression model. \n\n- It represents the proportion of variance in the dependent variable that is predictable by the regression model.\n\n$R^2$ is defined as:\n\n$$\nR^2 = 1 - \\frac{SSR}{SST} = \\frac{SST - SSR}{SST} = \\frac{SSE}{SST}\n$$\n\nwhere\n\n- $SSR = \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2 = \\sum_{i=1}^{n} e_i^2$ is the sum of squared residuals (unexplained variation)\n- $SST = \\sum_{i=1}^{n} (Y_i - \\bar{Y})^2$ is the total sum of squares (total variation)\n- $SSE = \\sum_{i=1}^{n} (\\hat{Y}_i - \\bar{Y})^2$ is the explained sum of squares (explained variation)\n- $\\bar{Y}= \\frac{1}{n}\\sum_{i=1}^n Y_i$ is the mean of the dependent variable\n\nNote that adding variables always increases $R^2$, even if the new variables are not statistically significant. To address this, we use the **adjusted $R^2$**.\n\nIn a regression model with multiple explanatory variables, we often useÂ **adjusted**Â $R^2$Â that adjusts the number of explanatory variables.\n\n\n## Hypothesis Testing in Regression\n\nWe have been using t-statistics and p-values to for testing hypotheses such as whether a correlation coefficient is significantly different from zero or if a regression coefficient is significantly different from zero.\n\nIn the following section, we will introduce the procedure of hypothesis testing formally. \n\nNote that we will focus on two-sided hypothesis tests in this course as they are most commonly used in practice.\n\n:::{.callout-important title=\"Study objectives for hypothesis testing\"}\nThe goal is be able to conduct hypothesis test manually given necessary information.\n\nFor example, in case of testing if a regression coefficient is significantly different from zero, you should be able to \n\n::: {.step-list style=\"font-size: 1.1rem;\"}\n1. Establish null and alternative hypotheses.\n2. Calculate the t-statistic given the coefficient estimate, standard error.\n3. Find the critical value of your test statistic given the significance level, 5% is commonly used.\n   \n   You need to be familiar with the distribution table to find the critical value. The tables can be found in the formula sheet on Canvas under the \"Past Exam\" module.\n4. Decision rule. \n   \n   Compare the t-stat with the critical value to make decision on rejecting or not rejecting the null hypothesis.\n5. Interpret the result in context. What it means in reality?\n:::\n\n:::\n\n### t-test for Single Coefficient\n\nUsing the simple linear regression model of California School Test Scores in the previous session as an example, test whether expenditure has a significant effect on test scores at the 5% significance level.\n\n```\nCall:\nlm(formula = TestScore ~ expenditure, data = cas)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-50.146 -14.206   0.689  13.513  50.127 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 6.236e+02  7.720e+00  80.783  < 2e-16 ***\nexpenditure 5.749e-03  1.443e-03   3.984 7.99e-05 ***\n---\nSignif. codes:  0 â***â 0.001 â**â 0.01 â*â 0.05 â.â 0.1 â â 1\n\nResidual standard error: 18.72 on 418 degrees of freedom\nMultiple R-squared:  0.03659,   Adjusted R-squared:  0.03428 \nF-statistic: 15.87 on 1 and 418 DF,  p-value: 7.989e-05\n```\n\nHere is the hypothesis test step by step for the expenditure coefficient in the simple linear regression of California school test scores.\n\n::: {.step-list}\n1. State Hypotheses\n\n   * $H_0:\\ \\beta_{\\text{exp}} = 0$  (expenditure has no effect on test scores)\n   * $H_1:\\ \\beta_{\\text{exp}} \\neq 0$  (two-sided)\n\n2. Calculate test statistic\n   \n   $$\n   t=\\frac{\\hat\\beta - \\beta}{\\text{SE}(\\hat\\beta)} \\sim t_{df}\n   $$\n\n   where the test statistic follows a $t$-distribution with **degrees of freedom (df)** $= n - k - 1.$ $k$ is the number of predictors (<span class=\"env-orange\">excluding the intercept</span>).\n\n   Coming back to the regression output, we have estimate $= 0.005749,$ value to test against with $= 0,$ standard error $= 0.001443.$\n\n   That is, $\\hat\\beta = 0.005749,$ $\\beta = 0,$ and $\\text{SE}(\\hat\\beta) = 0.001443,$ \n\n   $$\n   t=\\frac{\\hat\\beta - \\beta}{\\text{SE}(\\hat\\beta)}=\\frac{0.005749 - 0}{0.001443}=3.984\n   $$\n\n   where $n=420$ (number of observations) and $k=1$ (number of predictors), so $df=420-1-1=418$.\n\n3. Find critical value, $C_{\\alpha/2}.$\n   \n   For significance level at 5%, i.e., $\\alpha=0.05$, the critical value is given by\n\n   $$\n   C_{\\alpha/2} = t_{0.975, df}\n   $$\n   \n   Referring to the [$t$-distribution table][distribution-table], $t_{0.975,418}\\approx 1.96$.\n\n   [distribution-table]: https://drive.google.com/file/d/1Iwec44x4N5AItbpFugbUOKHBIE10cMOw/view?usp=drive_link \"distribution tables all-in-one\"\n\n   :::{.callout-tip title=\"Finding critical value\"}\n   Notice that when df is large, the critical value approaches that of the standard normal distribution, $z_{0.975}=1.96$.\n\n   The rule-of-thumb is that when $df > 30$, you can use the standard normal critical values.\n   :::\n\n4. Decision rule\n   \n   - Reject $H_0$ if $|t|>1.96$. \n   - Fail to reject $H_0$ if $|t|\\leq 1.96$.\n   \n   Since $3.984>1.97$, reject $H_0$.\n\n5. Interpretation in context\n   \n   Expenditure has a statistically significant positive association with test scores at the 5% level. The point estimate implies that a \\$1,000 increase in per-student expenditure is associated with about $0.005749\\times 1000=5.75$ points higher test scores. \n:::\n\n--------------------------------------------------------------------------------\n\n#### P-value Approach\n\nStatistical software often reports the p-value, which is the smallest significance level at which you would reject the null hypothesis.\n\n- If the p-value $< \\alpha$, reject $H_0$.\n- If the p-value $\\geq \\alpha$, fail to reject $H_0$.\n- Common significance levels are 1%, 5%, and 10%.\n\nQ: How to find the p-value given test statistic? \\\nA: For two-sided test, the p-value is given by\n\n$$\np\\text{-value} = 2\\;\\P(T > |t|)\n$$\n\n\n::: {.example #exm-1 title=\"Finding p-value from t-statistic\"}\nA marketing manager wants to understand whether the number of social media posts influences monthly customer engagement for an online store. She runs a regression analysis and finds a t-statistic of $t = 2.457$ for the coefficient on the number of posts, with $df = 30$.\n\n* Compute the two-sided p-value.\n* Based on a 5% significance level, would you reject the null hypothesis that the number of posts has no effect on customer engagement?\n:::\n\n<button class=\"solution-btn\" onclick=\"myFunction('solution1')\">Solution</button>\n::: {#solution1 .solution-answer .env-green}\nThe p-value is obtained by:\n$$p\\text{-value} = 2\\;\\P(T > |t|) = 2 \\times 0.01 = 0.02$$\nThe p-value is 0.02, which is less than the common significance level of 0.05. Therefore, we reject the null hypothesis and conclude that the regression coefficient is statistically significant at the 5% level.\n\n**Interpretation:** There is statistically significant evidence that the number of social media posts affects customer engagement for the online store.\n:::\n\n\n::: {.example #exm-2}\nA financial analyst is studying the relationship between advertising expenditure and sales revenue for a chain of retail stores. She runs a regression and obtains an estimated coefficient for advertising spend. To test whether advertising has a statistically significant effect on sales, she computes a t-statistic of $t = 2.15$ with $df = 25$.\n\nAt the 5% significance level, should she reject the null hypothesis that advertising has no effect on sales?\n:::\n\n<button class=\"solution-btn\" onclick=\"myFunction('solution2')\">Solution</button>\n::: {#solution2 .solution-answer .env-green}\nThe critical value at 5% significance level is $t_{0.975,25}=2.060$. Since $2.15 > 2.060$, we reject the null hypothesis at the 5% significance level. \n\n**Interpretation:** There is statistically significant evidence that advertising expenditure affects sales revenue for the retail chain.\n:::\n\n\n::: {.example #exm-3}\nA store manager wants to investigate whether the amount spent on in-store promotions affects weekly sales. She runs a regression and obtains a t-statistic of $t = 0.82$ for the coefficient on promotion spending, with $df = 28$.\n\nAt the 5% significance level, would you reject the null hypothesis that promotion spending has no effect on weekly sales?\n:::\n\n<button class=\"solution-btn\" onclick=\"myFunction('solution3')\">Solution</button>\n::: {#solution3 .solution-answer .env-green}\nThe critical value at 5% significance level is $t_{0.975,28}=2.048$. Since $0.82 < 2.048$, we **fail to reject** the null hypothesis at the 5% significance level.\n\n**Interpretation:**\n\nThere is no statistically significant evidence that the amount spent on in-store promotions affects weekly sales.\n:::\n\n### Confidence Interval for Single Coefficient\n\nA 95 percent confidence interval for the slope is given by\n\n$$\n\\left(\\hat{\\beta}-C_{\\alpha/2}\\times SE(\\hat{\\beta}),\\; \\hat{\\beta}+C_{\\alpha/2}\\times SE(\\hat{\\beta})\\right)\n$$\n\nwhere $C_{\\alpha/2}$ is the critical value for a two-sided test at significance level $\\alpha$.\n\nFor the expenditure coefficient in the simple linear regression of California school test scores, $\\hat{\\beta}=0.005749$, $SE(\\hat{\\beta})=0.001443$, and $C_{0.025}=1.96$.\n\n\n\nA 95% confidence interval is therefore\n\n$$\n0.005749 \\pm 1.97\\times 0.001443 \\approx [0.0029,\\ 0.0086],\n$$\n\nwhich corresponds to roughly 2.9 to 8.6 points per \\$1,000. \n\nThe effect is statistically significant but modest in size, and the low $R^2$ indicates that many other factors also influence test scores.\n\n### F-test for Significance of the Overall Regression\n\nF-test is also known as the one-way analysis of variance (ANOVA) test. It tests whether the regression model as a whole is statistically significant.\n\n[To be finished...]\n\n\n\n### Homoskedasticity \n\nHomoskedasticity assumption requires constant variance of the error term across all levels of the independent variable(s).\n\n$$\n\\var(\\varepsilon_i | X) = \\sigma^2\n$$\n\nWhen the assumption is violated, we have heteroskedasticity:\n\n$$\n\\var(\\varepsilon_i | X) = \\sigma^2_i\n$$\n\nthat is, the variance of the error term varies with the level of the independent variable(s).\n\nIf the error has heteroskedasticity, the standard error under homoskedasticity assumption will be underestimated, leading to invalid t statistics, affecting hypothesis tests and confidence intervals.\n\nA quick fix is to use <span class=\"env-green\">heteroskedasticity-robust standard errors</span>, which are valid even when the homoskedasticity assumption is violated.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(sandwich) # for robust standard errors\nlibrary(lmtest)   # for coeftest()\nf_name <- \"https://raw.githubusercontent.com/my1396/FIN5005-Fall2025/refs/heads/main/data/CASchools_test_score.csv\"\ncas <- read_csv(f_name,\n    col_types = cols(\n        county = col_factor(), # read as factor\n        grades = col_factor()\n    )\n)\ncas <- cas %>%\n    mutate(TestScore = (read + math) / 2)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(TestScore ~ expenditure, data = cas)\n# heteroskedasticity-robust standard errors\ncoeftest(model, vcov = vcovHC(model, type = \"HC1\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nt test of coefficients:\n\n              Estimate Std. Error t value  Pr(>|t|)    \n(Intercept) 6.2362e+02 8.4664e+00  73.657 < 2.2e-16 ***\nexpenditure 5.7488e-03 1.6203e-03   3.548 0.0004322 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n**Under the heteroskedasticity-robust standard errors:**\n\n- no effects on the coefficient estimates\n- the Std. Error is slightly larger (0.001620 vs. 0.001443). The coefficient remains statistically significant at the 5% level.\n\n**Interpretation**\n\nThe heteroskedasticity-robust results suggest that the OLS standard errors may have been too optimistic (underestimated variability). While the statistical significance of the expenditure effect is slightly reduced, it remains highly significant, indicating a robust positive relationship between expenditure and test scores.",
    "supporting": [
      "04_Theoretical_Framework_LR_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}